{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d38df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import argparse\n",
    "from datetime import date\n",
    "import json\n",
    "import copy\n",
    "from scipy.interpolate import RectBivariateSpline\n",
    "import scipy\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from itertools import product\n",
    "from datetime import date\n",
    "\n",
    "'''\n",
    "\n",
    "os.system(\"jupyter nbconvert --to script 'climate_data.ipynb'\")\n",
    "os.system(\"jupyter nbconvert --to script 'climate_models.ipynb'\")\n",
    "'''\n",
    "\n",
    "import climate_data\n",
    "import climate_models\n",
    "\n",
    "def options(string_input=[]):\n",
    "    parser=argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-b\",\"--batch\",type=int,default=4)\n",
    "    parser.add_argument(\"-e\",\"--epoch\",type=int,default=4)\n",
    "    parser.add_argument(\"-r\",\"--rootdir\",type=str,default='/scratch/cg3306/climate/runs/')\n",
    "    parser.add_argument(\"--nworkers\",type=int,default=1)\n",
    "    parser.add_argument(\"-o\",\"--outdir\",type=str,default=\"\")\n",
    "    parser.add_argument(\"--testrun\",type=int,default=0)\n",
    "    parser.add_argument(\"--action\",type=str,default=\"train\")\n",
    "    parser.add_argument(\"--model_id\",type=str,default=\"0\")\n",
    "    parser.add_argument(\"--data_address\",type=str,default=\\\n",
    "                        '/scratch/ag7531/mlruns/19/bae994ef5b694fc49981a0ade317bf07/artifacts/forcing/')\n",
    "    parser.add_argument(\"--relog\",type=int,default=0)\n",
    "    parser.add_argument(\"--rerun\",type=int,default=0)\n",
    "    parser.add_argument(\"--lr\",type=float,default=0.01)\n",
    "    parser.add_argument(\"--model_bank_id\",type=str,default=\"0\")\n",
    "    parser.add_argument(\"--physical_dom_id\",type=int,default=0)\n",
    "    parser.add_argument(\"--subtime\",type=float,default=1)\n",
    "    parser.add_argument(\"--disp\",type=int,default=500)\n",
    "    parser.add_argument(\"--co2\",type=int,default=0)\n",
    "    parser.add_argument(\"--depth\",type=int,default=0)\n",
    "    if len(string_input)==0:\n",
    "        return parser.parse_args()\n",
    "    return parser.parse_args(string_input)\n",
    "def default_collate(batch):\n",
    "    if len(batch[0])==3:\n",
    "        data = torch.stack([item[0] for item in batch])\n",
    "        mask = torch.stack([item[1] for item in batch])\n",
    "        target = torch.stack([item[2] for item in batch])\n",
    "        return data,mask,target\n",
    "    else:\n",
    "        data = torch.stack([item[0] for item in batch])\n",
    "        target = torch.stack([item[1] for item in batch])\n",
    "        return data,target\n",
    "def load_from_save(args):\n",
    "    if len(args.outdir)>0:\n",
    "        root = args.rootdir +str(args.outdir) +str(args.model_id)\n",
    "    else:\n",
    "        root = args.rootdir +str(args.model_bank_id) + \"-\"+str(args.model_id)\n",
    "    PATH0 = root + \"/last-model\"\n",
    "    PATH1 = root + \"/best-model\"\n",
    "    LOG = root + \"/log.json\"\n",
    "    net,loss,data_init,partition=climate_models.model_bank(args)\n",
    "    \n",
    "    isdir = os.path.isdir(args.rootdir)\n",
    "    if not isdir:\n",
    "        os.mkdir(args.rootdir)\n",
    "    isdir = os.path.isdir(root) \n",
    "    if not isdir:\n",
    "        os.mkdir(root)\n",
    "    if not args.rerun:\n",
    "        try:\n",
    "            net.load_state_dict(torch.load(PATH1,map_location=get_device()))\n",
    "            net.train()\n",
    "            print(\"Loaded the existing model\")\n",
    "        except IOError:\n",
    "            print(\"No existing model found - new beginnings\")\n",
    "    try: \n",
    "        with open(LOG) as f:\n",
    "            logs = json.load(f)\n",
    "    except IOError:\n",
    "        if not net.gan:\n",
    "            logs = {\"epoch\":[],\"train-loss\":[],\"test-loss\":[],\"val-loss\":[],\"lr\":[],\"batchsize\":[]}\n",
    "        else:\n",
    "            logs = {\"epoch\":[],\"train-generator-loss\":[],\"train-discriminator-loss\":[],\\\n",
    "                                \"test-generator-loss\":[],\"test-discriminator-loss\":[],\\\n",
    "                                \"val-generator-loss\":[],\"val-discriminator-loss\":[],\\\n",
    "                                 \"lr-generator\":[],\"lr-discriminator\":[],\\\n",
    "                            \"batchsize\":[]}\n",
    "    if args.relog:\n",
    "        if not net.gan:\n",
    "            logs = {\"epoch\":[],\"train-loss\":[],\"test-loss\":[],\"val-loss\":[],\"lr\":[],\"batchsize\":[]}\n",
    "        else:\n",
    "            logs = {\"epoch\":[],\"train-generator-loss\":[],\"train-discriminator-loss\":[],\\\n",
    "                                \"test-generator-loss\":[],\"test-discriminator-loss\":[],\\\n",
    "                                \"val-generator-loss\":[],\"val-discriminator-loss\":[],\\\n",
    "                                 \"lr-generator\":[],\"lr-discriminator\":[],\\\n",
    "                            \"batchsize\":[]}\n",
    "    return net,loss,(data_init,partition), logs,(PATH0,PATH1,LOG,root)\n",
    "def load_data( data_init,partition,args):\n",
    "    '''if args.testrun>100 or args.testrun==0:\n",
    "        max_epochs=args.epoch\n",
    "    if args.testrun>0\n",
    "        max_epochs=4'''\n",
    "    '''args.physical_dom_id=physical_dom_id\n",
    "    partition=climate_data.physical_domains(args.physical_dom_id)'''\n",
    "    timeout=180\n",
    "    params={'batch_size':args.batch,'shuffle':True, 'num_workers':args.nworkers,'timeout':timeout}\n",
    "        \n",
    "    training_set = data_init(partition['train']) #climate_data.Dataset(ds_zarr,partition['train'],net,subtime=args.subtime)\n",
    "    training_generator = torch.utils.data.DataLoader(training_set, **params,collate_fn=default_collate)\n",
    "    \n",
    "    \n",
    "    params={'batch_size':args.batch,'shuffle':False, 'num_workers':args.nworkers,'timeout':timeout}\n",
    "    \n",
    "    val_set = data_init(partition['validation'])#climate_data.Dataset(ds_zarr,partition['validation'],net,subtime=args.subtime)\n",
    "    val_generator = torch.utils.data.DataLoader(val_set,\\\n",
    "                                        **params,collate_fn=default_collate)\n",
    "\n",
    "    test_set = data_init(partition['test'])#climate_data.Dataset(ds_zarr,partition['test'],net,subtime=args.subtime)\n",
    "    test_generator = torch.utils.data.DataLoader(test_set,**params,collate_fn=default_collate)\n",
    "    \n",
    "    earth_set = data_init(partition['earth'])#climate_data.Dataset(ds_zarr,partition['earth'],net,subtime=args.subtime)\n",
    "    earth_generator = torch.utils.data.DataLoader(earth_set,\\\n",
    "                                          **params,collate_fn=default_collate)\n",
    "    return (training_set,training_generator),\\\n",
    "            (val_set,val_generator),\\\n",
    "                (test_set,test_generator),\\\n",
    "                    (earth_set,earth_generator)\n",
    "def get_device():\n",
    "    use_cuda=torch.cuda.is_available()\n",
    "    device=torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "    torch.backends.cudnn.benchmark=True\n",
    "    return device\n",
    "def prep(args):\n",
    "    print('prepping',flush=True)\n",
    "    net,criterion,(data_init,partition),logs,(PATH0,PATH1,LOG,root)=load_from_save(args)\n",
    "    \n",
    "    (training_set,training_generator),_,_,_=load_data(data_init,partition,args)\n",
    "    \n",
    "    training_set.save_scales()\n",
    "    training_set.save_masks()\n",
    "\n",
    "def linear_feature_run(generator,indim,outdim,net,save_iter,landmasks,root,tag):\n",
    "    if root[-1]=='/':\n",
    "        root=root[:-1]\n",
    "    tt=0\n",
    "    X2=torch.zeros(indim,indim)\n",
    "    XY=torch.zeros(indim,outdim)\n",
    "    Y2=torch.zeros(outdim)\n",
    "    device=get_device()\n",
    "    for X,M, Y in generator:\n",
    "        with torch.set_grad_enabled(False):\n",
    "            X=X.to(device)\n",
    "            X2_,XY_,Y2_=net.cross_products(X,Y,mask=landmasks[M])\n",
    "        X2+=X2_\n",
    "        XY+=XY_\n",
    "        Y2+=Y2_\n",
    "        tt+=1\n",
    "        if tt%save_iter==0:\n",
    "            np.save(root+'/X2-'+tag+'.npy',X2.numpy())\n",
    "            np.save(root+'/XY-'+tag+'.npy',XY.numpy())\n",
    "            np.save(root+'/Y2-'+tag+'.npy',Y2.numpy())\n",
    "            print('\\t\\t'+str(tt),flush=True)\n",
    "    np.save(root+'/X2-'+tag+'.npy',X2.numpy())\n",
    "    np.save(root+'/XY-'+tag+'.npy',XY.numpy())\n",
    "    np.save(root+'/Y2-'+tag+'.npy',Y2.numpy())\n",
    "def train(args):\n",
    "    net,criterion,(data_init,partition),logs,(PATH0,PATH1,LOG,root)=load_from_save(args)\n",
    "    (training_set,training_generator),(val_set,val_generator),(test_set,test_generator),(glbl_set,glbl_gen)=load_data(data_init,partition,args)\n",
    "    if isinstance(training_set,climate_data.Dataset2):\n",
    "        landmasks=training_set.get_masks()\n",
    "    else:\n",
    "        landmasks=climate_data.get_land_masks(val_generator)\n",
    "    device=get_device()\n",
    "    landmasks=landmasks.to(device)\n",
    "    landmasks.requires_grad=False\n",
    "    if isinstance(net, climate_models.RegressionModel):\n",
    "        outdim=len(training_set.outputs)\n",
    "        indim=net.outputdimen\n",
    "        save_iter=10\n",
    "\n",
    "        generators=[training_generator,val_generator,test_generator]\n",
    "        tags=['train','val','test']\n",
    "        for i in range(len(tags)):\n",
    "            generator=generators[i]\n",
    "            tag=tags[i]\n",
    "            print(tag)\n",
    "            linear_feature_run(generator,indim,outdim,net,save_iter,landmasks,root,tag)\n",
    "        return\n",
    "    \n",
    "    \n",
    "    \n",
    "    max_epochs = args.epoch\n",
    "    kgan=2\n",
    "    if not net.gan:\n",
    "        if len(logs['lr'])==0:\n",
    "            optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9)\n",
    "        else:\n",
    "            optimizer = optim.SGD(net.parameters(), lr=logs['lr'][-1], momentum=0.9)\n",
    "        scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor=0.5,patience=5)\n",
    "    else:\n",
    "        if len(logs['lr-generator'])==0:\n",
    "            discriminator_optimizer = optim.SGD(net.discriminator.parameters(), lr=args.lr, momentum=0.9)\n",
    "            generator_optimizer = optim.SGD(net.generator.parameters(), lr=args.lr, momentum=0.9)\n",
    "        else:\n",
    "            discriminator_optimizer = optim.SGD(net.discriminator.parameters(), lr=logs['lr-discriminator'][-1], momentum=0.9)\n",
    "            generator_optimizer = optim.SGD(net.generator.parameters(), lr=logs['lr-generator'][-1], momentum=0.9)\n",
    "        scheduler_discriminator=torch.optim.lr_scheduler.\\\n",
    "                        ReduceLROnPlateau(discriminator_optimizer,factor=0.5,patience=5)\n",
    "        scheduler_generator=torch.optim.lr_scheduler.\\\n",
    "                        ReduceLROnPlateau(generator_optimizer,factor=0.5,patience=5)\n",
    "\n",
    "    best_counter=0\n",
    "    print(\"epochs started\")\n",
    "    for epoch in range(max_epochs):\n",
    "        if not net.gan:\n",
    "            logs['train-loss'].append([])\n",
    "        else:\n",
    "            logs['train-generator-loss'].append([])\n",
    "            logs['train-discriminator-loss'].append([])\n",
    "        tt=0\n",
    "        for local_batch,dom_num, local_labels in training_generator:\n",
    "            local_batch, local_labels = local_batch.to(device),local_labels.to(device)\n",
    "            #print('train: '+str(local_batch.shape))\n",
    "            if not net.gan:\n",
    "                outputs = net.forward(local_batch)\n",
    "                loss = criterion(outputs, local_labels,landmasks[dom_num])\n",
    "                logs['train-loss'][-1].append(loss.item())\n",
    "                scheduler.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                scheduler.optimizer.step()\n",
    "            else:\n",
    "                shp=torch.tensor(local_batch.shape)\n",
    "                shp[1]=1\n",
    "                random_field=torch.randn(shp.tolist()).to(device)\n",
    "                yhat = net.generator_forward(local_batch,random_field)\n",
    "                phat = net.discriminator_forward(local_batch,yhat)#,local_labels)\n",
    "                p = net.discriminator_forward(local_batch,local_labels)#,local_labels)\n",
    "                if tt%kgan!=kgan-1:\n",
    "                    loss = (criterion(p, 1,landmasks[dom_num])+criterion(phat, 0,landmasks[dom_num]))/2\n",
    "                    logs['train-discriminator-loss'][-1].append(loss.item())\n",
    "                    scheduler_discriminator.optimizer.zero_grad()\n",
    "                    scheduler_generator.optimizer.zero_grad()  \n",
    "                    loss.backward()\n",
    "                    scheduler_discriminator.optimizer.step()\n",
    "                else:\n",
    "                    loss = criterion(phat, 1,landmasks[dom_num])\n",
    "                    logs['train-generator-loss'][-1].append(loss.item())\n",
    "                    scheduler_discriminator.optimizer.zero_grad()\n",
    "                    scheduler_generator.optimizer.zero_grad()  \n",
    "                    loss.backward()\n",
    "                    scheduler_generator.optimizer.step()\n",
    "            tt+=1\n",
    "            if tt%args.disp==0:\n",
    "                if not net.gan:\n",
    "                    print('\\t\\t\\t train-loss: ',str(np.mean(np.array(logs['train-loss'][-1]))),\\\n",
    "                          '\\t ±',\\\n",
    "                          str(np.std(np.array(logs['train-loss'][-1]))),flush=True)\n",
    "                else:\n",
    "                    print('\\t\\t\\t train-generator-loss: ',str(np.mean(np.array(logs['train-generator-loss'][-1]))),\\\n",
    "                          '\\t ±',\\\n",
    "                          str(np.std(np.array(logs['train-generator-loss'][-1]))),flush=True)\n",
    "                    print('\\t\\t\\t train-discriminator-loss: ',str(np.mean(np.array(logs['train-discriminator-loss'][-1]))),\\\n",
    "                          '\\t ±',\\\n",
    "                          str(np.std(np.array(logs['train-discriminator-loss'][-1]))),flush=True)\n",
    "        with torch.set_grad_enabled(False):\n",
    "            if not net.gan:\n",
    "                val_loss=0.\n",
    "            else:\n",
    "                val_loss_d=0.\n",
    "                val_loss_g=0.\n",
    "            num_val=0\n",
    "            for local_batch,dom_num, local_labels in val_generator:\n",
    "                local_batch,local_labels = local_batch.to(device),local_labels.to(device)\n",
    "                if not net.gan:\n",
    "                    outputs = net.forward(local_batch)\n",
    "                    loss = criterion(outputs, local_labels,landmasks[dom_num])\n",
    "                    val_loss+=loss.item()\n",
    "                else:\n",
    "                    shp=torch.tensor(local_batch.shape)\n",
    "                    shp[1]=1\n",
    "                    random_field=torch.randn(shp.tolist()).to(device)\n",
    "                    yhat = net.generator_forward(local_batch,random_field)\n",
    "                    phat = net.discriminator_forward(local_batch,yhat)#,local_labels)\n",
    "                    p = net.discriminator_forward(local_batch,local_labels)#,local_labels)\n",
    "                    loss_g=criterion(phat, 1,landmasks[dom_num])\n",
    "                    loss_d=criterion(p, 1,landmasks[dom_num])+criterion(phat, 0,landmasks[dom_num])\n",
    "                    val_loss_d+=loss_d.item()\n",
    "                    val_loss_g+=loss_g.item()\n",
    "                num_val+=1\n",
    "            if not net.gan:\n",
    "                logs['val-loss'].append(val_loss/num_val)\n",
    "                logs['lr'].append(scheduler.optimizer.param_groups[0]['lr'])\n",
    "                scheduler.step(logs['val-loss'][-1])\n",
    "            else:\n",
    "                logs['val-discriminator-loss'].append(val_loss_d/num_val)\n",
    "                logs['val-generator-loss'].append(val_loss_d/num_val)\n",
    "                \n",
    "                logs['lr-generator'].append(scheduler_generator.optimizer.param_groups[0]['lr'])\n",
    "                scheduler_generator.step(logs['val-generator-loss'][-1])\n",
    "                #logs['val-generator-loss'][-1]=logs['val-generator-loss'][-1]\n",
    "                \n",
    "                logs['lr-discriminator'].append(scheduler_discriminator.optimizer.param_groups[0]['lr'])\n",
    "                scheduler_generator.step(logs['val-discriminator-loss'][-1])\n",
    "                #logs['val-discriminator-loss'][-1]=logs['val-discriminator-loss'][-1]\n",
    "\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            if not net.gan:\n",
    "                val_loss=0.\n",
    "            else:\n",
    "                val_loss_d=0.\n",
    "                val_loss_g=0.\n",
    "            num_val=0\n",
    "            for local_batch,dom_num, local_labels in val_generator:\n",
    "                local_batch,local_labels = local_batch.to(device),local_labels.to(device)\n",
    "                if not net.gan:\n",
    "                    outputs = net.forward(local_batch)\n",
    "                    loss = criterion(outputs, local_labels,landmasks[dom_num])\n",
    "                    val_loss+=loss.item()\n",
    "                else:\n",
    "                    shp=torch.tensor(local_batch.shape)\n",
    "                    shp[1]=1\n",
    "                    random_field=torch.randn(shp.tolist()).to(device)\n",
    "                    yhat = net.generator_forward(local_batch,random_field)\n",
    "                    phat = net.discriminator_forward(local_batch,yhat)#,local_labels)\n",
    "                    p = net.discriminator_forward(local_batch,local_labels)#,local_labels)\n",
    "                    loss_g=criterion(phat, 1,landmasks[dom_num])\n",
    "                    loss_d=criterion(p, 1,landmasks[dom_num])+criterion(phat, 0,landmasks[dom_num])\n",
    "                    val_loss_d+=loss_d.item()\n",
    "                    val_loss_g+=loss_g.item()\n",
    "                num_val+=1\n",
    "            if not net.gan:\n",
    "                logs['test-loss'].append(val_loss/num_val)\n",
    "                logs['batchsize'].append(args.batch)\n",
    "            else:\n",
    "                logs['test-discriminator-loss'].append(val_loss_d/num_val)\n",
    "                logs['test-generator-loss'].append(val_loss_g/num_val)\n",
    "\n",
    "        if len(logs['epoch'])>0:\n",
    "            logs['epoch'].append(logs['epoch'][-1]+1)\n",
    "        else:\n",
    "            logs['epoch'].append(1)\n",
    "        if not net.gan:\n",
    "            print('#epoch ',str(logs['epoch'][-1]),' ',\\\n",
    "                      ' test-loss: ',str(logs['test-loss'][-1]),\\\n",
    "                      ' val-loss: ',str(logs['val-loss'][-1]),\\\n",
    "                      ' train-loss: ',str(np.mean(np.array(logs['train-loss'][-1]))),\\\n",
    "                      ' lr: ',str(logs['lr'][-1]),flush=True)\n",
    "        else:\n",
    "            print('#epoch :',str(logs['epoch'][-1]),' ',\\\n",
    "                      ' (generator) test-loss: ',str(logs['test-generator-loss'][-1]),\\\n",
    "                      ' val-loss: ',str(logs['val-generator-loss'][-1]),\\\n",
    "                      ' train-loss: ',str(np.mean(np.array(logs['train-generator-loss'][-1]))),\\\n",
    "                      ' lr: ',str(logs['lr-generator'][-1]),flush=True)\n",
    "            print('\\t (discriminator):',\\\n",
    "                      ' test-loss: ',str(logs['test-discriminator-loss'][-1]),\\\n",
    "                      ' val-loss: ',str(logs['val-discriminator-loss'][-1]),\\\n",
    "                      ' train-loss: ',str(np.mean(np.array(logs['train-discriminator-loss'][-1]))),\\\n",
    "                      ' lr: ',str(logs['lr-discriminator'][-1]),flush=True)\n",
    "\n",
    "\n",
    "        torch.save(net.state_dict(), PATH0)\n",
    "        with open(LOG, 'w') as outfile:\n",
    "            json.dump(logs, outfile)\n",
    "        if not net.gan:\n",
    "            if np.min(logs['test-loss']) == logs['test-loss'][-1]:\n",
    "                torch.save(net.state_dict(), PATH1)\n",
    "                best_counter=0\n",
    "            else:\n",
    "                best_counter+=1\n",
    "            if logs['lr'][-1]<1e-7:\n",
    "                break\n",
    "        else:\n",
    "            torch.save(net.state_dict(), PATH1)\n",
    "            '''if np.min(logs['test-generator-loss']) == logs['test-generator-loss'][-1]:\n",
    "                torch.save(net.state_dict(), PATH1)\n",
    "                best_counter=0\n",
    "            else:\n",
    "                best_counter+=1'''\n",
    "            if logs['lr-generator'][-1]<1e-7 and logs['lr-discriminator'][-1]<1e-7:\n",
    "                break\n",
    "def grad_probe_features2(uv,uv1,g,yc,xc,listout=False,projection=[],geoflag=True):\n",
    "    if listout:        \n",
    "        '''[y_coords[mid],x_coords[mid],\\\n",
    "           uv5,nuv,duvdt,duvdy,duvdx,duvdyy,duvdxy,duvdxx,\\\n",
    "              g5,r,cl2]'''\n",
    "        names=[]\n",
    "        t=0\n",
    "        names.append(['coords',0,t+2])\n",
    "        t+=2\n",
    "        names.append(['uv5',t,t+2*5**2])\n",
    "        t+=2*5**2\n",
    "        names.append(['nuv',t,t+2])\n",
    "        t+=2\n",
    "        names.append(['duvdt',t,t+2])\n",
    "        t+=2\n",
    "        names.append(['duvdy',t,t+2])\n",
    "        t+=2\n",
    "        names.append(['duvdx',t,t+2])\n",
    "        t+=2\n",
    "        names.append(['duvdyy',t,t+2])\n",
    "        t+=2\n",
    "        names.append(['duvdxy',t,t+2])\n",
    "        t+=2\n",
    "        names.append(['duvdxx',t,t+2])\n",
    "        t+=2\n",
    "        names.append(['g5',t,t+2*5**2])\n",
    "        t+=2*5**2\n",
    "        names.append(['r',t,t+2*11])\n",
    "        t+=2*11\n",
    "        if geoflag:\n",
    "            names.append(['cl2',t,t+6])\n",
    "            t+=6\n",
    "        else:\n",
    "            names.append(['cl2',t,t+4])\n",
    "            t+=4\n",
    "        if len(projection)>0:\n",
    "            names.append(['l-g5',t,t+2*5**2])\n",
    "            t+=2*5**2\n",
    "            names.append(['l-r',t,t+2*11])\n",
    "            t+=2*11\n",
    "            if geoflag:\n",
    "                names.append(['l-cl2',t,t+6])\n",
    "                t+=6\n",
    "            else:\n",
    "                names.append(['l-cl2',t,t+4])\n",
    "                t+=4\n",
    "        return names\n",
    "    \n",
    "    width=len(xc)\n",
    "    mid=(width-1)//2\n",
    "    \n",
    "    uv=torch.reshape(uv,[-1,width,width])\n",
    "    uv1=torch.reshape(uv1,[-1,width,width])\n",
    "    g=torch.reshape(g,[-1,width,width])\n",
    "    nchan=uv.shape[0]\n",
    "    \n",
    "\n",
    "    \n",
    "    uv,_=uv.split([2,uv.shape[0]-2],dim=0)\n",
    "    uv1,_=uv1.split([2,uv1.shape[0]-2],dim=0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    area=(yc[-1]-yc[0])*(xc[-1]-xc[0])\n",
    "    \n",
    "    dyc= yc[1:]-yc[:-1]\n",
    "    dxc= xc[1:]-xc[:-1]\n",
    "    \n",
    "    \n",
    "    ddyc= (dyc[1:] + dyc[:-1])/2\n",
    "    ddxc= (dxc[1:] + dxc[:-1])/2\n",
    "    \n",
    "    dyc=dyc.reshape([1,-1,1])\n",
    "    dxc=dxc.reshape([1,1,-1])\n",
    "    ddyc=ddyc.reshape([1,-1,1])\n",
    "    ddxc=ddxc.reshape([1,1,-1])\n",
    "    \n",
    "    duvdy=(uv[:,:-1]-uv[:,1:])/dyc\n",
    "    duvdx=(uv[:,:,:-1]-uv[:,:,1:])/dxc\n",
    "    \n",
    "    duvdyy=(duvdy[:,:-1]-duvdy[:,1:])/ddyc\n",
    "    duvdxy=(duvdy[:,:,:-1]-duvdy[:,:,1:])/dxc\n",
    "    duvdxx=(duvdx[:,:,:-1]-duvdx[:,:,1:])/ddxc\n",
    "    \n",
    "    duvdy=torch.sum(duvdy**2,dim=[1,2])*area\n",
    "    duvdx=torch.sum(duvdx**2,dim=[1,2])*area\n",
    "    \n",
    "    duvdyy=torch.sum(duvdyy**2,dim=[1,2])*area\n",
    "    duvdxy=torch.sum(duvdxy**2,dim=[1,2])*area\n",
    "    duvdxx=torch.sum(duvdxx**2,dim=[1,2])*area\n",
    "    \n",
    "    duvdt=torch.sum((uv1-uv)**2,dim=[1,2])*area\n",
    "    \n",
    "\n",
    "    nuv=(uv**2).sum(axis=(1,2))*area\n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    uv5=uv[:2,mid-2:mid+3,mid-2:mid+3]\n",
    "    \n",
    "    \n",
    "    ng=(g**2).sum(axis=(1,2),keepdim=True)*area\n",
    "    g5=g[:2,mid-2:mid+3,mid-2:mid+3]\n",
    "    r=torch.zeros(2,mid+1)\n",
    "    r[:,0]=g[:2,mid,mid]**2\n",
    "    \n",
    "    for i in range(1,mid+1):\n",
    "        r[:,i]+=torch.sum(g[:2,mid-i:mid+i,mid+i]**2,dim=1)\n",
    "        r[:,i]+=torch.sum(g[:2,mid+i,mid-i:mid+i]**2,dim=1)\n",
    "        r[:,i]+=torch.sum(g[:2,mid-i:mid+i,mid-i]**2,dim=1)\n",
    "        r[:,i]+=torch.sum(g[:2,mid-i,mid-i:mid+i]**2,dim=1)\n",
    "        r[:,i]=r[:,i]/( (2*i+1)*4-4)\n",
    "    cl2=ng/torch.sqrt(torch.sum(ng**2))\n",
    "    \n",
    "    F=[yc[mid:mid+1],xc[mid:mid+1],\\\n",
    "           uv5,nuv,duvdt,duvdy,duvdx,duvdyy,duvdxy,duvdxx,\\\n",
    "              g5,r,cl2]\n",
    "    \n",
    "    \n",
    "    if len(projection)>0:\n",
    "        g_=(projection@(projection.T@(g[:2].reshape([-1])))).reshape([-1,width,width])\n",
    "        g=torch.cat([g_,g[2:]],axis=0)\n",
    "        ng=(g**2).sum(axis=(1,2),keepdim=True)*area\n",
    "        g5_=g[:2,mid-2:mid+3,mid-2:mid+3]\n",
    "        r_=torch.zeros(2,mid+1)\n",
    "        r_[:,0]=g[:2,mid,mid]**2\n",
    "        for i in range(1,mid+1):\n",
    "            r_[:,i]+=torch.sum(g[:2,mid-i:mid+i,mid+i]**2,dim=1)\n",
    "            r_[:,i]+=torch.sum(g[:2,mid+i,mid-i:mid+i]**2,dim=1)\n",
    "            r_[:,i]+=torch.sum(g[:2,mid-i:mid+i,mid-i]**2,dim=1)\n",
    "            r_[:,i]+=torch.sum(g[:2,mid-i,mid-i:mid+i]**2,dim=1)\n",
    "            r_[:,i]=r[:,i]/( (2*i+1)*4-4)\n",
    "        cl2_=ng/torch.sqrt(torch.sum(ng**2))\n",
    "        F=F+[g5_,r_,cl2_]\n",
    "    F=[f.reshape([-1]) for f in F]\n",
    "    return torch.cat(F,dim=0)\n",
    "def grad_probe_features3(g,yc,xc,inchan,spread,listout=False,geoflag=False):\n",
    "    if listout:        \n",
    "        names=[]\n",
    "        t=0\n",
    "        names.append(['coords',0,t+2])\n",
    "        t+=2\n",
    "        names.append(['r',t,t+inchan*(spread+1)])\n",
    "        t+=inchan*(spread+1)\n",
    "        if geoflag:\n",
    "            names.append(['cl2',t,t+inchan+2])\n",
    "            t+=6\n",
    "        return names\n",
    "    \n",
    "    width=len(xc)\n",
    "    mid=spread\n",
    "    \n",
    "    g=torch.reshape(g,[inchan,width,width])\n",
    "    #nchan=uv.shape[0]\n",
    "    \n",
    "    \n",
    "    r=torch.zeros(inchan,mid+1)\n",
    "    r[:,0]=g[:inchan,mid,mid]**2\n",
    "    \n",
    "    for i in range(1,mid+1):\n",
    "        r[:,i]+=torch.sum(g[:inchan,mid-i:mid+i,mid+i]**2,dim=1)\n",
    "        r[:,i]+=torch.sum(g[:inchan,mid+i,mid-i:mid+i]**2,dim=1)\n",
    "        r[:,i]+=torch.sum(g[:inchan,mid-i:mid+i,mid-i]**2,dim=1)\n",
    "        r[:,i]+=torch.sum(g[:inchan,mid-i,mid-i:mid+i]**2,dim=1)\n",
    "    for i in range(mid+1):\n",
    "        r[:,i]=torch.sum(r[:,i:],dim=1)\n",
    "    if geoflag:   \n",
    "        ng=(g**2).sum(axis=(1,2),keepdim=True)\n",
    "        cl2=ng/torch.sqrt(torch.sum(ng**2))\n",
    "        F=[yc[mid:mid+1],xc[mid:mid+1],\\\n",
    "               r,cl2]\n",
    "    else:\n",
    "        F=[yc[mid:mid+1],xc[mid:mid+1],\\\n",
    "               r]\n",
    "    F=[f.reshape([-1]) for f in F]\n",
    "    return torch.cat(F,dim=0)\n",
    "def grad_probe(args):\n",
    "    net,criterion,(data_init,partition),logs,(PATH0,PATH1,LOG,root)=load_from_save(args)\n",
    "    (training_set,training_generator),(val_set,val_generator),(test_set,test_generator),(dataset,glbl_gen)=load_data(data_init,partition,args)\n",
    "    if isinstance(training_set,climate_data.Dataset2):\n",
    "        landmasks=training_set.get_masks(glbl=True)\n",
    "    else:\n",
    "        landmasks=climate_data.get_land_masks(val_generator)\n",
    "    device=get_device()\n",
    "    MASK=landmasks.to(device)\n",
    "    yc,xc=dataset.coords[0]\n",
    "    xc=torch.tensor(xc)\n",
    "    yc=torch.tensor(yc)\n",
    "\n",
    "    if isinstance(dataset,climate_data.Dataset2):\n",
    "        MASK=MASK[0,0]\n",
    "        MASK[MASK==0]=np.nan\n",
    "        MASK[MASK==MASK]=1\n",
    "        MASK[MASK!=MASK]=0\n",
    "    \n",
    "\n",
    "    device=get_device()\n",
    "    net.eval()\n",
    "    for i in range(len(net.nn_layers)-1):\n",
    "        try:\n",
    "            net.nn_layers[i].weight.requires_grad=False\n",
    "        except:\n",
    "            QQ=1\n",
    "    geoflag=net.freq_coord\n",
    "    inchan=net.nn_layers[0].weight.data.shape[1]#net.initwidth\n",
    "    outchan=net.outwidth\n",
    "    names=grad_probe_features3([],[],[],inchan,net.spread,listout=True)#,geoflag=geoflag)\n",
    "    numprobe=names[-1][-1]\n",
    "\n",
    "\n",
    "    maxsamplecount=1000\n",
    "    samplecount=np.minimum(len(dataset),maxsamplecount)\n",
    "    dt=np.maximum(len(dataset)//samplecount,1)\n",
    "    numbatch=3\n",
    "    width=net.receptive_field\n",
    "\n",
    "    chss=np.arange(maxsamplecount)*dt\n",
    "    np.random.shuffle(chss)\n",
    "    \n",
    "    tot=0\n",
    "    ii=0\n",
    "    dd=0\n",
    "    TOTMAX=np.inf\n",
    "    F=[]\n",
    "    spread=net.spread\n",
    "    MASKK=MASK[:-width,:-width]\n",
    "    stsz=[np.maximum(1,MASKK.shape[i]//70) for i in range(2)]\n",
    "    MASKK=MASKK[::stsz[0],::stsz[1]]\n",
    "    KK,LL=np.where(MASKK>0)\n",
    "    snum=0\n",
    "    dd=0\n",
    "    \n",
    "    samplecount1=20\n",
    "    GRADS=torch.zeros(samplecount1,len(KK),outchan,inchan,width,width)\n",
    "    GS=torch.zeros(samplecount,len(KK),numprobe*outchan)\n",
    "\n",
    "    for i in range(samplecount):\n",
    "        UV,_,S = dataset[chss[i]]\n",
    "        for j in range(len(KK)):\n",
    "            K,L=KK[j]*stsz[0],LL[j]*stsz[1]\n",
    "            for chan in range(outchan):\n",
    "                uv=torch.stack([UV[:,K:K+width,L:L+width]],dim=0).to(device)\n",
    "                uv.requires_grad=True\n",
    "                output=net.forward(uv)\n",
    "                x0=output.shape[2]\n",
    "                x1=output.shape[3]\n",
    "                m0=(x0-1)//2\n",
    "                m1=(x1-1)//2\n",
    "                ou=output[0,chan,m0,m1]\n",
    "                ou.backward(retain_graph=True)\n",
    "                g=uv.grad\n",
    "                uv.grad=None\n",
    "                uv=uv.to(torch.device(\"cpu\")).detach()\n",
    "                g=g.to(torch.device(\"cpu\")).detach()\n",
    "                sample=grad_probe_features3(g,yc[K:K+width],xc[L:L+width],inchan,net.spread)#,geoflag=geoflag)\n",
    "                GS[i, j,chan*len(sample):(chan+1)*len(sample)]=sample\n",
    "                if i<samplecount1:\n",
    "                    GRADS[i,j,chan]=g.reshape([inchan,width,width])\n",
    "            dd+=1\n",
    "        if i%args.disp==0:\n",
    "            print(i,samplecount,flush=True)\n",
    "            np.save(root+'/grad-probe-data.npy', GS[:i+1])\n",
    "            if i<samplecount1:\n",
    "                np.save(root+'/grad-samples.npy', GRADS[:i+1])\n",
    "        if i==samplecount1:\n",
    "            np.save(root+'/grad-samples.npy', GRADS)\n",
    "                \n",
    "\n",
    "\n",
    "    np.save(root+'/grad-probe-data.npy', GS)\n",
    "\n",
    "\n",
    "def grad_analysis(args):\n",
    "    net,criterion,(data_init,partition),logs,(PATH0,PATH1,LOG,root)=load_from_save(args)\n",
    "    _,_,_,(dataset,datagen)=load_data(data_init,partition,args)\n",
    "    \n",
    "    MASK=climate_data.get_land_masks(datagen)[0,0]\n",
    "    device=get_device()\n",
    "    net.eval()\n",
    "    for i in range(len(net.nn_layers)-1):\n",
    "        try:\n",
    "            net.nn_layers[i].weight.requires_grad=False\n",
    "        except:\n",
    "            QQ=1\n",
    "    width=net.receptive_field\n",
    "    spread=net.spread\n",
    "    dx=spread\n",
    "    dy=spread\n",
    "    \n",
    "    W=np.reshape(np.arange(spread+1),[-1,1])\n",
    "    sx=dataset.dimens[1]-width+1\n",
    "    sy=dataset.dimens[0]-width+1\n",
    "    \n",
    "    xx=np.arange(0,sx,dx)\n",
    "    yy=np.arange(0,sy,dy)\n",
    "    nx=len(xx)\n",
    "    ny=len(yy)\n",
    "    \n",
    "    \n",
    "    UV,_,_ = dataset[0]\n",
    "    nchan=UV.shape[0]\n",
    "    G=np.zeros((nchan*3,ny*width, nx*width))\n",
    "    width=net.receptive_field\n",
    "    maxsamplenum=4*width**2+2\n",
    "    CN=np.zeros((4*width**2+2,4*width**2+2))\n",
    "    MS=np.zeros((4*width**2+2,maxsamplenum))\n",
    "    samplecount=0\n",
    "    tot=0\n",
    "    ii=0\n",
    "    dd=0\n",
    "    TOTMAX=np.inf\n",
    "    for i in range(len(dataset)):\n",
    "        UV,_,_ = dataset[i]\n",
    "        #for local_batch,nan_mask, _ in datagen:\n",
    "        #uv, nan_mask = local_batch.to(device),nan_mask.to(device)\n",
    "        for k in range(ny):\n",
    "            for l in range(nx):\n",
    "                dd+=1\n",
    "                K,L=yy[k],xx[l]\n",
    "                if MASK[K,L]>0:  \n",
    "                    uv=torch.stack([UV[:,K:K+width,L:L+width]],dim=0).to(device)\n",
    "                    uv.requires_grad=True\n",
    "                    output=net.forward(uv)\n",
    "                    x0=output.shape[2]\n",
    "                    x1=output.shape[3]\n",
    "                    m0=(x0-1)//2\n",
    "                    m1=(x1-1)//2\n",
    "                    ou=output[0,0,m0,m1]\n",
    "                    ou.backward(retain_graph=True)\n",
    "                    g=uv.grad\n",
    "                    uv.grad=None\n",
    "                    uv=uv.to(torch.device(\"cpu\")).detach().numpy()\n",
    "                    g=g.to(torch.device(\"cpu\")).detach().numpy()\n",
    "                    g=np.reshape(g,[nchan,width,width])\n",
    "                    for j in range(nchan):\n",
    "                        G[j,k*width:(k+1)*width,l*width:(l+1)*width]+=g[j]**2\n",
    "                        fg=np.abs(np.fft.fftshift(np.fft.fft2(g[j])))\n",
    "                        G[j+nchan,k*width:(k+1)*width,l*width:(l+1)*width]+=fg**2\n",
    "                        G[j+2*nchan,k*width:(k+1)*width,l*width:(l+1)*width]+=np.abs(g[j])/np.sum(np.abs(g[j]))\n",
    "                    \n",
    "                    ii+=1\n",
    "                    uv_=np.reshape(uv[0,:2],[-1,1])\n",
    "                    g_=np.reshape(g[:2],[-1,1])\n",
    "                    ou=np.reshape(ou.to(torch.device(\"cpu\")).detach().numpy(),[-1,1])\n",
    "                    vec_=np.concatenate((uv_,ou,g_,np.ones((1,1))),axis=0)\n",
    "                    CN=CN+vec_@vec_.T\n",
    "                    tot+=1\n",
    "                    if samplecount<maxsamplenum:\n",
    "                        if np.random.randint(2, size=1)[0]==0:\n",
    "                            MS[:,samplecount:samplecount+1]=vec_\n",
    "                            samplecount+=1\n",
    "                    if tot>TOTMAX:\n",
    "                        break\n",
    "                else:\n",
    "                    G[:,k*width:(k+1)*width,l*width:(l+1)*width]=np.nan\n",
    "                if dd%1000==0:\n",
    "                    print('\\t\\t '+str(dd/nx/ny),flush=True)\n",
    "            if tot>TOTMAX:\n",
    "                break\n",
    "                \n",
    "        print(tot,flush=True)\n",
    "        with open(root+'/global-grad.npy', 'wb') as f:\n",
    "            np.save(f, G/tot)\n",
    "        with open(root+'/global-grad-covariance.npy', 'wb') as f:\n",
    "            np.save(f, CN/tot)\n",
    "        with open(root+'/global-grad-samples.npy', 'wb') as f:\n",
    "            np.save(f, MS)\n",
    "        if tot>TOTMAX:\n",
    "            break\n",
    "def data_fourier_analysis(args):\n",
    "    # WILL NOT WORK BECAUSE OF CHANGES\n",
    "    net,criterion,logs,PATH0,PATH1,LOG,root=load_from_save(args)\n",
    "\n",
    "    width=41\n",
    "    dx=(width-1)//2\n",
    "    dy=(width-1)//2\n",
    "\n",
    "    net.spread=dx\n",
    "    _,_,_,(dataset,datagen)=load_data(net,args)\n",
    "    MASK=climate_data.get_land_masks(datagen)[0,0]\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    sx=dataset.dimens[1]-2*dx-2*dx\n",
    "    sy=dataset.dimens[0]-2*dx-2*dx\n",
    "    \n",
    "    xx=np.arange(0,sx,dx)\n",
    "    yy=np.arange(0,sy,dy)\n",
    "    nx=len(xx)\n",
    "    ny=len(yy)\n",
    "    \n",
    "    G=np.zeros((8,ny*width, nx*width))\n",
    "    \n",
    "    \n",
    "    tot=0\n",
    "    \n",
    "    for i in range(len(dataset)):\n",
    "        UV,_,SXY = dataset[i]\n",
    "        #for local_batch,nan_mask, _ in datagen:\n",
    "        #uv, nan_mask = local_batch.to(device),nan_mask.to(device)\n",
    "        for k in range(ny):\n",
    "            for l in range(nx):\n",
    "                K,L=yy[k],xx[l]\n",
    "                if MASK[K,L]>0:    \n",
    "                    uv=UV[:,K:K+width,L:L+width].numpy()\n",
    "                    sxy=SXY[:,K:K+width,L:L+width].numpy()\n",
    "                    ff=[np.abs(np.fft.fftshift(np.fft.fft2(uv[oo]))) for oo in range(2)]\n",
    "                    ff=ff+[np.abs(np.fft.fftshift(np.fft.fft2(sxy[oo]))) for oo in range(2)]\n",
    "                    for j in range(len(ff)):\n",
    "                        G[j,k*width:(k+1)*width,l*width:(l+1)*width]+=ff[j]**2\n",
    "                else:\n",
    "                    G[:,k*width:(k+1)*width,l*width:(l+1)*width]=np.nan\n",
    "        tot+=1\n",
    "        print(tot,flush=True)\n",
    "        NG=torch.tensor(G)\n",
    "        NG[:4]=NG[:4]/tot\n",
    "        for k in range(ny):\n",
    "            for l in range(nx):\n",
    "                NN=NG[:4,k*width:(k+1)*width,l*width:(l+1)*width]\n",
    "                MNN=torch.sum(NN,dim=[1,2],keepdim=True)\n",
    "                NN=NN/MNN\n",
    "                NG[4:,k*width:(k+1)*width,l*width:(l+1)*width]=NN\n",
    "        with open('/scratch/cg3306/climate/global-fourier-analysis.npy', 'wb') as f:\n",
    "            np.save(f, NG.numpy()) \n",
    "\n",
    "            \n",
    "def data_covariance(args):\n",
    "    net,criterion,(data_init,partition),logs,(PATH0,PATH1,LOG,root)=load_from_save(args)\n",
    "    _,_,_,(dataset,datagen)=load_data(data_init,partition,args)\n",
    "    \n",
    "    MASK=climate_data.get_land_masks(datagen)[0,0]\n",
    "    device=get_device()\n",
    "    net.eval()\n",
    "    width=net.receptive_field\n",
    "    spread=net.spread\n",
    "    dx=spread\n",
    "    dy=spread\n",
    "    \n",
    "    W=np.reshape(np.arange(spread+1),[-1,1])\n",
    "    sx=dataset.dimens[1]-width+1\n",
    "    sy=dataset.dimens[0]-width+1\n",
    "    \n",
    "    xx=np.arange(0,sx,dx)\n",
    "    yy=np.arange(0,sy,dy)\n",
    "    nx=len(xx)\n",
    "    ny=len(yy)\n",
    "    \n",
    "    \n",
    "    UV,_,_ = dataset[0]\n",
    "    nchan=UV.shape[0]\n",
    "    G=np.zeros((nchan*2,ny*width, nx*width))\n",
    "    width=net.receptive_field\n",
    "    COV=np.zeros((2*width**2+1,2*width**2+1))\n",
    "    samplecount=0\n",
    "    tot=0\n",
    "    TOTMAX=np.inf\n",
    "    for i in range(len(dataset)):\n",
    "        UV,_,_ = dataset[i]\n",
    "        #for local_batch,nan_mask, _ in datagen:\n",
    "        #uv, nan_mask = local_batch.to(device),nan_mask.to(device)\n",
    "        for k in range(ny):\n",
    "            for l in range(nx):\n",
    "                K,L=yy[k],xx[l]\n",
    "                if MASK[K,L]>0:  \n",
    "                    uv=torch.reshape(UV[:2,K:K+width,L:L+width],(1,-1))\n",
    "                    uv=torch.cat([uv,torch.ones(1,1)],dim=1)\n",
    "                    COV=COV+(uv.T@uv).numpy()\n",
    "                    tot+=1\n",
    "                    if tot>TOTMAX:\n",
    "                        break\n",
    "            if tot>TOTMAX:\n",
    "                break\n",
    "        if tot>TOTMAX:\n",
    "            break\n",
    "        if i%10==0:\n",
    "            print('\\t\\t '+str(tot/nx/ny/len(dataset)),flush=True)\n",
    "        with open('/scratch/cg3306/climate/data-covariance.npy', 'wb') as f:\n",
    "            np.save(f, COV/tot) \n",
    "            \n",
    "def projection_analysis(args):\n",
    "    model_id=int(args.model_id)%4\n",
    "    sigma_id=int(args.model_id)//4\n",
    "    sigma_vals=[4,8,12,16]\n",
    "    sigma=sigma_vals[sigma_id]\n",
    "    data_root='/scratch/zanna/data/cm2.6/coarse-'\n",
    "    raw_add=['3D-data-sigma-'+str(sigma)+'.zarr','surf-data-sigma-'+str(sigma)+'.zarr']\n",
    "    raw_co2=['1pct-CO2-'+ss for ss in raw_add]\n",
    "    raw_add=raw_add+raw_co2\n",
    "    raw_data_address=raw_add[model_id]\n",
    "    ds_data=xr.open_zarr(data_root+raw_data_address)\n",
    "    MSELOC='/scratch/cg3306/climate/projection_analysis/'+raw_data_address.replace('.zarr','')+'MSE.npy'\n",
    "    SC2LOC='/scratch/cg3306/climate/projection_analysis/'+raw_data_address.replace('.zarr','')+'SC2.npy'\n",
    "    noutsig=3\n",
    "    names='Su Sv ST'.split()\n",
    "    MSE=torch.zeros(noutsig,ds_data.Su.shape[-2], ds_data.Su.shape[-1])\n",
    "    SC2=torch.zeros(noutsig,ds_data.Su.shape[-2], ds_data.Su.shape[-1])\n",
    "    print(MSELOC)\n",
    "    T=len(ds_data.time.values)\n",
    "    for i in range(T):\n",
    "        uv=ds_data.isel(time=i)\n",
    "        for j in range(len(names)):\n",
    "            Sxy=uv[names[j]].values\n",
    "            output=Sxy-uv[names[j]+'_r'].values\n",
    "            SC2[j]+=Sxy**2\n",
    "            MSE[j]+=(Sxy-output)**2\n",
    "        if i%50==0:\n",
    "            with open(MSELOC, 'wb') as f:\n",
    "                np.save(f, MSE/(i+1))\n",
    "            with open(SC2LOC, 'wb') as f:\n",
    "                np.save(f, SC2/(i+1))\n",
    "            print('\\t #'+str(i),flush=True)\n",
    "    with open(MSELOC, 'wb') as f:\n",
    "        np.save(f, MSE/(i+1))\n",
    "    with open(SC2LOC, 'wb') as f:\n",
    "        np.save(f, SC2/(i+1))\n",
    "def global_averages():\n",
    "    DIR='/scratch/zanna/data/cm2.6/'\n",
    "    roots=['3D-data',\\\n",
    "           'surf-data']\n",
    "    roots=roots+['1pct-CO2-'+r for r in roots]\n",
    "    roots=['coarse-'+r for r in roots]\n",
    "\n",
    "    saveloc='/scratch/cg3306/climate/portrays/'\n",
    "    sigmavals=[4,8,12,16]\n",
    "    varnames=['Su','Sv','ST']\n",
    "    save_buff=500\n",
    "    maxtime=-1\n",
    "    varnames=varnames+[var+'_r' for var in varnames]\n",
    "    for file in roots:\n",
    "        for i in range(len(sigmavals)):\n",
    "            fname=file+'-sigma-'+str(sigmavals[i])\n",
    "            print(fname)\n",
    "            ds_zarr=xr.open_zarr(DIR+fname+'.zarr')\n",
    "            ny=len(ds_zarr.yu_ocean)\n",
    "            nx=len(ds_zarr.xu_ocean)\n",
    "            X=np.zeros((len(varnames),ny,nx))\n",
    "            if maxtime>0:\n",
    "                T=np.minimum(len(ds_zarr.time),maxtime)\n",
    "            else:\n",
    "                T=len(ds_zarr.time)\n",
    "            for t in range(T):\n",
    "                ds=ds_zarr.isel(time=t)\n",
    "                for j in range(len(varnames)):\n",
    "                    X[j]+=ds[varnames[j]].values**2\n",
    "                if t%save_buff==0 or t==T-1:\n",
    "                    print('\\t\\t'+str(t),flush=True)\n",
    "                    np.save(saveloc+fname,X/(t+1))\n",
    "def linear_model_fit(root):\n",
    "    filenames=['X2','XY','Y2']\n",
    "    tags=['train','val','test']\n",
    "    D=[]\n",
    "    for i in range(len(tags)):\n",
    "        D.append([])\n",
    "        for j in range(len(filenames)):\n",
    "            D[-1].append(np.load(root+'/'+filenames[j]+'-'+tags[i]+'.npy'))\n",
    "    lmbds=10**np.linspace(-8,1,num=100)\n",
    "    errs=np.zeros((len(lmbds),3))\n",
    "    xmean=np.mean(np.abs(D[0][0]))\n",
    "    for i in range(len(lmbds)):\n",
    "        lmbd=lmbds[i]\n",
    "        w_=np.linalg.solve(D[0][0]+lmbd*xmean*np.eye(D[0][0].shape[0]),D[0][1])\n",
    "        for j in range(w_.shape[1]):\n",
    "            w=w_[:,j]\n",
    "            errs[i,j]=w.T@(D[1][0]@w)-2*D[1][1][:,j]@w+D[1][2][j]\n",
    "    I=np.argmin(errs,axis=0)\n",
    "    w=np.zeros((len(w),3))\n",
    "    for i in range(3):\n",
    "        lmbd=lmbds[I[i]]\n",
    "        w_=np.linalg.solve(D[0][0]+lmbd*xmean*np.eye(D[0][0].shape[0]),D[0][1])\n",
    "        w[:,i]=w_[:,i]\n",
    "    return w \n",
    "def binned_r2_analysis(args,save=True):\n",
    "    net,criterion,(data_init,partition),logs,(PATH0,PATH1,LOG,root)=load_from_save(args)\n",
    "    (training_set,training_generator),(val_set,val_generator),(test_set,test_generator),(dataset,glbl_gen)=load_data(data_init,partition,args)\n",
    "    residue_flag=dataset.outputs[0]=='Su_r'\n",
    "    if residue_flag:\n",
    "        numoutput=len(dataset.outputs)\n",
    "        dataset.outputs+=['Su','Sv','ST']\n",
    "        dataset.outscales=dataset.outscales*2\n",
    "    if isinstance(net, climate_models.RegressionModel):\n",
    "        w=linear_model_fit(root)\n",
    "    device=get_device()\n",
    "    net.eval()\n",
    "\n",
    "    MSELOC=root+'/binned-mse.npy'\n",
    "    MSERESLOC=root+'/binned-mse-res.npy'\n",
    "    SC2LOC=root+'/binned-sc2.npy'\n",
    "    FREQLOC=root+'/binned-freq.npy'\n",
    "    EDGESLOC=root+'/binned-edges.npy'\n",
    "    EQEDGESLOC=root+'/binned-equid-edges.npy'\n",
    "    \n",
    "    LOCS=[MSELOC,MSERESLOC,SC2LOC,FREQLOC,EDGESLOC,EQEDGESLOC]\n",
    "    if args.co2:\n",
    "        LOCS=[LL.replace('.npy','-co2.npy') for LL in LOCS]\n",
    "            \n",
    "    print(root)\n",
    "    \n",
    "    spread=net.spread\n",
    "    noutsig=net.outwidth\n",
    "\n",
    "\n",
    "\n",
    "    if args.depth:\n",
    "        LOCS=[LL.replace('.npy','-depth.npy') for LL in LOCS]\n",
    "        numdepths=np.maximum(len(training_set.depthvals),1)\n",
    "    else:\n",
    "        numdepths=1\n",
    "        \n",
    "    MSELOC,MSERESLOC,SC2LOC,FREQLOC,EDGESLOC,EQEDGESLOC=LOCS\n",
    "    print(' doing binned R2 analysis')\n",
    "\n",
    "    nedges=101\n",
    "\n",
    "    num_init_samples=30\n",
    "    pooler=nn.MaxPool2d(2*dataset.mask_spread+1,stride=1)\n",
    "\n",
    "    arr=np.arange(len(dataset))\n",
    "    np.random.shuffle(arr)\n",
    "\n",
    "    uv,Sxy=dataset.input_output(arr[0],periodic_lon_expand=True)\n",
    "\n",
    "    def gen_mask(uv,pooler):\n",
    "        if len(uv.shape)==3:\n",
    "            MASK=uv[:1]*1\n",
    "        elif len(uv.shape)==2:\n",
    "            MASK=uv*1\n",
    "        MASK[MASK==MASK]=0\n",
    "        MASK[MASK!=MASK]=1\n",
    "        MASK=1-pooler(MASK)\n",
    "        return MASK[0]\n",
    "\n",
    "    MASK=gen_mask(uv,pooler)\n",
    "    sp_extend=np.int64(torch.sum(MASK).item())\n",
    "    SU=torch.zeros(numdepths,noutsig,sp_extend*num_init_samples)\n",
    "\n",
    "    for di in range(num_init_samples*numdepths):\n",
    "        i=di%num_init_samples\n",
    "        depthind=di//num_init_samples\n",
    "        if args.depth:\n",
    "            dataset.depthind=depthind\n",
    "            uv,Sxy=dataset.input_output(arr[i],scale=False,periodic_lon_expand=True)\n",
    "            if i==0:\n",
    "                training_set.depthind=depthind\n",
    "                insc,outsc=training_set.compute_scales()\n",
    "                insc,outsc=np.reshape(insc,[-1,1,1]),np.reshape(outsc,[-1,1,1])\n",
    "                if residue_flag:\n",
    "                    outsc=np.concatenate([outsc,outsc],axis=0)\n",
    "                MASK=gen_mask(uv,pooler)\n",
    "        else:\n",
    "            uv,Sxy=dataset.input_output(arr[i],scale=False,periodic_lon_expand=True)\n",
    "        if residue_flag:\n",
    "            _,Sxy=torch.split(Sxy,[3,3],dim=0)\n",
    "        Sxy=Sxy[:,MASK==1]\n",
    "        SU[depthind,:,i*sp_extend:(i+1)*sp_extend]=Sxy\n",
    "\n",
    "    ASU=torch.abs(SU)\n",
    "    SSU,_=torch.sort(ASU,dim=2)\n",
    "    I=np.arange(0,nedges)*((sp_extend*num_init_samples)//nedges)\n",
    "    #I=np.concatenate([I,[(sp_extend*num_init_samples)-1]])\n",
    "    I=torch.tensor(I)\n",
    "    edges1=SSU[:,:,I]\n",
    "    medges1=(edges1[:,:,1:]+edges1[:,:,:-1])/2\n",
    "\n",
    "    lastel=edges1[:,:,-1]\n",
    "    edges=torch.zeros(numdepths,noutsig,nedges)\n",
    "    for i in range(edges1.shape[0]):\n",
    "        for j in range(edges1.shape[1]):\n",
    "            edges[i,j,:]=torch.linspace(0,lastel[i,j],nedges)\n",
    "\n",
    "    def update_subroutine(SIG,DSIG,MSE,S2,FREQ,edges,edges1,depthind,only_mse_update=False):\n",
    "        ASIG=torch.abs(SIG)\n",
    "        i=depthind\n",
    "        for j in range(edges.shape[1]):\n",
    "            for k in range(edges.shape[2]-1):\n",
    "                e1=edges1[i,j,k]\n",
    "                e2=edges1[i,j,k+1]\n",
    "                I=(ASIG[i,j]>=e1)*(ASIG[i,j]<=e2)\n",
    "                MSE[i,j,k]+=torch.sum(DSIG[i,j,I]**2)\n",
    "                if not only_mse_update:\n",
    "                    S2[i,j,k]+=torch.sum(SIG[i,j,I]**2)\n",
    "                    e1=edges[i,j,k]\n",
    "                    e2=edges[i,j,k+1]\n",
    "                    I=(ASIG[i,j]>=e1)*(ASIG[i,j]<=e2)\n",
    "                    FREQ[i,j,k]+=torch.sum(I)\n",
    "        return MSE,S2,FREQ\n",
    "\n",
    "    MSE=torch.zeros(numdepths,noutsig,nedges-1)\n",
    "    S2=MSE*1\n",
    "    FREQ=torch.zeros(numdepths,noutsig,nedges-1)\n",
    "    \n",
    "    if save:\n",
    "        with open(EDGESLOC, 'wb') as f:\n",
    "            np.save(f,edges.numpy())\n",
    "        with open(EQEDGESLOC, 'wb') as f:\n",
    "            np.save(f,edges1.numpy())\n",
    "    \n",
    "\n",
    "    if residue_flag:\n",
    "        MSE_RES=MSE*1\n",
    "\n",
    "    for di in range(len(dataset)*numdepths):\n",
    "        i=di%len(dataset)\n",
    "        depthind=di//len(dataset)\n",
    "        if args.depth:\n",
    "            dataset.depthind=depthind\n",
    "            uv,Sxy=dataset.input_output(arr[i],scale=False,periodic_lon_expand=True)\n",
    "            if i==0:\n",
    "                training_set.depthind=depthind\n",
    "                insc,outsc=training_set.compute_scales()\n",
    "                insc,outsc=np.reshape(insc,[-1,1,1]),np.reshape(outsc,[-1,1,1])\n",
    "                if residue_flag:\n",
    "                    outsc=np.concatenate([outsc,outsc],axis=0)\n",
    "                MASK=gen_mask(uv,pooler)\n",
    "            uv=uv/insc\n",
    "            Sxy=Sxy/outsc\n",
    "            Sxy[Sxy!=Sxy]=0\n",
    "            uv[uv!=uv]=0\n",
    "        else:\n",
    "            outsc=np.array(dataset.outscales).reshape([-1,1,1])\n",
    "            uv,Sxy=dataset.input_output(arr[i],scale=True,periodic_lon_expand=True)\n",
    "            Sxy[Sxy!=Sxy]=0\n",
    "            uv[uv!=uv]=0\n",
    "        if residue_flag:\n",
    "            Sxy1,Sxy=torch.split(Sxy,[numoutput,numoutput],dim=0)\n",
    "        uv=torch.stack([uv]).to(device)\n",
    "        with torch.set_grad_enabled(False):\n",
    "            if isinstance(net, climate_models.RegressionModel):\n",
    "                output=net.forward(uv,w)\n",
    "            else:\n",
    "                output=net.forward(uv)\n",
    "        output=output[0].to(torch.device(\"cpu\"))\n",
    "        output,prec,_=torch.split(output,[noutsig,net.nprecision,output.shape[0]-noutsig-net.nprecision],dim=0)\n",
    "        output=output[:,MASK==1]\n",
    "        Sxy=Sxy[:,MASK==1]\n",
    "        if not residue_flag:\n",
    "            MSE,S2,FREQ=update_subroutine(Sxy*outsc,(Sxy-output)*outsc,MSE,S2,FREQ,edges,edges1,depthind)\n",
    "        else:\n",
    "            Sxy1=Sxy1[:,MASK==1]\n",
    "            MSE,S2,FREQ=update_subroutine(Sxy*outsc,(Sxy1-output)*outsc,MSE,S2,FREQ,edges,edges1,depthind)\n",
    "            MSE_RES,_,_=update_subroutine(Sxy*outsc,Sxy1*outsc,MSE_RES,S2,FREQ,edges,edges1,depthind,only_mse_update=True)\n",
    "        if di%10==0:\n",
    "                MSE_=MSE.numpy()\n",
    "                MSE_[:depthind]=MSE_[:depthind]/len(dataset)\n",
    "                MSE_[depthind]=MSE_[depthind]/(i+1)\n",
    "                SC2_=S2.numpy()\n",
    "                SC2_[:depthind]=SC2_[:depthind]/len(dataset)\n",
    "                SC2_[depthind]=SC2_[depthind]/(i+1)\n",
    "                if save:\n",
    "                    with open(MSELOC, 'wb') as f:\n",
    "                        np.save(f, MSE_)\n",
    "                    with open(SC2LOC, 'wb') as f:\n",
    "                        np.save(f, SC2_)\n",
    "                    with open(FREQLOC, 'wb') as f:\n",
    "                        np.save(f, FREQ.numpy())\n",
    "                if residue_flag:\n",
    "                    MSE_=MSE_RES.numpy()\n",
    "                    MSE_[:depthind]=MSE_[:depthind]/len(dataset)\n",
    "                    MSE_[depthind]=MSE_[depthind]/(i+1)\n",
    "                    if save:\n",
    "                        with open(MSERESLOC, 'wb') as f:\n",
    "                            np.save(f, MSE_)\n",
    "                if not args.depth:\n",
    "                    print('\\t #'+str(i),flush=True)\n",
    "                else:\n",
    "                    print('\\t depth# '+str(depthind)+', time# '+str(i),flush=True)\n",
    "                \n",
    "    MSE_=MSE.numpy()\n",
    "    MSE_[:depthind]=MSE_[:depthind]/len(dataset)\n",
    "    MSE_[depthind]=MSE_[depthind]/(i+1)\n",
    "    SC2_[:depthind]=SC2_[:depthind]/len(dataset)\n",
    "    SC2_[depthind]=SC2_[depthind]/(i+1)\n",
    "    if save:\n",
    "        with open(MSELOC, 'wb') as f:\n",
    "            np.save(f, MSE_)\n",
    "        with open(SC2LOC, 'wb') as f:\n",
    "            np.save(f, SC2_)\n",
    "        with open(FREQLOC, 'wb') as f:\n",
    "            np.save(f, FREQ.numpy())\n",
    "        if residue_flag:\n",
    "            MSE_=MSE_RES.numpy()\n",
    "            MSE_[:depthind]=MSE_[:depthind]/len(dataset)\n",
    "            MSE_[depthind]=MSE_[depthind]/(i+1)\n",
    "            if save:\n",
    "                with open(MSERESLOC, 'wb') as f:\n",
    "                    np.save(f, MSE_)\n",
    "    print('analysis is done',flush=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89fa1ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis(args):\n",
    "    net,criterion,(data_init,partition),logs,(PATH0,PATH1,LOG,root)=load_from_save(args)\n",
    "    (training_set,training_generator),(val_set,val_generator),(test_set,test_generator),(dataset,glbl_gen)=load_data(data_init,partition,args)\n",
    "    residue_flag=dataset.outputs[0]=='Su_r'\n",
    "    if residue_flag:\n",
    "        numoutputs=len(dataset.outputs)\n",
    "        dataset.outputs+=[ss.replace('_r','') for ss in dataset.outputs]\n",
    "        dataset.outscales=dataset.outscales*2\n",
    "    if isinstance(net, climate_models.RegressionModel):\n",
    "        w=linear_model_fit(root)\n",
    "    device=get_device()\n",
    "    net.eval()\n",
    "    if args.co2:\n",
    "        MSELOC=root+'/MSE-co2.npy'\n",
    "        SC2LOC=root+'/SC2-co2.npy'\n",
    "    else:\n",
    "        MSELOC=root+'/MSE.npy'\n",
    "        SC2LOC=root+'/SC2.npy'\n",
    "\n",
    "    spread=net.spread\n",
    "    noutsig=net.outwidth\n",
    "\n",
    "\n",
    "\n",
    "    if args.depth:\n",
    "        MSELOC=MSELOC.replace('.npy','-depth.npy')\n",
    "        SC2LOC=SC2LOC.replace('.npy','-depth.npy')\n",
    "        numdepths=len(training_set.depthvals)\n",
    "    else:\n",
    "        numdepths=1\n",
    "    recfield=0\n",
    "    if not dataset.padded:\n",
    "        recfield=2*spread\n",
    "    MSE=torch.zeros(numdepths,noutsig,dataset.dimens[0]-recfield, dataset.dimens[1]-recfield)\n",
    "    #LIKE=torch.zeros(noutsig,dataset.dimens[0]-spread*2, dataset.dimens[1]-spread*2)\n",
    "    SC2=torch.zeros(numdepths,noutsig,dataset.dimens[0]-recfield, dataset.dimens[1]-recfield)\n",
    "    print(MSELOC)\n",
    "    arr=np.arange(len(dataset))\n",
    "    np.random.shuffle(arr)\n",
    "\n",
    "    for di in range(len(dataset)*numdepths):\n",
    "        i=di%len(dataset)\n",
    "        depthind=di//len(dataset)\n",
    "        if args.depth:\n",
    "            dataset.depthind=depthind\n",
    "            uv,Sxy=dataset.input_output(arr[i],scale=False,periodic_lon_expand=True)\n",
    "            if i==0:\n",
    "                training_set.depthind=depthind\n",
    "                insc,outsc=training_set.compute_scales()\n",
    "                insc,outsc=np.reshape(insc,[-1,1,1]),np.reshape(outsc,[-1,1,1])\n",
    "                if residue_flag:\n",
    "                    outsc=np.concatenate([outsc,outsc],axis=0)\n",
    "            uv[:insc.shape[0]]=uv[:insc.shape[0]]/insc\n",
    "            Sxy[:outsc.shape[0]]=Sxy[:outsc.shape[0]]/outsc\n",
    "            uv[uv!=uv]=0\n",
    "            Sxy[Sxy!=Sxy]=0\n",
    "            uv,Sxy=dataset.pad_with_zero(uv,0),dataset.pad_with_zero(Sxy,dataset.spread)\n",
    "        else:\n",
    "            uv,_,Sxy=dataset[arr[i]]\n",
    "        if residue_flag:\n",
    "            Sxy1,Sxy=torch.split(Sxy,[numoutputs,numoutputs],dim=0)\n",
    "        uv=torch.stack([uv]).to(device)\n",
    "        #net.set_coarsening(0)\n",
    "        with torch.set_grad_enabled(False):\n",
    "            if isinstance(net, climate_models.RegressionModel):\n",
    "                output=net.forward(uv,w)\n",
    "            else:\n",
    "                output=net.forward(uv)\n",
    "        output=output[0].to(torch.device(\"cpu\"))\n",
    "        output,prec,_=torch.split(output,[noutsig,net.nprecision,output.shape[0]-noutsig-net.nprecision],dim=0)\n",
    "\n",
    "\n",
    "        SC2[depthind]=SC2[depthind] + Sxy**2\n",
    "        if residue_flag:\n",
    "            MSE[depthind]=MSE[depthind] + (Sxy1-output)**2\n",
    "        else:\n",
    "            MSE[depthind]=MSE[depthind] + (Sxy-output)**2\n",
    "        if di%10==0:\n",
    "            MSE_=MSE.numpy()/(i+1)\n",
    "            SC2_=SC2.numpy()/(i+1)\n",
    "            \n",
    "            with open(MSELOC, 'wb') as f:\n",
    "                np.save(f, MSE_)\n",
    "            with open(SC2LOC, 'wb') as f:\n",
    "                np.save(f, SC2_)\n",
    "            if not args.depth:\n",
    "                print('\\t #'+str(i),flush=True)\n",
    "            else:\n",
    "                print('\\t depth# '+str(depthind)+', time# '+str(i),flush=True)\n",
    "                \n",
    "    MSE_=MSE.numpy()/len(dataset)\n",
    "    SC2_=SC2.numpy()/len(dataset)\n",
    "\n",
    "    with open(MSELOC, 'wb') as f:\n",
    "        np.save(f, MSE_)\n",
    "    with open(SC2LOC, 'wb') as f:\n",
    "        np.save(f, SC2_)\n",
    "    print('analysis is done',flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b4163c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_geo_analysis(args):    \n",
    "    net,criterion,(data_init,partition),logs,(PATH0,PATH1,LOG,root)=load_from_save(args)\n",
    "    _,_,_,(dataset,datagen)=load_data(data_init,partition,args)\n",
    "    \n",
    "    MASK=climate_data.get_land_masks(datagen)[0,0]\n",
    "    ymax=MASK.shape[0]\n",
    "    xmax=MASK.shape[1]\n",
    "    xx=torch.linspace(20,xmax-20,10,dtype=torch.long)\n",
    "    yy=torch.linspace(20,ymax-20,10,dtype=torch.long)\n",
    "    MM=MASK[yy,:]\n",
    "    MM=MM[:,xx]\n",
    "    ycord,xcord=np.where(MM>0)\n",
    "    ycord=yy[ycord]\n",
    "    xcord=xx[xcord]\n",
    "    PTS=[[ycord[i].item(),xcord[i].item()] for i in range(len(xcord))]\n",
    "    PTS=np.array(PTS)\n",
    "    device=get_device()\n",
    "\n",
    "    net.eval()\n",
    "\n",
    "    R2LOC=root+'/R2-shift.npy'\n",
    "    PTSLOC=root+'/PTS-shift.npy'\n",
    "    spread=net.spread\n",
    "    mm=(2*spread+1)\n",
    "    MSE=torch.zeros(len(PTS),(dataset.dimens[0]-spread*2)//mm+1, (dataset.dimens[1]-spread*2)//mm+1)\n",
    "    SC2=torch.zeros(len(PTS),(dataset.dimens[0]-spread*2)//mm+1, (dataset.dimens[1]-spread*2)//mm+1)\n",
    "    coarsening=net.init_coarsen>0\n",
    "    arr=np.arange(len(dataset))\n",
    "    np.random.shuffle(arr)\n",
    "\n",
    "    dd=0\n",
    "    PTSMAP=MSE*0\n",
    "    for ppi in range(len(PTS)):\n",
    "        pp=PTS[ppi]\n",
    "        PTSMAP[ppi,pp[0]//mm,pp[1]//mm]=1\n",
    "        for i in range(len(arr)):\n",
    "            uv,_,Sxy=dataset[arr[i]]\n",
    "            geo=uv[2:]\n",
    "            uv=uv[:2]\n",
    "            UV=uv[:,pp[0]:pp[0]+mm,pp[1]:pp[1]+mm]\n",
    "\n",
    "            #uv_=uv[:,pp[0]%mm:,pp[1]%mm:]\n",
    "            uv=torch.roll(uv,shifts=(-(pp[0]%mm),-(pp[1]%mm)),dims=(1,2))\n",
    "            geo=torch.roll(geo,shifts=(-(pp[0]%mm),-(pp[1]%mm)),dims=(1,2))\n",
    "            for t in range(uv.shape[1]//mm):\n",
    "                for tt in range(uv.shape[2]//mm):\n",
    "                    uv[:,t*mm:(t+1)*mm,tt*mm:(tt+1)*mm]=UV\n",
    "            uv=torch.cat([uv,geo],dim=0)\n",
    "            Sxy=Sxy[:,pp[0],pp[1]].view(-1,1,1)\n",
    "            uv=torch.stack([uv]).to(device)\n",
    "            with torch.set_grad_enabled(False):\n",
    "                outschedulerput=net.forward(uv)\n",
    "            output=output[0].to(torch.device(\"cpu\"))\n",
    "            output,_=torch.split(output,[2,output.shape[0]-2],dim=0)\n",
    "            output=output[:,::mm,::mm]\n",
    "            SC2[ppi,:output.shape[1],:output.shape[2]]+= torch.sum((Sxy-output*0)**2,dim=0)\n",
    "            MSE[ppi,:output.shape[1],:output.shape[2]]+= torch.sum((Sxy-output)**2,dim=0)\n",
    "            dd+=1\n",
    "            if dd%300==0:\n",
    "                print(ppi,i)\n",
    "                R2=1-MSE/SC2\n",
    "                with open(R2LOC, 'wb') as f:\n",
    "                    np.save(f, R2.numpy())\n",
    "                with open(PTSLOC, 'wb') as f:\n",
    "                    np.save(f, PTSMAP.numpy())\n",
    "    R2=1-MSE/SC2\n",
    "    with open(R2LOC, 'wb') as f:\n",
    "        np.save(f, R2.numpy())\n",
    "    with open(PTSLOC, 'wb') as f:\n",
    "        np.save(f, PTSMAP.numpy())\n",
    "\n",
    "def save_scales():\n",
    "    sigmas=[2,4,6,8,12,16]\n",
    "    scales=np.zeros((len(sigmas),3))\n",
    "    M=50\n",
    "    \n",
    "    for i in range(len(sigmas)):\n",
    "        sigma=sigmas[i]\n",
    "        ds_data=xr.open_zarr('/scratch/cg3306/climate/data-read/data/sigma-'+str(sigma)+'-data.zarr')\n",
    "        scales[i,0]=sigma\n",
    "        usc=0\n",
    "        uss=0\n",
    "        Ts=np.random.random_integers(0,len(ds_data.time)-1,M)\n",
    "        give_sc=lambda AA: np.mean(np.abs(AA[AA==AA]))\n",
    "        for j in Ts:\n",
    "            dsi=ds_data.isel(time=np.arange(j,j+1))\n",
    "            u,v,S_x,S_y=dsi.usurf.values[0],dsi.vsurf.values[0],dsi.S_x.values[0],dsi.S_y.values[0]\n",
    "            u=np.stack([u,v],axis=0)\n",
    "            S=np.stack([S_x,S_y],axis=0)\n",
    "            usc=np.maximum(usc,give_sc(u))\n",
    "            uss=np.maximum(uss,give_sc(S))\n",
    "        #usc=usc/M\n",
    "        #uss=uss/M\n",
    "        scales[i,1]=usc\n",
    "        scales[i,2]=uss\n",
    "        print(scales[i])\n",
    "    np.save('/scratch/cg3306/climate/climate_research/scales.npy',scales)\n",
    "def ismember_(a,b):\n",
    "    for a_ in b:\n",
    "        if a==a_:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def quadratic_fit(args):\n",
    "    args.batch=1\n",
    "    net,criterion,(data_init,partition),logs,(PATH0,PATH1,LOG,root)=load_from_save(args)\n",
    "    \n",
    "    (training_set,training_generator),(val_set,val_generator),(test_set,test_generator),(glbl_set,glbl_gen)=load_data(data_init,partition,args)\n",
    "    landmasks=climate_data.get_land_masks(val_generator)\n",
    "    M=np.load('/scratch/cg3306/climate/runs/'+str(args.model_bank_id)+'-'+str(args.model_id)+'/quadratic_M.npy')\n",
    "    M=torch.tensor(M).type(torch.float)\n",
    "    Mfit=M*0\n",
    "    landmasks=landmasks.to(device)\n",
    "    landmasks.requires_grad=False\n",
    "    max_epochs = args.epoch\n",
    "    if len(logs['lr'])==0:\n",
    "        optimizer = optim.SGD(Mfit, lr=args.lr, momentum=0.9)\n",
    "    else:\n",
    "        optimizer = optim.SGD(Mfit, lr=logs['lr'][-1], momentum=0.9)\n",
    "        \n",
    "    scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor=0.5,patience=5)\n",
    "    \n",
    "    best_counter=0\n",
    "    print(\"epochs started\")\n",
    "    for epoch in range(max_epochs):\n",
    "        logs['train-loss'].append([])\n",
    "        #for local_batch,nan_mask, local_labels in training_generator:\n",
    "        tt=0\n",
    "        for local_batch,dom_num, _ in training_generator:\n",
    "            local_batch, local_labels = local_batch.to(device),local_labels.to(device)\n",
    "            outputs = net.forward(local_batch)\n",
    "            if not net.generative:\n",
    "                loss = criterion(outputs, local_labels,landmasks[dom_num])\n",
    "            else:\n",
    "                loss = torch.mean((local_batch-outputs)**2)\n",
    "            logs['train-loss'][-1].append(loss.item())\n",
    "            scheduler.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            scheduler.optimizer.step()\n",
    "            tt+=1\n",
    "            if tt%args.disp==0:\n",
    "                print('\\t\\t\\t train-loss: ',str(np.mean(np.array(logs['train-loss'][-1]))),\\\n",
    "                      '\\t ±',\\\n",
    "                      str(np.std(np.array(logs['train-loss'][-1]))),flush=True)\n",
    "        with torch.set_grad_enabled(False):\n",
    "            val_loss=0.\n",
    "            num_val=0\n",
    "            for local_batch,dom_num, ldeocal_labels in val_generator:\n",
    "                local_batch,local_labels = local_batch.to(device),local_labels.to(device)\n",
    "                #print('val: '+str(local_batch.shape))\n",
    "                outputs = net.forward(local_batch)\n",
    "                if not net.generative:\n",
    "                    loss = criterion(outputs, local_labels,landmasks[dom_num])\n",
    "                else:\n",
    "                    loss = torch.mean((local_batch-outputs)**2)\n",
    "                val_loss+=loss.item()\n",
    "                num_val+=1\n",
    "            logs['val-loss'].append(val_loss/num_val)\n",
    "        logs['lr'].append(scheduler.optimizer.param_groups[0]['lr'])\n",
    "        scheduler.step(logs['val-loss'][-1])\n",
    "        with torch.set_grad_enabled(False):\n",
    "            val_loss=0.\n",
    "            num_val=0\n",
    "            for local_batch,dom_num, local_labels in test_generator:\n",
    "                local_batch,local_labels = local_batch.to(device), local_labels.to(device)\n",
    "                outputs = net.forward(local_batch)\n",
    "                if not net.generative:\n",
    "                    loss = criterion(outputs, local_labels,landmasks[dom_num])\n",
    "                else:\n",
    "                    loss = torch.mean((local_batch-outputs)**2)\n",
    "                val_loss+=loss.item()\n",
    "                num_val+=1\n",
    "            logs['test-loss'].append(val_loss/num_val)\n",
    "            logs['batchsize'].append(args.batch)\n",
    "        \n",
    "        if len(logs['epoch'])>0:\n",
    "            logs['epoch'].append(logs['epoch'][-1]+1)\n",
    "        else:\n",
    "            logs['epoch'].append(1)\n",
    "    \n",
    "        print('#epoch ',str(logs['epoch'][-1]),' ',\\\n",
    "                  ' test-loss: ',str(logs['test-loss'][-1]),\\\n",
    "                  ' val-loss: ',str(logs['val-loss'][-1]),\\\n",
    "                  ' train-loss: ',str(np.mean(np.array(logs['train-loss'][-1]))),\\\n",
    "                  ' lr: ',str(logs['lr'][-1]),flush=True)\n",
    "        \n",
    "        \n",
    "        torch.save(net.state_dict(), PATH0)\n",
    "        with open(LOG, 'w') as outfile:\n",
    "            json.dump(logs, outfile)\n",
    "        if np.min(logs['test-loss']) == logs['test-loss'][-1]:\n",
    "            torch.save(net.state_dict(), PATH1)\n",
    "            best_counter=0\n",
    "        else:\n",
    "            best_counter+=1\n",
    "        if logs['lr'][-1]<1e-7:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8206b500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'args=options(string_input=    \"--b 2 -e 4 --nworkers 10 --subtime 0.1 --lr 0.01 --model_id 0 --model_bank_id 4\".split())'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''args=options(string_input=\\\n",
    "    \"--b 2 -e 4 --nworkers 10 --subtime 0.1 --lr 0.01 --model_id 0 --model_bank_id 4\".split())'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48ffb059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_analysis(args):\n",
    "        net,criterion,logs,PATH0,PATH1,LOG,root=load_from_save(args)\n",
    "        net.set_coarsening(0)\n",
    "        (_,training_generator),(_,_),(_,test_generator),_=load_data(net,args)\n",
    "        device=get_device()\n",
    "        net.eval()\n",
    "        logs['total-err']=np.zeros(2)\n",
    "        tot=0\n",
    " \n",
    "        for local_batch,nan_mask, local_labels in training_generator:\n",
    "            local_batch, nan_mask, local_labels = local_batch.to(device),nan_mask.to(device), local_labels.to(device)\n",
    "            with torch.set_grad_enabled(False):\n",
    "                outputs = net.forward(local_batch)*nan_mask\n",
    "            outputs,_=torch.split(outputs,[2,outputs.shape[1]-2],dim=1)\n",
    "            err=outputs-local_labels\n",
    "            logs['total-err'][0]+=torch.sum(err**2).item()\n",
    "            err=err*nan_mask+(1-nan_mask)*1e9\n",
    "            err=err.to(torch.device(\"cpu\"))\n",
    "            err=err.numpy()\n",
    "            tot+=np.sum( (np.abs(err)<1e9).astype(int))\n",
    "        \n",
    "        logs['total-err'][0]=logs['total-err'][0]/tot\n",
    "        tot=0\n",
    "        \n",
    "        \n",
    "        for local_batch,nan_mask, local_labels in test_generator:\n",
    "            local_batch, nan_mask, local_labels = local_batch.to(device),nan_mask.to(device), local_labels.to(device)\n",
    "            with torch.set_grad_enabled(False):\n",
    "                outputs = net.forward(local_batch)*nan_mask\n",
    "            outputs,_=torch.split(outputs,[2,outputs.shape[1]-2],dim=1)\n",
    "            err=outputs-local_labels\n",
    "            logs['total-err'][1]+=torch.sum(err**2).item()\n",
    "            err=err*nan_mask+(1-nan_mask)*1e9\n",
    "            err=err.to(torch.device(\"cpu\"))\n",
    "            err=err.numpy()\n",
    "            tot+=np.sum( (np.abs(err)<1e9).astype(int))\n",
    "            \n",
    "        \n",
    "        logs['total-err'][1]=logs['total-err'][1]/tot\n",
    "        logs['total-err']=logs['total-err'].tolist()\n",
    "        with open(LOG, 'w') as outfile:\n",
    "            json.dump(logs, outfile)\n",
    "            \n",
    "        coarsening=net.init_coarsen>0\n",
    "        if not coarsening:\n",
    "            return \n",
    "        net.initial_coarsening()\n",
    "        (_,training_generator),(_,_),(_,test_generator),_,ds_zarr=load_data(net,args)\n",
    "        \n",
    "        logs['total-err-coarse']=np.zeros(2)\n",
    "        tot=0    \n",
    "        for local_batch,nan_mask, local_labels in training_generator:\n",
    "            local_batch, nan_mask, local_labels = local_batch.to(device),nan_mask.to(device), local_labels.to(device)\n",
    "            with torch.set_grad_enabled(False):\n",
    "                outputs = net.forward(local_batch)*nan_mask\n",
    "            outputs,_=torch.split(outputs,[2,outputs.shape[1]-2],dim=1)\n",
    "            err=outputs-local_labels\n",
    "            logs['total-err-coarse'][0]+=torch.sum(err**2).item()\n",
    "            err=err*nan_mask+(1-nan_mask)*1e9\n",
    "            err=err.to(torch.device(\"cpu\"))\n",
    "            err=err.numpy()\n",
    "            tot+=np.sum( (np.abs(err)<1e9).astype(int))\n",
    "        \n",
    "        \n",
    "        logs['total-err-coarse'][0]=logs['total-err-coarse'][0]/tot\n",
    "        #hist=np.zeros(len(edges)-1)\n",
    "        tot=0\n",
    "        \n",
    "        for local_batch,nan_mask, local_labels in test_generator:\n",
    "            local_batch, nan_mask, local_labels = local_batch.to(device),nan_mask.to(device), local_labels.to(device)\n",
    "            with torch.set_grad_enabled(False):\n",
    "                outputs = net.forward(local_batch)*nan_mask\n",
    "            outputs,_=torch.split(outputs,[2,outputs.shape[1]-2],dim=1)\n",
    "            err=outputs-local_labels\n",
    "            logs['total-err-coarse'][1]+=torch.sum(err**2).item()\n",
    "            err=err*nan_mask+(1-nan_mask)*1e9\n",
    "            err=err.to(torch.device(\"cpu\"))\n",
    "            err=err.numpy()\n",
    "            tot+=np.sum( (np.abs(err)<1e9).astype(int))\n",
    "            \n",
    "        \n",
    "        logs['total-err-coarse'][1]=logs['total-err-coarse'][1]/tot\n",
    "        logs['total-err-coarse']=logs['total-err-coarse'].tolist()\n",
    "        with open(LOG, 'w') as outfile:\n",
    "            json.dump(logs, outfile)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3abba8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic_model_matrix(args):\n",
    "    print('loading net')\n",
    "    net,criterion,(data_init,partition),logs,(PATH0,PATH1,LOG,root)=load_from_save(args)\n",
    "    net.eval()\n",
    "    m=net.spread*2+1\n",
    "    if isinstance(net,climate_models.CQCNN):\n",
    "        ncl=net.classnum\n",
    "        M=torch.zeros(ncl,2,2*m*m,2*m*m)\n",
    "        b=torch.zeros(ncl,2,2*m*m)\n",
    "        for j in range(ncl):\n",
    "            uv=torch.zeros(1,2,m,m).to(get_device())\n",
    "            print('filling b-'+str(j))\n",
    "            b[j,0,:]=torch.reshape(give_dev_wrt_input(net,m,dim=0,class_index=j,index=-1),[2*m*m])\n",
    "            b[j,1,:]=torch.reshape(give_dev_wrt_input(net,m,dim=1,class_index=j,index=-1),[2*m*m])\n",
    "            print('filling c')\n",
    "            #c[j]=give_dev_wrt_input(net,m,class_index=j,constant=True)#torch.reshape(net(uv).detach(),[-1])\n",
    "            uv.data.zero_()\n",
    "            print('filling M-'+str(j))\n",
    "            for i in range(M.shape[2]):\n",
    "                M[j,0,i,:]=(torch.reshape(give_dev_wrt_input(net,m,dim=0,class_index=j,index=i),[2*m*m])-b[j,0])/2\n",
    "                M[j,1,i,:]=(torch.reshape(give_dev_wrt_input(net,m,dim=1,class_index=j,index=i),[2*m*m])-b[j,1])/2\n",
    "        print('\\t saving')\n",
    "        with open(root+'/quadratic_M.npy', 'wb') as f:\n",
    "            np.save(f, M.numpy())\n",
    "        with open(root+'/quadratic_b.npy', 'wb') as f:\n",
    "            np.save(f, b.numpy())\n",
    "        '''with open(root+'/quadratic_c.npy', 'wb') as f:\n",
    "            np.save(f, c.numpy())  '''\n",
    "    else:\n",
    "        ncl=-1\n",
    "        M=torch.zeros(2,2*m*m,2*m*m)\n",
    "        b=torch.zeros(2,2*m*m)\n",
    "        uv=torch.zeros(1,2,m,m).to(get_device())\n",
    "        print('filling b')\n",
    "        b[0,:]=torch.reshape(give_dev_wrt_input(net,m,dim=0,index=-1),[2*m*m])\n",
    "        b[1,:]=torch.reshape(give_dev_wrt_input(net,m,dim=1,index=-1),[2*m*m])\n",
    "        print('filling c')\n",
    "        uv.data.zero_()\n",
    "        print('filling M')\n",
    "        for i in range(M.shape[2]):\n",
    "            M[0,i,:]=(torch.reshape(give_dev_wrt_input(net,m,dim=0,index=i),[2*m*m])-b[0])/2\n",
    "            M[1,i,:]=(torch.reshape(give_dev_wrt_input(net,m,dim=1,index=i),[2*m*m])-b[1])/2\n",
    "        print('\\t saving')\n",
    "        with open(root+'/quadratic_M.npy', 'wb') as f:\n",
    "            np.save(f, M.numpy())\n",
    "        with open(root+'/quadratic_b.npy', 'wb') as f:\n",
    "            np.save(f, b.numpy())\n",
    "\n",
    "    \n",
    "    #c=torch.zeros(ncl)\n",
    "    \n",
    "\n",
    "def give_dev_wrt_input(net,m,dim=0,index=0,class_index=-1,constant=False):\n",
    "    i=index\n",
    "    uv=torch.zeros(1,2,m,m)\n",
    "    if constant:\n",
    "        if class_index<0:\n",
    "            y=net.forward(uv)\n",
    "        else:\n",
    "            y=net.quadratic_forward(uv,class_index=class_index)\n",
    "        return y\n",
    "    if i>=0:\n",
    "        I=np.unravel_index(i, [2,m,m])\n",
    "        uv[0,I[0],I[1],I[2]]=1\n",
    "    #uv=uv.to(get_device())\n",
    "    uv.requires_grad_(True)\n",
    "    uv.grad=None\n",
    "    if class_index<0:\n",
    "        y=net.forward(uv)\n",
    "    else:\n",
    "        y=net.quadratic_forward(uv,class_index=class_index)\n",
    "    y=torch.reshape(y,[-1])\n",
    "    \n",
    "    y[dim].backward()\n",
    "    g=uv.grad\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c53054a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'options' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-55545c3dc2d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mglobal_averages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-55545c3dc2d2>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtoday\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoday\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Today's date:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoday\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'options' is not defined"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    today = date.today()\n",
    "    print(\"Today's date:\", today,flush=True)\n",
    "    args=options()\n",
    "    print(args)\n",
    "    if args.action==\"train\":\n",
    "        train(args)\n",
    "    if args.action==\"analysis\":\n",
    "        analysis(args)\n",
    "    if args.action==\"binned-r2\":\n",
    "        binned_r2_analysis(args)\n",
    "    if args.action==\"quadratic\":\n",
    "        quadratic_model_matrix(args)\n",
    "    if args.action==\"error-analysis\":\n",
    "        error_analysis(args)\n",
    "    if args.action==\"grad-analysis\":\n",
    "        grad_analysis(args)\n",
    "    if args.action==\"data-cov-analysis\":\n",
    "        data_covariance(args)\n",
    "    if args.action==\"fourier-analysis\":\n",
    "        data_fourier_analysis(args)\n",
    "    if args.action==\"shift-geo-analysis\":\n",
    "        shift_geo_analysis(args)\n",
    "    if args.action==\"grad-probe\":\n",
    "        grad_probe(args)\n",
    "    if args.action==\"prep\":\n",
    "        prep(args)\n",
    "    if args.action==\"projection-analysis\":\n",
    "        projection_analysis(args)\n",
    "    if args.action==\"global-averages\":\n",
    "        global_averages()\n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52b4308",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce11004",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
