{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e59768e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "from datetime import date\n",
    "import json\n",
    "import copy\n",
    "import scipy\n",
    "import climate_data\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product, combinations_with_replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57b198b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ganlossfun(output,target,mask,eps=0):\n",
    "    '''pout=torch.sum(output*mask,dim=(1,2,3))/torch.sum(mask,dim=(1,2,3))\n",
    "    return torch.mean(torch.log((pout)*target+(1-target)*(1-pout)+eps))'''\n",
    "    #pout=torch.sum(output*mask,dim=(1,2,3))/torch.sum(mask,dim=(1,2,3))\n",
    "    return torch.sum((target-output)**2*mask)/torch.sum(mask)\n",
    "def lossfun(output,target,mask,neglog=False,logloss=False,heteroscedastic=False,outsize=2):\n",
    "    loss=0\n",
    "    if neglog:\n",
    "        if heteroscedastic:\n",
    "            outsize+=1\n",
    "        output,probs= torch.split(output, [outsize,output.shape[1]-outsize], dim=1)\n",
    "        loss+=negloglikelihood(probs,mask)\n",
    "    if heteroscedastic:\n",
    "        loss+=heteroscedasticGaussianLoss(output, target,mask,outsize=outsize)\n",
    "    else:\n",
    "        output,_= torch.split(output, [outsize,output.shape[1]-outsize], dim=1)\n",
    "        if logloss:\n",
    "            loss+=logLoss(output, target,mask)\n",
    "        else:\n",
    "            loss+=l2Loss(output, target,mask)\n",
    "    return loss\n",
    "def negloglikelihood(probs,mask,eps=1e-2):\n",
    "    numclass=probs.shape[1]\n",
    "    N=torch.sum(mask)\n",
    "    #-probs*torch.log10(probs + eps)-(1-probs)*torch.log10(1-probs+eps)\n",
    "    probs=torch.mean(torch.log10(probs+eps)+torch.log10(1-probs+eps),dim=1,keepdim=True)\n",
    "    probs=probs[mask>0]\n",
    "    return torch.sum(probs)/N\n",
    "\n",
    "def heteroscedasticGaussianLoss(output, target,mask,eps=1e-5,outsize=2):\n",
    "    mean, precision = torch.split(output, [outsize,output.shape[1]-outsize], dim=1)\n",
    "    precision=precision + eps\n",
    "    nprecision=precision.shape[1]\n",
    "    N=torch.sum(mask)\n",
    "    if nprecision>1:\n",
    "        err2=(target - mean)**2\n",
    "        loss=torch.mean(\\\n",
    "                - 1 / 2 *  torch.log(precision) \\\n",
    "                +  1 / 2 * err2 * precision \\\n",
    "                + 1 / 2 * np.log(1e3),dim=1,keepdim=True)\n",
    "        loss=loss[mask>0]\n",
    "        loss=torch.sum(loss)\n",
    "    else:\n",
    "        err2=torch.sum((target - mean)**2,dim=1,keepdim=True)\n",
    "        err2=err2[mask>0]\n",
    "        precision=precision[mask>0]\n",
    "        loss=torch.sum(\\\n",
    "                - 1 / 2 *  torch.log(precision) \\\n",
    "                +  1 / 2 * err2 * precision \\\n",
    "                + 1 / 2 * np.log(1e3))\n",
    "    return loss/N\n",
    "\n",
    "def logLoss(output, target,mask,eps=1e-5):\n",
    "    l2 = torch.log( (target - output)**2 + eps) - np.log(eps)\n",
    "    l2 = torch.sum(l2,dim=1,keepdim=True)\n",
    "    l2=l2[mask>0]\n",
    "    l2 = torch.mean(l2)\n",
    "    return l2\n",
    "\n",
    "def l2Loss(output, target,mask):\n",
    "    l2 =  1 / 2 * (target - output)**2\n",
    "    l2 = torch.sum(l2,dim=1,keepdim=True)\n",
    "    l2=l2[mask>0]\n",
    "    l2=torch.mean(l2)\n",
    "    return l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dec1cc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def approximate_widths(def_width,def_filters,filters):\n",
    "    nlyr=len(def_filters)\n",
    "    num_param=np.zeros(nlyr)\n",
    "    for i in range(nlyr):\n",
    "        num_param[i]=def_width[i]*def_width[i+1]*(def_filters[i]**2)/(filters[i]**2)\n",
    "    widths=np.zeros(nlyr+1)\n",
    "    widths[0]=def_width[0]\n",
    "    for i in range(1,nlyr+1):\n",
    "        widths[i]=int(num_param[i-1]/widths[i-1])\n",
    "    widths[-1]=def_width[-1]\n",
    "    widths=[int(w) for w in widths]\n",
    "    return widths[1:]\n",
    "def lcnn_architecture(width_scale,filter_size,mode=0):\n",
    "    widths=[128,64,32,32,32,32,32,3]\n",
    "    widths=[np.ceil(width_scale*w) for w in widths]\n",
    "    filters21=[5,5,3,3,3,3,3,3]\n",
    "    if filter_size<21:\n",
    "        filter_size=(filter_size//2)*2+1\n",
    "        cursize=21\n",
    "        filters=np.array(filters21)\n",
    "        while cursize>filter_size:\n",
    "            filters=filter_shrink_method(filters,mode)\n",
    "            cursize=np.sum(filters)-len(filters)+1\n",
    "    else:\n",
    "        filters=filters21\n",
    "    #widths=approximate_widths(widths,filters21,filters)\n",
    "    net=LCNN()\n",
    "    nparam0=net.nparam\n",
    "    net=LCNN(filter_size=filters)\n",
    "    nparam1=net.nparam\n",
    "    rat=np.sqrt(nparam0/nparam1)\n",
    "    widths=[int(np.ceil(rat*w)) for w in widths]\n",
    "    widths[-1]=3\n",
    "    net=LCNN(filter_size=filters,width=widths)\n",
    "    return widths,filters,net.nparam\n",
    "def filter_shrink_method(filters,mode):\n",
    "    if mode==0:\n",
    "        # Default\n",
    "        i=np.where(filters==np.amax(filters))[0][-1]\n",
    "        filters[i]-=2\n",
    "    elif mode==1:\n",
    "        # top-to-bottom equal shrink\n",
    "        i=np.where(filters==np.amax(filters))[0][-1]\n",
    "        filters[i]-=1\n",
    "    elif mode==2:\n",
    "        # top-to-bottom aggressive shrink\n",
    "        i=np.where(filters!=1)[0][-1]\n",
    "        filters[i]-=1\n",
    "    elif mode==3:\n",
    "        # bottom-to-top aggressive shrink\n",
    "        i=np.where(filters!=1)[0][0]\n",
    "        filters[i]-=1\n",
    "    else:\n",
    "        np.random.seed(mode)\n",
    "        order=np.argsort(np.random.rand(len(filters)))\n",
    "        I=np.where(filters==np.amax(filters))[0]\n",
    "        I=np.array([i for i in order if i in I])\n",
    "        i=I[0]\n",
    "        filters[i]-=1\n",
    "    return filters\n",
    "def unet_receptive_field_compute(filter_size,pools,deep_filters):\n",
    "    receptive_field=1\n",
    "    for i in range(len(pools)):\n",
    "        ww=np.sum(deep_filters[-1-i])-len(deep_filters[-1-i])\n",
    "        receptive_field=(receptive_field+ww)*pools[-i-1]\n",
    "    receptive_field+=np.sum(filter_size[:3])-3\n",
    "    return receptive_field\n",
    "def unet_architecture(sigma):\n",
    "    sigma1=4\n",
    "    receptive_field1=102\n",
    "    receptive_field=(int(receptive_field1/sigma*sigma1)//2+1)*2\n",
    "    filter_size=[5,5,3,3,3,3,3,3]\n",
    "    deep_filters=[[3,3,3,1,1,1],[3,3,3,1,1,1],[3,3,3,1,1,1]]\n",
    "    widths=[64,128,256,512]\n",
    "    if sigma==sigma1:\n",
    "        return widths,filter_size,deep_filters\n",
    "    pools=[2,2,2]\n",
    "    org_filter_size__=copy.deepcopy(filter_size)\n",
    "    org_deep_filters__=copy.deepcopy(deep_filters)\n",
    "    rec=unet_receptive_field_compute(filter_size,pools,deep_filters)\n",
    "    nomoreleft=False\n",
    "    while rec>receptive_field:\n",
    "        filter_size__=copy.deepcopy(filter_size)\n",
    "        deep_filters__=copy.deepcopy(deep_filters)\n",
    "        lvl=len(deep_filters)-1\n",
    "        while lvl>=0:\n",
    "            dlvl=np.array(deep_filters[lvl])\n",
    "            if not np.all(dlvl==1):\n",
    "                break\n",
    "            lvl-=1\n",
    "        if lvl>=0:\n",
    "            I=np.where(dlvl>1)[0][-1]\n",
    "            deep_filters[lvl][I]-=1\n",
    "        else:\n",
    "            ff=np.array(filter_size[:3])\n",
    "            if not np.all(ff==1):\n",
    "                I=np.where(ff>1)[0][-1]\n",
    "                filter_size[I]-=2\n",
    "            else:\n",
    "                nomoreleft=True\n",
    "                break\n",
    "        rec=unet_receptive_field_compute(filter_size,pools,deep_filters)\n",
    "    if nomoreleft:\n",
    "        nomoreleft=False\n",
    "        while rec>receptive_field:\n",
    "            filter_size__=copy.deepcopy(filter_size)\n",
    "            deep_filters__=copy.deepcopy(deep_filters)\n",
    "            ff=np.array(filter_size[3:])\n",
    "            if not np.all(ff==np.amax(ff)):\n",
    "                I=np.where(ff==np.amax(ff))[0][-1]\n",
    "                filter_size[I+3]-=2\n",
    "            elif not np.all(ff==1):\n",
    "                I=np.where(ff>1)[0][-1]\n",
    "                filter_size[I+3]-=2\n",
    "            else:\n",
    "                nomoreleft=True\n",
    "                break\n",
    "            rec=unet_receptive_field_compute(filter_size,pools,deep_filters)\n",
    "        if nomoreleft:\n",
    "            print('WTF!')\n",
    "        else:\n",
    "            filter_size=copy.deepcopy(filter_size__)\n",
    "            deep_filters=copy.deepcopy(deep_filters__)\n",
    "    else:\n",
    "        filter_size=copy.deepcopy(filter_size__)\n",
    "        deep_filters=copy.deepcopy(deep_filters__)\n",
    "    rec=unet_receptive_field_compute(filter_size,pools,deep_filters)\n",
    "\n",
    "    net=UNET()\n",
    "    nparam0=net.nparam\n",
    "    net=UNET(filter_size=filter_size,deep_filters=deep_filters)\n",
    "    nparam1=net.nparam\n",
    "    rat=np.sqrt(nparam0/nparam1)\n",
    "    \n",
    "    widths=[int(np.ceil(w*rat)) for w in widths]\n",
    "    net=UNET(widths=widths,filter_size=filter_size,deep_filters=deep_filters)\n",
    "    nparam2=net.nparam\n",
    "    return widths,filter_size,deep_filters\n",
    "\n",
    "def qcnn_receptive_field_compute(filter_size):\n",
    "    return np.sum(filter_size)-len(filter_size)+1\n",
    "def qcnn_architecture(sigma):\n",
    "    sigma1=4\n",
    "    \n",
    "    receptive_field1=21\n",
    "    receptive_field=int(receptive_field1/sigma*sigma1)//2*2+1\n",
    "    \n",
    "    filter_size=[5,5,3,3,3,3,3,3]\n",
    "    qwidth=64\n",
    "    widths=[128,64,32,32,32,32,32,1]\n",
    "    if sigma==sigma1:\n",
    "        return widths,filter_size,qwidth,[11,11]\n",
    "    \n",
    "    qfilt1=(receptive_field+1)//2\n",
    "    if qfilt1%2==0:\n",
    "        qfilt2=qfilt1-1\n",
    "        qfilt1+=1\n",
    "    else:\n",
    "        qfilt2=qfilt1\n",
    "    qfilt=[qfilt1,qfilt2]\n",
    "    \n",
    "    org_filter_size__=copy.deepcopy(filter_size)\n",
    "    rec=qcnn_receptive_field_compute(filter_size)\n",
    "    nomoreleft=False\n",
    "    \n",
    "    while rec>receptive_field:\n",
    "        filter_size__=copy.deepcopy(filter_size)\n",
    "        ff=np.array(filter_size)\n",
    "        if not np.all(ff==np.amax(ff)):\n",
    "            I=np.where(ff==np.amax(ff))[0][-1]\n",
    "            filter_size[I]-=2\n",
    "        elif not np.all(ff==1):\n",
    "            I=np.where(ff>1)[0][-1]\n",
    "            filter_size[I]-=2\n",
    "        else:\n",
    "            nomoreleft=True\n",
    "            break\n",
    "        rec=qcnn_receptive_field_compute(filter_size)\n",
    "    if nomoreleft:\n",
    "        print('WTF!')\n",
    "    net=QCNN()\n",
    "    nparam0=net.nparam\n",
    "    net=QCNN(filter_size=filter_size,qfilt=qfilt)\n",
    "    nparam1=net.nparam\n",
    "    rat=np.sqrt(nparam0/nparam1)\n",
    "    \n",
    "   \n",
    "    \n",
    "    qwidth=int(np.ceil(qwidth*rat**2))\n",
    "    widths=[int(np.ceil(w*rat)) for w in widths]\n",
    "    \n",
    "    net=QCNN(width=widths,qwidth=qwidth,filter_size=filter_size,qfilt=qfilt)\n",
    "    nparam2=net.nparam\n",
    "    return widths,filter_size,qwidth,qfilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "241ac23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "MODEL COMPARE: (2x2 = 4)  [0-3]\n",
    "    LCNN/QCNN\n",
    "    u+v+T -> Su+Sv+ST\n",
    "    Surf/Deep\n",
    "    (testing 1pct CO2)\n",
    "\n",
    "BASE TESTS: do each with\n",
    "    u+v+T -> Su+Sv+ST \n",
    "    LCNN\n",
    "    Surf/Deep\n",
    "    (testing 1pct CO2)\n",
    "filtersize (21) 15 9 7 5 3 1          (2x6)   = 10 [1000-1011]\n",
    "coarse-grain (4) 8 12 16              (2x3)   = 6  [2000-2005]\n",
    "geophys (4dom) + glbl + glbl lat + glbl long   (2x3)x2 = 12 [3000-3011]\n",
    "    (also with UNET)\n",
    "'''\n",
    "def golden_model_bank(args,descriptive=False,configure=False,verbose=True,only_description=False):\n",
    "    model_id=int(args.model_id)\n",
    "    model_bank_id=args.model_bank_id\n",
    "    \n",
    "    if not only_description:\n",
    "        data_info=climate_data.get_dict(model_bank_id,model_id)\n",
    "    \n",
    "    folder_root='/scratch/zanna/data/cm2.6/'\n",
    "    data_root=['coarse-surf-data-sigma-',\\\n",
    "                    'coarse-3D-data-sigma-',\\\n",
    "                    'coarse-1pct-CO2-surf-data-sigma-',\\\n",
    "                    'coarse-1pct-CO2-3D-data-sigma-']\n",
    "    model_names=['LCNN','QCNN','UNET','GAN','REG']\n",
    "    sigma_vals=[4,8,12,16]\n",
    "    filter_sizes=[21,15,9,7,5,3,1]\n",
    "    \n",
    "    STEP=1000\n",
    "    test_type=model_id//STEP\n",
    "    test_num=model_id%STEP\n",
    "    # default parameter choices\n",
    "    surf_deep=0\n",
    "    lat_features=False\n",
    "    long_features=False\n",
    "    direct_coords=False\n",
    "    residue_training=False\n",
    "    temperature=[True,True]\n",
    "    co2test_flag= args.co2==1\n",
    "    physical_dom_id=0\n",
    "    depthvals=[5.03355 , 55.853249,  110.096153, 181.312454,  330.007751,1497.56189 , 3508.633057]\n",
    "    sigma_id=0\n",
    "    filt_mode=0\n",
    "    arch_id=0\n",
    "    filter_size=21\n",
    "    outwidth=3\n",
    "    inwidth=3\n",
    "    depthind=2\n",
    "    resnet=False\n",
    "    # index parameter init\n",
    "    tt=test_num\n",
    "    if test_type==1:\n",
    "        # FILTERSIZE\n",
    "        # 15 9 7 5 3 1\n",
    "        surf_deep=tt%2\n",
    "        tt=tt//2\n",
    "        filter_size_id=tt%(len(filter_sizes)-1)+1\n",
    "        filter_size=filter_sizes[filter_size_id]\n",
    "    elif test_type==2:\n",
    "        # COARSE-GRAIN\n",
    "        physical_dom_id=3\n",
    "        args.batch=2\n",
    "        surf_deep=tt%2\n",
    "        tt=tt//2\n",
    "        sigma_id=tt%(len(sigma_vals))\n",
    "        sigma=sigma_vals[sigma_id]\n",
    "        args.batch=int(2*(sigma/4)**2)\n",
    "        filter_size=np.int(np.ceil(21/sigma*4))//2*2+1\n",
    "    elif test_type==4:\n",
    "        # FULL TYPE TRAINING\n",
    "        \n",
    "        # DATASET (2)\n",
    "        # SURF/DEEP \n",
    "\n",
    "        # FILTERSIZE (7)\n",
    "        # 21 15 9 7 5 3 1\n",
    "        \n",
    "        # SIGMAVALS (4)\n",
    "        # 4 8 12 16 \n",
    "        \n",
    "        # GEOPHYS (3)\n",
    "        # NONE - GLOBAL - (GLOBAL+COORDS)\n",
    "        \n",
    "        # RESIDUE TARGET(2)\n",
    "        # YES - NO\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        EXPERIMENT 1\n",
    "            Filtersize + Sigmaval + GEOPHY\n",
    "                15/9/5/1 + 4  + GLBCOORDS\n",
    "                15/9/5/1 + 8  + GLBCOORDS\n",
    "                15/9/5/1 + 16 + GLBCOORDS\n",
    "            VALUES \n",
    "               4114,4115,4116,4117,4120,4121,4124,4125,\n",
    "               4128,4129,4130,4131,4134,4135,4138,4139, \n",
    "               4156,4157,4158,4159,4162,4163,4166,4167\n",
    "       EXPERIMENT 2\n",
    "            Filtersize + Sigmaval + GEOPHY + NO RESIDUE\n",
    "                15/9/5/1 + 4  + GLBCOORDS\n",
    "                15/9/5/1 + 8  + GLBCOORDS\n",
    "                15/9/5/1 + 16 + GLBCOORDS\n",
    "            VALUES \n",
    "               4282,4283,4284,4285,4288,4289,4292,4293,\n",
    "               4296,4297,4298,4299,4302,4303,4306,4307,\n",
    "               4324,4325,4326,4327,4330,4331,4334,4335\n",
    "        '''\n",
    "        \n",
    "        surf_deep=tt%2\n",
    "        tt=tt//2\n",
    "        \n",
    "        filter_size_id=tt%len(filter_sizes)\n",
    "        tt=tt//len(filter_sizes)\n",
    "        \n",
    "        sigma_id=tt%len(sigma_vals)\n",
    "        tt=tt//len(sigma_vals)\n",
    "        \n",
    "        sigma=sigma_vals[sigma_id]\n",
    "        filter_size=filter_sizes[filter_size_id]\n",
    "        \n",
    "        args.batch=256+64\n",
    "        \n",
    "        geophys=tt%3\n",
    "        if geophys>0:\n",
    "            physical_dom_id=3\n",
    "            args.batch=int(2*(sigma/4)**2)\n",
    "        if geophys==2:\n",
    "            lat_features =True\n",
    "            direct_coords=True\n",
    "        \n",
    "        tt=tt//3\n",
    "        residue_training=(tt%2)==0\n",
    "    elif test_type==5:\n",
    "        # FULL TYPE TRAINING\n",
    "        \n",
    "        # DATASET (2)\n",
    "        # SURF/DEEP \n",
    "\n",
    "        # ARCHITECTURE (3)\n",
    "        # LCNN/QCNN/UNET\n",
    "        \n",
    "        # SIGMAVALS (4)\n",
    "        # 4 8 12 16 \n",
    "        \n",
    "        # GEOPHYS (3)\n",
    "        # NONE - GLOBAL - (GLOBAL+COORDS)\n",
    "        \n",
    "        # RESIDUE TARGET(2)\n",
    "        # YES - NO\n",
    "        '''\n",
    "        EXPERIMENT 1\n",
    "            Dataset (2) + LCNN/QCNN (2) + Sigmavals (4) + 4 Domains + Res\n",
    "                5000,5001,5002,5003,5006,5007,5008,5009,5012,5013,5014,5015,5018,5019,5020,5021\n",
    "        EXPERIMENT 2\n",
    "            Dataset (2) + LCNN/QCNN/UNET (3) + Sigmavals (4) + GLBL + Res \n",
    "            Dataset (2) + LCNN/UNET (2) + Sigmavals (4) + COORDS + Res \n",
    "                5024,5025,5026,5027,5028,5029,5030,5031,5032,5033,5034,5035,5036,5037,5038,5039,\\\n",
    "                5040,5041,5042,5043,5044,5045,5046,5047,5048,5049,5052,5053,5054,5055,5058,5059,\\\n",
    "                5060,5061,5064,5065,5066,5067,5070,5071\n",
    "        EXPERIMENT 1.5\n",
    "            Dataset (2) + LCNN/QCNN (2) + Sigmavals (4) + 4 Domains + No Res\n",
    "                5072,5073,5074,5075,5078,5079,5080,5081,5084,5085,5086,5087,5090,5091,5092,5093\n",
    "        EXPERIMENT 2.5\n",
    "            Dataset (2) + LCNN/QCNN/UNET (3) + Sigmavals (4) + GLBL + No Res \n",
    "                5096,5097,5098,5099,5100,5101,5102,5103,5104,5105,5106,5107,5108,5109,5110,5111,5112,5113,5114,5115,5116,5117,5118,5119\n",
    "        '''\n",
    "        \n",
    "        C=[2,3,4,3,2]\n",
    "        title='full-training'\n",
    "        names=[['dataset', 'surf','depth 110m'],\\\n",
    "                       ['architecture','LCNN','QCNN','UNET'],\\\n",
    "                       ['sigma']+[str(sig) for sig in sigma_vals],\\\n",
    "                       ['training-doms','4regions','global','global+coords'],\\\n",
    "                       ['residue','yes','no']]\n",
    "        surf_deep=tt%2\n",
    "        tt=tt//2\n",
    "        \n",
    "        arch_id=tt%3\n",
    "        tt=tt//3\n",
    "        \n",
    "        sigma_id=tt%len(sigma_vals)\n",
    "        tt=tt//len(sigma_vals)\n",
    "        \n",
    "        sigma=sigma_vals[sigma_id]\n",
    "        filter_size=int(21*4/sigma//2)*2+1\n",
    "        args.batch=128#4#256\n",
    "        \n",
    "        geophys=tt%3\n",
    "        if geophys>0:\n",
    "            physical_dom_id=3\n",
    "            args.batch=int(2*(sigma/4)**2)\n",
    "        if geophys==2:\n",
    "            lat_features =True\n",
    "            #direct_coords=True\n",
    "        tt=tt//3\n",
    "        residue_training=(tt%2)==0\n",
    "    elif test_type==6:\n",
    "        '''\n",
    "        Regression model with various settings\n",
    "        EXPERIMENT\n",
    "            Dataset (2) + Regression (1) + Sigmavals (4) + Training(3) + Res/No (2)\n",
    "            6000-6048\n",
    "        '''\n",
    "        \n",
    "        C=[2,4,3,2]\n",
    "        title='linear regression'\n",
    "        names=[['dataset', 'surf','depth 110m'],\\\n",
    "                       ['sigma']+[str(sig) for sig in sigma_vals],\\\n",
    "                       ['training-doms','4regions','global','global+coords'],\\\n",
    "                       ['residue','yes','no']]\n",
    "        \n",
    "        surf_deep=tt%2\n",
    "        tt=tt//2\n",
    "        \n",
    "        arch_id=4\n",
    "        \n",
    "        sigma_id=tt%len(sigma_vals)\n",
    "        tt=tt//len(sigma_vals)\n",
    "        \n",
    "        sigma=sigma_vals[sigma_id]\n",
    "        args.batch=4\n",
    "        \n",
    "        geophys=tt%3\n",
    "        if geophys>0:\n",
    "            args.batch=1\n",
    "            physical_dom_id=3\n",
    "        if geophys==2:\n",
    "            lat_features =True\n",
    "            direct_coords=True\n",
    "        tt=tt//3\n",
    "        residue_training=(tt%2)==0\n",
    "    elif test_type==7:\n",
    "        '''\n",
    "        Testing various shrinkage types\n",
    "        EXPERIMENT\n",
    "            Sigmavals (4) + Shrinkage (6)\n",
    "        '''\n",
    "        \n",
    "        C=[4,6]\n",
    "        title='shrinkage procedures'\n",
    "        names=[['sigma']+[str(sig) for sig in sigma_vals],\\\n",
    "                   ['shrinkage type']+[str(sig) for sig in range(6)]]\n",
    "        \n",
    "        sigma_id=tt%4\n",
    "        tt=tt//4\n",
    "        sigma=sigma_vals[sigma_id]\n",
    "        filter_size=int(21*4/sigma//2)*2+1\n",
    "        args.batch=4\n",
    "        filt_mode=tt+1\n",
    "    elif test_type==8:\n",
    "        # FULL TYPE TRAINING\n",
    "        \n",
    "        # DATASET (2)\n",
    "        # SURF/DEEP \n",
    "\n",
    "        # FILTERSIZE (9)\n",
    "        # 21 15 9 7 5 4 3 2 1\n",
    "        \n",
    "        # SIGMAVALS (4)\n",
    "        # 4 8 12 16 \n",
    "        \n",
    "        # RESIDUE TARGET(2)\n",
    "        # YES - NO\n",
    "        \n",
    "        filter_sizes=[21,15,9,7,5,4,3,2,1]\n",
    "        \n",
    "        C=[2,9,4,2]\n",
    "        title='filter size training'\n",
    "        names=[['dataset', 'surf','depth 110m'],\\\n",
    "                       ['filter sozes']+[str(sig) for sig in filter_sizes],\\\n",
    "                       ['sigma']+[str(sig) for sig in sigma_vals],\\\n",
    "                       ['residue','yes','no']]\n",
    "        filt_mode=1\n",
    "        \n",
    "        surf_deep=tt%2\n",
    "        tt=tt//2\n",
    "        \n",
    "        filter_size_id=tt%len(filter_sizes)\n",
    "        tt=tt//len(filter_sizes)\n",
    "        \n",
    "        sigma_id=tt%len(sigma_vals)\n",
    "        tt=tt//len(sigma_vals)\n",
    "        \n",
    "        residue_training=(tt%2)==0\n",
    "        \n",
    "        sigma=sigma_vals[sigma_id]\n",
    "        filter_size=filter_sizes[filter_size_id]\n",
    "        \n",
    "        geophys=1\n",
    "        physical_dom_id=3\n",
    "        args.batch=int(2*(sigma/4)**2)\n",
    "    elif test_type==9:\n",
    "        C=[2,2,2,2,len(sigma_vals),3]\n",
    "        title='root improvement'\n",
    "        names=[['temp','no','yes'],\\\n",
    "                    ['global','no','yes'],\\\n",
    "                          ['res','no','yes'],\\\n",
    "                              ['geophys','no','yes'],\\\n",
    "                                ['sigma']+[str(sig) for sig in sigma_vals],\\\n",
    "                                      ['widths']+[str(sig) for sig in [0,1,2]]]\n",
    "        \n",
    "        \n",
    "        resnet=True\n",
    "        surf_deep=0\n",
    "        temperature[0]=1-(tt%2 == 0)\n",
    "        temperature[1]=1-(tt%2 == 0)\n",
    "        tt=tt//2\n",
    "        \n",
    "        if not temperature[0]:\n",
    "            inwidth=2\n",
    "        if not temperature[1]:\n",
    "            outwidth=2 \n",
    "        \n",
    "        if tt%2==0:\n",
    "            physical_dom_id=0\n",
    "        else:\n",
    "            physical_dom_id=3\n",
    "        tt=tt//2\n",
    "        \n",
    "        \n",
    "        residue_training=(tt%2)!=0\n",
    "        tt=tt//2\n",
    "        \n",
    "        lat_features=(tt%2)!=0\n",
    "        tt=tt//2\n",
    "        \n",
    "        sigma_id=tt%len(sigma_vals)\n",
    "        tt=tt//len(sigma_vals)\n",
    "        print(sigma_id,sigma_vals)\n",
    "        sigma=sigma_vals[sigma_id]\n",
    "        \n",
    "        width_id=tt\n",
    "        if sigma==4:\n",
    "            # spread = 10\n",
    "            filters=[3]*10+[1]*6\n",
    "        elif sigma==8:\n",
    "            # spread = 5\n",
    "            filters=[3]*5+[1]*11\n",
    "        elif sigma==12:\n",
    "            # spread = 4\n",
    "            filters=[3]*4+[1]*12\n",
    "        elif sigma==16:\n",
    "            # spread = 3\n",
    "            filters=[3]*3+[1]*13\n",
    "        widths=[[64,32,1],[128,64,1],[256,128,1]]\n",
    "        widths=widths[width_id]\n",
    "        \n",
    "        if physical_dom_id==3:\n",
    "            args.batch=int(2*(sigma/4)**2)\n",
    "            if width_id==2:\n",
    "                args.batch=int(args.batch/2)\n",
    "        else:\n",
    "            args.batch=165\n",
    "            \n",
    "        filter_size=int(21*4/sigma/2)*2+1\n",
    "    elif test_type==3:\n",
    "        C=[2,2,2,2,len(sigma_vals)]\n",
    "        title='root improvement'\n",
    "        names=[['temp','no','yes'],\\\n",
    "                    ['global','no','yes'],\\\n",
    "                          ['res','no','yes'],\\\n",
    "                              ['geophys','no','yes'],\\\n",
    "                                  ['sigma']+[str(sig) for sig in sigma_vals]]\n",
    "        \n",
    "        \n",
    "        surf_deep=0\n",
    "        temperature[0]=1-(tt%2 == 0)\n",
    "        temperature[1]=1-(tt%2 == 0)\n",
    "        tt=tt//2\n",
    "        if tt%2==0:\n",
    "            physical_dom_id=0\n",
    "        else:\n",
    "            physical_dom_id=3\n",
    "        tt=tt//2\n",
    "        \n",
    "        residue_training=(tt%2)!=0\n",
    "        tt=tt//2\n",
    "        \n",
    "        lat_features=(tt%2)!=0\n",
    "        #direct_coords=(tt%2)!=0\n",
    "        tt=tt//2\n",
    "        \n",
    "        sigma_id=tt\n",
    "        sigma=sigma_vals[sigma_id]\n",
    "        tt=tt//len(sigma_vals)\n",
    "        \n",
    "        if physical_dom_id==3:\n",
    "            args.batch=int(2*(sigma/4)**2)\n",
    "        else:\n",
    "            args.batch=165\n",
    "            \n",
    "        filter_size=int(21*4/sigma/2)*2+1\n",
    "        if not temperature[0]:\n",
    "            inwidth=2\n",
    "        if not temperature[1]:\n",
    "            outwidth=2\n",
    "    elif test_type==0:\n",
    "        C=[2,2,2,7,len(sigma_vals)]\n",
    "        depthvals=[5.03355,55.853249,110.096153,181.312454,330.007751, 1497.56189 , 3508.633057]\n",
    "        title='depth test'\n",
    "        names=[['temp','yes','no'],\\\n",
    "                    ['res','no','yes'],\\\n",
    "                        ['geophys','no','yes'],\\\n",
    "                            ['training-depth']+[str(i) for i in range(7)],\\\n",
    "                              ['sigma']+[str(i) for i in sigma_vals]]\n",
    "        \n",
    "        \n",
    "        surf_deep=1\n",
    "        temperature[0]=1-(tt%2)\n",
    "        temperature[1]=temperature[0]\n",
    "        tt=tt//2\n",
    "        \n",
    "        if not temperature[0]:\n",
    "            inwidth=2\n",
    "        if not temperature[1]:\n",
    "            outwidth=2 \n",
    "        \n",
    "        \n",
    "        residue_training=(tt%2)!=0\n",
    "        tt=tt//2\n",
    "        \n",
    "        lat_features=(tt%2)!=0\n",
    "        #direct_coords=(tt%2)!=0\n",
    "        tt=tt//2\n",
    "        \n",
    "        physical_dom_id=3\n",
    "        \n",
    "        \n",
    "        \n",
    "        depthind=tt%7\n",
    "        tt=tt//7\n",
    "        \n",
    "        \n",
    "        sigma_id=tt\n",
    "        sigma=sigma_vals[tt]\n",
    "        \n",
    "        if physical_dom_id==3:\n",
    "            args.batch=int(2*(sigma/4)**2)\n",
    "        else:\n",
    "            args.batch=128\n",
    "        filter_size=int(21*4/sigma/2)*2+1\n",
    "            \n",
    "    if only_description:\n",
    "        title+=' '+str(STEP*test_type)\n",
    "        if verbose:\n",
    "            print(title)\n",
    "            for i in range(len(names)):\n",
    "                print('\\t'+names[i][0])\n",
    "                outputstr='\\t\\t'\n",
    "                for j in range(1,len(names[i])):\n",
    "                    outputstr+=names[i][j]+' - '\n",
    "                print(outputstr)\n",
    "        return C,names\n",
    "    if co2test_flag:\n",
    "        surf_deep+=2\n",
    "    sigma=sigma_vals[sigma_id]\n",
    "    args.data_address=folder_root+data_root[surf_deep]+str(sigma)\n",
    "    \n",
    "    \n",
    "    args.data_address+='.zarr'\n",
    "    \n",
    "    if arch_id==0: #LCNN\n",
    "        width_scale=1\n",
    "        if not resnet:\n",
    "            widths,filters,nparam=lcnn_architecture(width_scale,filter_size,mode=filt_mode)\n",
    "        else:\n",
    "            _,filters,nparam=lcnn_architecture(width_scale,filter_size,mode=filt_mode)\n",
    "        net=LCNN(initwidth=inwidth,outwidth=outwidth,\\\n",
    "                 filter_size=filters,\\\n",
    "                 width=widths,\\\n",
    "                 nprecision=outwidth,\\\n",
    "                 latsig=lat_features,\\\n",
    "                 latsign=lat_features,\\\n",
    "                 longitude=long_features,\\\n",
    "                 freq_coord=lat_features and not direct_coords,\\\n",
    "                 direct_coord=direct_coords,\\\n",
    "                 skipcons=resnet)\n",
    "    elif arch_id==1: #QCNN\n",
    "        widths,filter_size__,qwidth,qfilt=qcnn_architecture(sigma)\n",
    "        net=QCNN(width=widths,qwidth=qwidth,filter_size=filter_size__,qfilt=qfilt,\\\n",
    "                 initwidth=inwidth,outwidth=outwidth,\\\n",
    "                 nprecision=outwidth,\\\n",
    "                 latsig=lat_features,\\\n",
    "                 latsign=lat_features,\\\n",
    "                 longitude=long_features,\\\n",
    "                 freq_coord=lat_features)\n",
    "    elif arch_id==2: #UNET\n",
    "        widths,filter_size__,deep_filters=unet_architecture(sigma)\n",
    "        net=UNET(widths=widths,deep_filters=deep_filters,filter_size=filter_size__,\\\n",
    "                 initwidth=inwidth,outwidth=outwidth,\\\n",
    "                 nprecision=outwidth,\\\n",
    "                 latsig=lat_features,\\\n",
    "                 latsign=lat_features,\\\n",
    "                 longitude=long_features,\\\n",
    "                 freq_coord=lat_features)\n",
    "    elif arch_id==3: #GAN\n",
    "        net=GAN(initwidth=inwidth,outwidth=outwidth,\\\n",
    "                latsig=lat_features,\\\n",
    "                 latsign=lat_features,\\\n",
    "                 longitude=long_features,\\\n",
    "                 freq_coord=lat_features)\n",
    "    elif arch_id==4: #Regression\n",
    "        net=RegressionModel(initwidth=inwidth,\\\n",
    "                 latsig=lat_features,\\\n",
    "                 latsign=lat_features,\\\n",
    "                 direct_coord=direct_coords)\n",
    "    if arch_id!=3:\n",
    "        loss=lambda output, target, mask: \\\n",
    "            lossfun(output, target, mask,heteroscedastic=True,outsize=outwidth)\n",
    "    else:\n",
    "        loss=lambda output, target, mask: \\\n",
    "            ganlossfun(output, target, mask)\n",
    "    description=model_names[arch_id]\n",
    "    if arch_id==0:\n",
    "        stt=str(filter_size)\n",
    "        stt=stt+'x'+stt\n",
    "        description+=' + '+stt\n",
    "    if surf_deep%2==0:\n",
    "        description+=' + '+'surface'\n",
    "    elif surf_deep%2==1:\n",
    "        depthval=depthvals[depthind]\n",
    "        depthval=str(int(np.round(depthval)))\n",
    "        description+=' + '+'deep ('+str(depthval)+'m)'\n",
    "        \n",
    "    if surf_deep//2==1:\n",
    "        description+=' +1%CO2'\n",
    "    if residue_training:\n",
    "        description+=' + '+'res'\n",
    "    if physical_dom_id==0:\n",
    "        description+=' + '+'4 domains'\n",
    "    elif physical_dom_id==3:\n",
    "        description+=' + '+'glbl'\n",
    "        if lat_features:\n",
    "            description+=' + '+'lat'\n",
    "        if long_features:\n",
    "            description+=' + '+'long'\n",
    "    \n",
    "    description+=' + '+'coarse('+str(sigma)+')'\n",
    "    if verbose:\n",
    "        print(description+' + '+'batch= '+str(args.batch), flush=True)\n",
    "    partition=climate_data.physical_domains(physical_dom_id)\n",
    "    ds_zarr=climate_data.load_ds_zarr(args)\n",
    "    model_bank_id='G'\n",
    "    \n",
    "    if configure:\n",
    "        data_info['direct_coord']=direct_coords\n",
    "        data_info['freq_coord']=lat_features\n",
    "        data_info['lat_feat']=lat_features\n",
    "        data_info['long_feat']=long_features\n",
    "        data_info['inputs']=\"usurf vsurf surface_temp\".split()\n",
    "        if not temperature[0]:\n",
    "            data_info['inputs']=data_info['inputs'][:2]\n",
    "            \n",
    "        if residue_training:\n",
    "            data_info['outputs']=\"Su_r Sv_r ST_r\".split()\n",
    "        else:\n",
    "            data_info['outputs']=\"Su Sv ST\".split()\n",
    "        if not temperature[1]:\n",
    "            data_info['outputs']=data_info['outputs'][:2]\n",
    "        maskloc='/scratch/cg3306/climate/masks/'\n",
    "        if surf_deep==0:\n",
    "            maskloc+='surf'\n",
    "        elif surf_deep==1:\n",
    "            maskloc+='deep'\n",
    "        maskloc+='-sigma'+str(sigma)\n",
    "        maskloc+='-filter'+str(filter_size)\n",
    "        if physical_dom_id==0:\n",
    "            maskloc+='-dom4'\n",
    "        if physical_dom_id==3:\n",
    "            maskloc+='-glbl'\n",
    "        if resnet:\n",
    "            maskloc+='-padded'\n",
    "        maskloc+='.npy'\n",
    "        data_info['maskloc']=maskloc\n",
    "    if not descriptive and configure:\n",
    "        climate_data.update_model_info(data_info,model_bank_id,model_id)\n",
    "    data_init=lambda partit : climate_data.Dataset2(ds_zarr,partit,model_id,model_bank_id,\\\n",
    "                                                    net,subtime=args.subtime,parallel=args.nworkers>1,\\\n",
    "                                                    depthind=depthind)\n",
    "    \n",
    "    if not descriptive:\n",
    "        return net,loss,data_init,partition\n",
    "    else:\n",
    "        return description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f941bbd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import numpy as np\\nC=[2,3,4,3,2]\\nx=[[0,1],[0,1,2],[0,1,2,3],[1,2],[1]]\\nlxi=[len(xx) for xx in x]\\nn=np.prod(lxi)\\ny=[]\\nfor i in range(n):\\n    ii=i\\n    tt=0\\n    TT=1\\n    for j in range(len(x)):\\n        tt+=x[j][ii%lxi[j]]*TT\\n        ii=ii//lxi[j]\\n        TT*=C[j]\\n    y.append(tt)\\ny=[tt+5000 for tt in y] \\nstr(y).replace(' ','')\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import numpy as np\n",
    "C=[2,3,4,3,2]\n",
    "x=[[0,1],[0,1,2],[0,1,2,3],[1,2],[1]]\n",
    "lxi=[len(xx) for xx in x]\n",
    "n=np.prod(lxi)\n",
    "y=[]\n",
    "for i in range(n):\n",
    "    ii=i\n",
    "    tt=0\n",
    "    TT=1\n",
    "    for j in range(len(x)):\n",
    "        tt+=x[j][ii%lxi[j]]*TT\n",
    "        ii=ii//lxi[j]\n",
    "        TT*=C[j]\n",
    "    y.append(tt)\n",
    "y=[tt+5000 for tt in y] \n",
    "str(y).replace(' ','')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69270d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_bank(args,descriptive=False,configure=False,verbose=True):\n",
    "    try:\n",
    "        model_bank_id=int(args.model_bank_id)\n",
    "    except:\n",
    "        if args.model_bank_id=='G':\n",
    "            return golden_model_bank(args,descriptive=descriptive,configure=configure,verbose=verbose)\n",
    "    model_id=int(args.model_id)\n",
    "    hetsc=1\n",
    "    #if not descriptive:\n",
    "    data_info=climate_data.get_dict(model_bank_id,model_id)\n",
    "    if configure:\n",
    "        data_info['direct_coord']=False\n",
    "        data_info['freq_coord']=False\n",
    "    if model_bank_id>12 and model_bank_id<=16:\n",
    "        folder_root='/scratch/zanna/data/cm2.6/'\n",
    "        data_root=['coarse-surf-data-sigma-',\\\n",
    "                        'coarse-3D-data-sigma-',\\\n",
    "                        'coarse-1pct-CO2-surf-data-sigma-',\\\n",
    "                        'coarse-1pct-CO2-3D-data-sigma-4-',\\\n",
    "                           ]\n",
    "    elif model_bank_id==11 or model_bank_id==12:\n",
    "        args.data_address='/scratch/zanna/data/cm2.6/coarse-3D-data-sigma-4-1.zarr/'\n",
    "        \n",
    "        if model_id<10:\n",
    "            physical_dom_id=0\n",
    "        else:\n",
    "            physical_dom_id=3\n",
    "        outwidth=3\n",
    "        initwidth=2\n",
    "        if model_bank_id==12:\n",
    "            outwidth=2\n",
    "            initwidth=3\n",
    "        if configure:\n",
    "            data_info['inputs']=\"usurf vsurf\".split()\n",
    "            data_info['outputs']=\"Su Sv\".split()\n",
    "            \n",
    "            if model_bank_id==12:\n",
    "                data_info['inputs']=\"usurf vsurf surface_temp\".split()\n",
    "                data_info['outputs']=\"ST\".split()\n",
    "\n",
    "            data_info['st_ocean']=[model_id%5]\n",
    "            data_info['maskloc']='/scratch/cg3306/climate/masks/'+\\\n",
    "                            'coarse-3D-data-sigma-4-physdoms-'+str(physical_dom_id)+\\\n",
    "                            '-depth-'+str(model_id%5)+'.np'\n",
    "        if model_id//5==0:\n",
    "            if not descriptive:\n",
    "                net=LCNN(initwidth=initwidth,outwidth=outwidth)\n",
    "                loss=lambda output, target, mask: lossfun(output, target, mask,heteroscedastic=True)\n",
    "            description='LCNN ocean depth = '+str(model_id%5)\n",
    "        elif model_id//5==1:\n",
    "            if not descriptive:\n",
    "                net=QCNN(initwidth=initwidth,outwidth=outwidth)\n",
    "                loss=lambda output, target, mask: lossfun(output, target, mask,heteroscedastic=True)\n",
    "            description='QCNN ocean depth = '+str(model_id%5)\n",
    "        elif model_id//5==2:\n",
    "            if not descriptive:\n",
    "                net=LCNN(initwidth=initwidth,outwidth=outwidth,latsig=True,latsign=True,longitude=True,freq_coord=True)\n",
    "                loss=lambda output, target, mask: lossfun(output, target, mask,heteroscedastic=True)\n",
    "            description='LCNN ocean glbl-coords depth = '+str(model_id%5)\n",
    "        elif model_id//5==3:\n",
    "            if not descriptive:\n",
    "                net=LCNN(initwidth=initwidth,outwidth=outwidth)\n",
    "                loss=lambda output, target, mask: lossfun(output, target, mask,heteroscedastic=True)\n",
    "            description='LCNN ocean glbl depth = '+str(model_id%5)\n",
    "        elif model_id>=20:\n",
    "            filtid=model_id-20\n",
    "            physical_dom_id=0\n",
    "            def_width=[2,256,128,64,64,64,32,32,3]\n",
    "            def_filters=[5,5,3,3,3,3,3,3] # 21\n",
    "            if configure:\n",
    "                data_info['st_ocean']=[2]\n",
    "            if filtid==0:\n",
    "                filters=[7,7,5,3,3,3,3,3] # 27\n",
    "            elif filtid==1:\n",
    "                filters=[3,3,3,3,3,3,3,1] # 15\n",
    "            elif filtid==2:\n",
    "                filters=[3,3,3,3,1,1,1,1] # 9\n",
    "            elif filtid==3:\n",
    "                filters=[3,3,3,1,1,1,1,1] # 7\n",
    "            elif filtid==4:\n",
    "                filters=[3,3,1,1,1,1,1,1] # 5\n",
    "            elif filtid==5:\n",
    "                filters=[3,1,1,1,1,1,1,1] # 3\n",
    "            widths=approximate_widths(def_width,def_filters,filters)\n",
    "            if not descriptive:\n",
    "                net=LCNN(initwidth=initwidth,outwidth=outwidth,filter_size=filters,width=widths)\n",
    "                loss=lambda output, target, mask: lossfun(output, target, mask,heteroscedastic=True)\n",
    "            description='LCNN ocean depth = 2, input-box size = '+str(np.sum(filters)-len(filters)+1)\n",
    "    elif model_bank_id==10:\n",
    "        physical_dom_id=0\n",
    "        \n",
    "        args.data_address='/scratch/zanna/data/cm2.6/coarse-surf-data-sigma-4.zarr/'\n",
    "        if model_id==0:\n",
    "            hetsc=5\n",
    "            if configure:\n",
    "                data_info['inputs']=\"usurf vsurf\".split()\n",
    "                data_info['outputs']=\"Su Sv\".split()\n",
    "            if not descriptive:\n",
    "                net=LCNN(initwidth=2,outwidth=3)\n",
    "                loss=lambda output, target, mask: lossfun(output, target, mask,heteroscedastic=True,outsize=2)\n",
    "            description='LCNN surface vel'\n",
    "        elif model_id==1:\n",
    "            if configure:\n",
    "                data_info['inputs']=\"usurf vsurf surface_temp\".split()\n",
    "                data_info['outputs']=\"ST\".split()\n",
    "            if not descriptive:\n",
    "                net=LCNN(initwidth=3,outwidth=2)\n",
    "                loss=lambda output, target, mask: lossfun(output, target, mask,heteroscedastic=True,outsize=1)\n",
    "            description='LCNN surface temp'\n",
    "        elif model_id==2:\n",
    "            if configure:\n",
    "                data_info['inputs']=\"usurf vsurf surface_temp\".split()\n",
    "                data_info['outputs']=\"ST\".split()\n",
    "            if not descriptive:\n",
    "                net=QCNN(initwidth=3,outwidth=2)\n",
    "                loss=lambda output, target, mask: lossfun(output, target, mask,heteroscedastic=True,outsize=1)\n",
    "            description='QCNN surface temp'\n",
    "        elif model_id==3:\n",
    "            physical_dom_id=3\n",
    "            if configure:\n",
    "                data_info['inputs']=\"usurf vsurf surface_temp\".split()\n",
    "                data_info['outputs']=\"ST\".split()\n",
    "            if not descriptive:\n",
    "                net=LCNN(initwidth=3,outwidth=2,latsig=True,latsign=True,longitude=True,freq_coord=True)\n",
    "                loss=lambda output, target, mask: lossfun(output, target, mask,heteroscedastic=True,outsize=1)\n",
    "            description='LCNN surface temp glbl-coord'\n",
    "        elif model_id==4:\n",
    "            physical_dom_id=3\n",
    "            if configure:\n",
    "                data_info['inputs']=\"usurf vsurf surface_temp\".split()\n",
    "                data_info['outputs']=\"ST\".split()\n",
    "            if not descriptive:\n",
    "                net=LCNN(initwidth=3,outwidth=2)\n",
    "                loss=lambda output, target, mask: lossfun(output, target, mask,heteroscedastic=True,outsize=1)\n",
    "            description='LCNN surface temp glbl'\n",
    "        elif model_id>=5:\n",
    "            \n",
    "            def_width=[2,256,128,64,64,64,32,32,3]\n",
    "            def_filters=[5,5,3,3,3,3,3,3] # 21\n",
    "            filtid=model_id-5\n",
    "            if filtid==0:\n",
    "                filters=[7,7,5,3,3,3,3,3] # 27\n",
    "            elif filtid==1:\n",
    "                filters=[3,3,3,3,3,3,3,1] # 15\n",
    "            elif filtid==2:\n",
    "                filters=[3,3,3,3,1,1,1,1] # 9\n",
    "            elif filtid==3:\n",
    "                filters=[3,3,3,1,1,1,1,1] # 7\n",
    "            elif filtid==4:\n",
    "                filters=[3,3,1,1,1,1,1,1] # 5\n",
    "            elif filtid==5:\n",
    "                filters=[3,1,1,1,1,1,1,1] # 3\n",
    "      \n",
    "            widths=approximate_widths(def_width,def_filters,filters)\n",
    "            if configure:\n",
    "                data_info['inputs']=\"usurf vsurf surface_temp\".split()\n",
    "                data_info['outputs']=\"ST\".split()\n",
    "                \n",
    "           \n",
    "            filtersize=np.sum(filters)-len(filters)+1\n",
    "            if not descriptive:\n",
    "                net=LCNN(initwidth=3,outwidth=2,filter_size=filters,width=widths)\n",
    "                loss=lambda output, target, mask: lossfun(output, target, mask,heteroscedastic=True,outsize=1)\n",
    "            description='LCNN surface temp - filter size= '+str(filtersize)\n",
    "        if configure:\n",
    "            if model_id<5:\n",
    "                data_info['maskloc']='/scratch/cg3306/climate/masks/coarse-surf-data-sigma-4-mask-'+str(physical_dom_id)+'.np'\n",
    "            elif model_id>=5:\n",
    "                data_info['maskloc']='/scratch/cg3306/climate/masks/'+\\\n",
    "                    'coarse-surf-data-sigma-4-mask-filtersize-'+str(filtersize)+'.np'\n",
    "    elif model_bank_id==9:\n",
    "        physical_dom_id=0\n",
    "        \n",
    "        args.data_address='/scratch/zanna/data/cm2.6/coarse-surf-data-sigma-4.zarr/'\n",
    "        \n",
    "        def_width=[2,256,128,64,64,64,32,32,3]\n",
    "        def_filters=[5,5,3,3,3,3,3,3] # 21\n",
    "        mm=5\n",
    "        filtid=model_id\n",
    "        if filtid==0:\n",
    "            filters=[7,7,5,3,3,3,3,3] # 27\n",
    "        elif  filtid==1:\n",
    "            filters=[5,5,3,3,3,3,3,3] # 21\n",
    "        elif filtid==2:\n",
    "            filters=[3,3,3,3,3,3,3,1] # 15\n",
    "        elif filtid==3:\n",
    "            filters=[3,3,3,3,1,1,1,1] # 9\n",
    "        elif filtid==4:\n",
    "            filters=[3,3,3,1,1,1,1,1] # 7\n",
    "        elif filtid==5:\n",
    "            filters=[3,3,1,1,1,1,1,1] # 5\n",
    "        elif filtid==6:\n",
    "            filters=[3,1,1,1,1,1,1,1] # 3\n",
    "        \n",
    "        widths=approximate_widths(def_width,def_filters,filters)\n",
    "        \n",
    "        filtersize=np.sum(filters)-len(filters)+1\n",
    "        if configure:\n",
    "            data_info['inputs']=\"usurf vsurf\".split()\n",
    "            data_info['outputs']=\"Su Sv\".split()\n",
    "            data_info['maskloc']='/scratch/cg3306/climate/masks/'+\\\n",
    "                            'coarse-surf-data-sigma-4-filtersize-'+str(filtersize)+'.np'\n",
    "        if not descriptive:\n",
    "            net=LCNN(initwidth=2,outwidth=3,filter_size=filters,width=widths)\n",
    "            loss=lambda output, target, mask: lossfun(output, target, mask,heteroscedastic=True,outsize=2)\n",
    "        description='LCNN surface flow - filter size= '+str(np.sum(filters)-len(filters)+1)\n",
    "    elif model_bank_id==8:\n",
    "        physical_dom_id=3\n",
    "        if model_bank_id==8:\n",
    "            geofeat=False\n",
    "        else:\n",
    "            geofeat=True\n",
    "        def_width=[2,256,128,64,64,64,32,32,3]\n",
    "        def_filters=[5,5,3,3,3,3,3,3]\n",
    "        if model_id==0:\n",
    "            sigma=2\n",
    "            filters=[11,11,5,5,5,5,5,5]\n",
    "        elif model_id==1:\n",
    "            sigma=4\n",
    "            filters=[5,5,3,3,3,3,3,3]\n",
    "        elif model_id==2:\n",
    "            sigma=6\n",
    "            filters=[3,3,3,3,3,3,3,3]\n",
    "        elif model_id==3:\n",
    "            sigma=8\n",
    "            filters=[3,3,3,3,3,1,1,1]\n",
    "        elif model_id==4:\n",
    "            sigma=12\n",
    "            filters=[3,3,3,1,1,1,1,1]\n",
    "        elif model_id==5:\n",
    "            sigma=16\n",
    "            filters=[3,3,3,1,1,1,1,1]\n",
    "        #rescale=[1/10,1/10]\n",
    "        scales=np.load('/scratch/cg3306/climate/climate_research/scales.npy')\n",
    "        ind=np.where(scales[:,0]==sigma)[0][0]\n",
    "        rescale=[scales[ind,1],scales[ind,2]]\n",
    "        if not descriptive:\n",
    "            widths=approximate_widths(def_width,def_filters,filters)\n",
    "            net=LCNN(latsig=geofeat,latsign=geofeat, longitude=geofeat,\\\n",
    "                         filter_size=filters,\\\n",
    "                         width=widths,\\\n",
    "                            rescale=rescale)\n",
    "            loss=lambda output, target, mask: lossfun(output, target, mask, heteroscedastic=True)\n",
    "        description='LCNN sigma='+str(sigma)\n",
    "        args.data_address='/scratch/cg3306/climate/data-read/data/sigma-'+str(sigma)+'-data.zarr'\n",
    "        \n",
    "    elif model_bank_id==7:\n",
    "        if model_id==0:\n",
    "            physical_dom_id=3\n",
    "        elif model_id==1:\n",
    "            physical_dom_id=2\n",
    "        if not descriptive:\n",
    "            net=CQCNN()\n",
    "            loss=lambda output, target, mask: lossfun(output, target, mask,heteroscedastic=True,neglog=True)\n",
    "        description='CQCNN'\n",
    "    elif model_bank_id==6:\n",
    "        physical_dom_id=3\n",
    "        if not descriptive:\n",
    "            net=Improved_QCNN()\n",
    "            loss=lambda output, target, mask: lossfun(output, target, mask,heteroscedastic=True)\n",
    "        fsize=21\n",
    "        description='Improved QCNN'\n",
    "    elif model_bank_id==5:\n",
    "        physical_dom_id=3\n",
    "        if model_id==0:\n",
    "            if not descriptive:\n",
    "                net=MatReg(order=2,width=64)\n",
    "                loss=lambda output, target, mask: lossfun(output, target, mask,heteroscedastic=True)\n",
    "            fsize=21\n",
    "            description='Local regression order 2 width 64 global training'\n",
    "        elif model_id==1:\n",
    "            if not descriptive:\n",
    "                net=NonlinearReg(order=2,width=64)\n",
    "                loss=lambda output, target, mask: lossfun(output, target, mask,heteroscedastic=True)\n",
    "            fsize=21\n",
    "            description='Local nonlinear order 2 global training'\n",
    "        elif model_id==2:\n",
    "            if not descriptive:\n",
    "                net=NonlinearReg(order=3,width=64)\n",
    "                loss=lambda output, target, mask: lossfun(output, target, mask,heteroscedastic=True)\n",
    "            fsize=21\n",
    "            description='Local nonlinear order 3 global training'    \n",
    "        if model_id==3:\n",
    "            if not descriptive:\n",
    "                net=MatReg(order=3,width=64)\n",
    "                loss=lambda output, target, mask: lossfun(output, target, mask,heteroscedastic=True)\n",
    "            fsize=21\n",
    "            description='Local regression order 3 width 64 global training'\n",
    "    elif model_bank_id==4:\n",
    "        physical_dom_id=3\n",
    "        loss=lambda output, target, mask: lossfun(output, target, mask,heteroscedastic=True)\n",
    "        if model_id==0:\n",
    "            net=UNET(latsig=True,latsign=True,longitude=True,freq_coord=True)\n",
    "            description='UNET-coords'\n",
    "        elif model_id==1:\n",
    "            net=UNET()\n",
    "            loss=lambda output, target, mask: lossfun(output, target, mask,heteroscedastic=True)\n",
    "            description='UNET'\n",
    "    elif model_bank_id==3:\n",
    "        physical_dom_id=3\n",
    "        if model_id==0:\n",
    "            if not descriptive:\n",
    "                net=LCNN()\n",
    "                loss=lambda output, target, mask: lossfun(output, target, mask,heteroscedastic=True)\n",
    "            fsize=21\n",
    "            description='LCNN global training'\n",
    "        elif model_id==1:\n",
    "            if not descriptive:\n",
    "                net=LCNN(latsig=True)\n",
    "                loss=lambda output, target, mask: lossfun(output, target, mask,heteroscedastic=True)\n",
    "            fsize=21\n",
    "            \n",
    "            description='LCNN global lat val training'\n",
    "        elif model_id==2:\n",
    "            if not descriptive:\n",
    "                net=LCNN(latsig=True,latsign=True)\n",
    "                loss=lambda output, target, mask: lossfun(output, target, mask,heteroscedastic=True)\n",
    "            fsize=21\n",
    "            description='LCNN latitude val+sign'\n",
    "        elif model_id==3:\n",
    "            if not descriptive:\n",
    "                net=LCNN(latsig=True,latsign=True,longitude=True)\n",
    "                loss=lambda output, target, mask: lossfun(output, target, mask,heteroscedastic=True)\n",
    "            fsize=21\n",
    "            description='LCNN latitude val+sign, longitude'\n",
    "        elif model_id==4:\n",
    "            if not descriptive:\n",
    "                net=LCNN(direct_coord=True)\n",
    "                loss=lambda output, target, mask: lossfun(output, target, mask,heteroscedastic=True)\n",
    "            fsize=21\n",
    "            description='LCNN direct coord'\n",
    "    elif model_bank_id==2:\n",
    "        physical_dom_id=3\n",
    "        if model_id==0:\n",
    "            physical_dom_id=0\n",
    "            if not descriptive:\n",
    "                net=LCNN(latsig=True,physical_domain_id=0)\n",
    "                loss=lambda output, target, mask: lossfun(output, target, mask,heteroscedastic=True)\n",
    "            fsize=21\n",
    "            description='LCNN('+str(fsize)+'x'+str(fsize)+') wt=0.05 heteroscedastic small domain'\n",
    "        elif model_id==1:\n",
    "            if not descriptive:\n",
    "                net=LCNN(latsig=True,direct_coord=False,longitude=True)\n",
    "                loss=lambda output, target, mask: lossfun(output, target, mask,heteroscedastic=True)\n",
    "            fsize=21\n",
    "            description='LCNN('+str(fsize)+'x'+str(fsize)+') freq encoded geo global training'\n",
    "        elif model_id==2:\n",
    "            if not descriptive:\n",
    "                net=LCNN(latsig=True,direct_coord=True,longitude=True)\n",
    "                loss=lambda output, target, mask: lossfun(output, target, mask,heteroscedastic=True)\n",
    "            fsize=21\n",
    "            description='LCNN('+str(fsize)+'x'+str(fsize)+') direct geo global training'\n",
    "        elif model_id==3:\n",
    "            if not descriptive:\n",
    "                net=LCNN(latsig=True,direct_coord=True,longitude=True,physical_force_features=True)\n",
    "                loss=lambda output, target, mask: lossfun(output, target, mask,heteroscedastic=True)\n",
    "            fsize=21\n",
    "            description='LCNN('+str(fsize)+'x'+str(fsize)+') freq encoded additional physical features global training'\n",
    "        elif model_id==4:\n",
    "            if not descriptive:\n",
    "                net=LCNN(latsig=True,direct_coord=True,longitude=True,timeshuffle=False)\n",
    "                loss=lambda output, target, mask: lossfun(output, target, mask,heteroscedastic=True)\n",
    "            fsize=21\n",
    "            description='LCNN('+str(fsize)+'x'+str(fsize)+') freq encoded heteroscedastic no time shuffle'\n",
    "        elif model_id==5:\n",
    "            physical_dom_id=4\n",
    "            if not descriptive:\n",
    "                net=LCNN(latsig=True,direct_coord=True,longitude=True,timeshuffle=False)\n",
    "                loss=lambda output, target, mask: lossfun(output, target, mask,heteroscedastic=True)\n",
    "            fsize=21\n",
    "            description='LCNN('+str(fsize)+'x'+str(fsize)+') freq encoded heteroscedastic equator'\n",
    "    elif model_bank_id==1:\n",
    "        physical_dom_id=0\n",
    "        #widths=[128,64,32,32,32,32,32,32,3],\\\n",
    "                    #filter_sizes=[5,5,3,3,3,3,3,3,3]\n",
    "        \n",
    "        widths=[\n",
    "                [128,64,32, 32, 32, 32, 32, 3],\\\n",
    "                [128,64,32, 32, 32, 32, 32, 3]\n",
    "        ]\n",
    "        \n",
    "        # 11 5 3\n",
    "        filter_sizes=[\n",
    "            [5,5,3,1,1,1,1,1],\\\n",
    "            [5,1,1,1,1,1,1,1]\n",
    "        ]\n",
    "        coarsen_levels=[\n",
    "            2,\\\n",
    "            4\n",
    "        ]\n",
    "        filter_sizes2=[\n",
    "            [7,5],\\\n",
    "            [3,3]\n",
    "        ]\n",
    "        if model_id<2:\n",
    "            coarsen_level=coarsen_levels[model_id]\n",
    "            if not descriptive:\n",
    "                net=LCNN(coarsen=coarsen_level)\n",
    "                loss=lambda output, target, mask: lossfun(output, target, mask,heteroscedastic=True)\n",
    "            fsize=21\n",
    "            description='LCNN('+str(fsize)+'x'+str(fsize)+') coarsen('+str(coarsen_level)+') heteroscedastic'\n",
    "        else:\n",
    "            width=widths[model_id-2]\n",
    "            filter_size=filter_sizes[model_id-2]\n",
    "            if not descriptive:\n",
    "                net=LCNN(width=width,filter_size=filter_size)\n",
    "                loss=lambda output, target, mask: lossfun(output, target, mask,heteroscedastic=True)\n",
    "            fsize=np.sum(filter_size)-len(filter_size)+1\n",
    "            description='LCNN('+str(fsize)+'x'+str(fsize)+') coarsen(0) heteroscedastic'\n",
    "        \n",
    "    elif model_bank_id==0:\n",
    "        physical_dom_id=0\n",
    "        if model_id==0:\n",
    "            if not descriptive:\n",
    "                net=LCNN()\n",
    "                loss=lambda output, target, mask: lossfun(output, target, mask,heteroscedastic=True)\n",
    "            \n",
    "            description='LCNN(21x21) coarsen(0) heteroscedastic'\n",
    "        elif model_id==1:\n",
    "            if not descriptive:\n",
    "                net=LCNN()\n",
    "                loss=lambda output, target, mask: lossfun(output, target, mask)\n",
    "            else:\n",
    "                description='LCNN(21x21) coarsen(0) l2'\n",
    "        elif model_id==2:\n",
    "            if not descriptive:\n",
    "                net=LCNN()\n",
    "                loss=lambda output, target, mask: lossfun(output, target, mask,logloss=True)\n",
    "            description='LCNN(21x21) coarsen(0) log'\n",
    "        elif model_id==3:\n",
    "            if not descriptive:\n",
    "                net=QCNN(qwidth=128)\n",
    "                loss=lambda output, target, mask: lossfun(output, target, mask,heteroscedastic=True)\n",
    "            description='QCNN(21) heteroskedastic'\n",
    "        elif model_id==4:\n",
    "            if not descriptive:\n",
    "                net=QCNN(qwidth=128)\n",
    "                loss=lambda output, target, mask: lossfun(output, target, mask,logloss=True)\n",
    "            description='QCNN(21) log'\n",
    "    print(description, flush=True)\n",
    "    partition=climate_data.physical_domains(physical_dom_id)\n",
    "    #if model_bank_id<8:\n",
    "    ds_zarr=climate_data.load_ds_zarr(args)\n",
    "    readfile2=False\n",
    "    if model_bank_id==8:\n",
    "        readfile2=True\n",
    "    if model_bank_id < 9:\n",
    "        data_init=lambda partit : climate_data.Dataset1(ds_zarr,partit,net,subtime=args.subtime,readfile2=readfile2)\n",
    "    else:\n",
    "        if not descriptive and configure:\n",
    "            climate_data.update_model_info(data_info,model_bank_id,model_id)\n",
    "        #print(args.subtime)\n",
    "        data_init=lambda partit : climate_data.Dataset2(ds_zarr,partit,model_id,model_bank_id,net,subtime=args.subtime,heteroscrescale=hetsc)\n",
    "    if not descriptive:\n",
    "        return net,loss,data_init,partition\n",
    "    else:\n",
    "        return description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18fbbe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClimateNet(nn.Module):\n",
    "    def __init__(self,spread=0,coarsen=0,rescale=[1/10,1/1e7],latsig=False,\\\n",
    "                 timeshuffle=True,direct_coord=True,longitude=False,latsign=False,gan=False):\n",
    "        super(ClimateNet, self).__init__()\n",
    "        if torch.cuda.is_available():  \n",
    "            device = \"cuda:0\" \n",
    "        else:  \n",
    "            device = \"cpu\"  \n",
    "        self.generative=False\n",
    "        self.timeshuffle=timeshuffle\n",
    "        self.device = torch.device(device) \n",
    "        self.spread=spread\n",
    "        self.latsig=latsig\n",
    "        self.direct_coord=direct_coord\n",
    "        self.longitude=longitude\n",
    "        self.latsign=latsign\n",
    "        self.coarsen=coarsen\n",
    "        self.coarse_grain_filters=[]\n",
    "        self.coarse_grain_filters.append([])\n",
    "        self.nn_layers = nn.ModuleList()\n",
    "        self.init_coarsen=coarsen\n",
    "        self.rescale=rescale\n",
    "        self.gan=gan\n",
    "        self.nprecision=0\n",
    "        for m in range(1,9):\n",
    "            gauss1=torch.zeros(2*m+1,2*m+1,dtype=torch.float32,requires_grad=False)\n",
    "            for i in range(m):\n",
    "                for j in range(m):\n",
    "                    gauss1[i,j]=np.exp( -(j**2+i**2)/((2*m)**2)/2)\n",
    "            gauss1=gauss1/gauss1.sum()\n",
    "            self.coarse_grain_filters.append(torch.reshape(gauss1,[1,1,2*m+1,2*m+1]).to(device))\n",
    "    def coarse_grain(self,x,m):\n",
    "        if m==0:\n",
    "            return x\n",
    "        b=x.shape[0]\n",
    "        c=x.shape[1]\n",
    "        h=x.shape[2]\n",
    "        w=x.shape[3]\n",
    "        return F.conv2d(x.view(b*c,1,h,w),self.coarse_grain_filters[m]).view(b,c,h-2*m,w-2*m)\n",
    "    def set_coarsening(self,c):\n",
    "        c_=self.coarsen\n",
    "        self.coarsen=c\n",
    "        self.spread=self.spread-c_+c\n",
    "    def initial_coarsening(self,):\n",
    "        self.spread=self.spread+self.init_coarsen-self.coarsen\n",
    "        self.coarsen=self.init_coarsen\n",
    "        \n",
    "def physical_forces(x):\n",
    "    dudy=x[:,:2,2:,1:-1]-x[:,:2,:-2,1:-1]\n",
    "    dudx=x[:,:2,1:-1,2:]-x[:,:2,1:-1,:-2]\n",
    "    x=x[:,:,1:-1,1:-1]\n",
    "    u_=x[:,0:1]\n",
    "    v_=x[:,1:2]\n",
    "    x=torch.cat([x,dudy,dudx,u_*dudy,v_*dudy,u_*dudx,v_*dudx],dim=1)\n",
    "    return x\n",
    "\n",
    "class LCNN(ClimateNet):\n",
    "    def __init__(self,spread=0,heteroscedastic=True,coarsen=0,\\\n",
    "                    width=[128,64,32,32,32,32,32,3],\\\n",
    "                    filter_size=[5,5,3,3,3,3,3,3],\\\n",
    "                    latsig=False,\\\n",
    "                    latsign=False,\\\n",
    "                    direct_coord=False,\\\n",
    "                    freq_coord=False,\\\n",
    "                    timeshuffle=False,\\\n",
    "                    physical_force_features=False,\\\n",
    "                    longitude=False,\\\n",
    "                    rescale=[1/10,1/1e7],\\\n",
    "                    initwidth=2,\\\n",
    "                    outwidth=2,\\\n",
    "                    nprecision=1,\\\n",
    "                    skipcons=False):\n",
    "        super(LCNN, self).__init__(spread=spread,coarsen=coarsen,latsig=latsig,timeshuffle=timeshuffle)\n",
    "        device=self.device\n",
    "        self.nn_layers = nn.ModuleList()\n",
    "        spread=0\n",
    "        self.nparam=0\n",
    "        self.rescale=torch.tensor(rescale,dtype=torch.float32,requires_grad=False)\n",
    "        self.freq_coord=freq_coord\n",
    "        self.heteroscedastic=heteroscedastic\n",
    "        width[-1]=outwidth+nprecision\n",
    "        self.width=width\n",
    "        self.outwidth=outwidth\n",
    "        self.initwidth=initwidth\n",
    "        self.filter_size=filter_size\n",
    "        self.num_layers=len(filter_size)\n",
    "        self.direct_coord=direct_coord\n",
    "        self.longitude=longitude\n",
    "        self.latsign=latsign\n",
    "        self.latsig=latsig\n",
    "        self.nprecision=nprecision\n",
    "        self.skipcons=skipcons\n",
    "        self.physical_force_features=physical_force_features\n",
    "        \n",
    "        self.bnflag=True#self.latsig or self.latsign or self.direct_coord\n",
    "        if self.direct_coord:\n",
    "            if self.latsig:\n",
    "                initwidth+=1\n",
    "            if self.latsign:\n",
    "                initwidth+=1\n",
    "            if self.longitude:\n",
    "                initwidth+=1\n",
    "        elif self.freq_coord:\n",
    "            if self.latsig:\n",
    "                initwidth+=1\n",
    "            if self.latsign:\n",
    "                initwidth+=1\n",
    "            if self.longitude:\n",
    "                initwidth+=2\n",
    "        if physical_force_features:\n",
    "            initwidth+=12\n",
    "            i=len(filter_size)\n",
    "            while i>0:\n",
    "                i-=1\n",
    "                if filter_size[i]>1:\n",
    "                    break\n",
    "            filter_size[i]-=2\n",
    "            spread+=1\n",
    "                \n",
    "        self.padding=[ff//2*self.skipcons for ff in filter_size]\n",
    "        if not self.skipcons:\n",
    "            self.nn_layers.append(nn.Conv2d(initwidth, width[0], filter_size[0],padding=self.padding[0]).to(device) )\n",
    "\n",
    "            self.nparam+=initwidth*width[0]*filter_size[0]**2\n",
    "            spread+=(filter_size[0]-1)/2\n",
    "            for i in range(1,self.num_layers):\n",
    "                if self.bnflag:\n",
    "                    self.nn_layers.append(nn.BatchNorm2d(width[i-1]).to(device) )\n",
    "                    self.nparam+=width[i-1]\n",
    "                self.nn_layers.append(nn.Conv2d(width[i-1], width[i], filter_size[i],padding=self.padding[i]).to(device) )\n",
    "                self.nparam+=width[i-1]*width[i]*filter_size[i]**2\n",
    "                spread+=(filter_size[i]-1)/2\n",
    "        else:\n",
    "            w0=width[0]\n",
    "            w1=width[1]\n",
    "            w2=width[-1]\n",
    "            self.nn_layers.append(nn.Conv2d(initwidth, w0, 1,padding=0).to(device) )\n",
    "            self.nn_layers.append(nn.BatchNorm2d(w0).to(device) )\n",
    "            self.nparam+=initwidth*w0+w0\n",
    "            for i in range(self.num_layers):\n",
    "                self.nn_layers.append(nn.Conv2d(w0, w1, 1,padding=0).to(device) )\n",
    "                self.nn_layers.append(nn.BatchNorm2d(w1).to(device) )\n",
    "                self.nn_layers.append(nn.Conv2d(w1, w1, filter_size[i],padding=self.padding[i]).to(device) )\n",
    "                self.nn_layers.append(nn.BatchNorm2d(w1).to(device) )\n",
    "                self.nn_layers.append(nn.Conv2d(w1, w0, 1,padding=0).to(device) )\n",
    "                self.nparam+=w0*w1*2+w1**2*filter_size[i]**2+w0+w1*2\n",
    "                spread+=(filter_size[i]-1)/2\n",
    "            self.nn_layers.append(nn.Conv2d(w0, w2, 1,padding=0).to(device) )\n",
    "            self.nparam+=w2*w0                \n",
    "        self.nn_layers.append(nn.Softplus().to(device))\n",
    "        spread+=coarsen\n",
    "        self.spread=np.int64(spread)\n",
    "        self.receptive_field=self.spread*2+1\n",
    "    def forward(self, x):\n",
    "        #x=x/self.rescale[0]\n",
    "        #if self.physical_force_features:\n",
    "        #    x=physical_forces(x)\n",
    "        #x=self.coarse_grain(x,self.coarsen)\n",
    "        if not self.skipcons:\n",
    "            cn=0\n",
    "            for i in range(self.num_layers-1):            \n",
    "                x = self.nn_layers[cn](x)\n",
    "                cn+=1\n",
    "                if self.bnflag:\n",
    "                    x = F.relu(self.nn_layers[cn](x))\n",
    "                    cn+=1\n",
    "                else:\n",
    "                    x = F.relu(x)\n",
    "            x=self.nn_layers[cn](x)\n",
    "            cn+=1\n",
    "        else:\n",
    "            cn=0\n",
    "            x = self.nn_layers[cn](x)\n",
    "            cn+=1\n",
    "            x = self.nn_layers[cn](x)\n",
    "            cn+=1\n",
    "            for i in range(self.num_layers):\n",
    "                init=x*1                \n",
    "                x = self.nn_layers[cn](x)\n",
    "                cn+=1\n",
    "                x = F.relu(self.nn_layers[cn](x))\n",
    "                cn+=1\n",
    "                x = self.nn_layers[cn](x)\n",
    "                cn+=1\n",
    "                x = F.relu(self.nn_layers[cn](x))\n",
    "                cn+=1\n",
    "                x = self.nn_layers[cn](x)\n",
    "                cn+=1\n",
    "                x+=init\n",
    "            x = self.nn_layers[cn](x)\n",
    "            cn+=1\n",
    "        mean,precision=torch.split(x,[x.shape[1]-self.nprecision,self.nprecision],dim=1)\n",
    "        precision=self.nn_layers[cn](precision)\n",
    "        x=torch.cat([mean,precision],dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37393d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionModel(ClimateNet):\n",
    "    def __init__(self,spread=1, degree=3,initwidth=3,outwidth=3,latsig=False,\\\n",
    "                    latsign=False,\\\n",
    "                    direct_coord=False,\\\n",
    "                    freq_coord=False):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        device=self.device\n",
    "        self.nn_layers = nn.ModuleList()\n",
    "        self.spread=spread\n",
    "        self.nparam=0\n",
    "        self.freq_coord=freq_coord\n",
    "        self.heteroscedastic=False\n",
    "        self.direct_coord=direct_coord\n",
    "        self.longitude=False\n",
    "        self.latsign=latsign\n",
    "        self.latsig=latsig\n",
    "        self.degree=degree\n",
    "        receptive_field=2*spread+1\n",
    "        self.receptive_field=receptive_field\n",
    "        if self.direct_coord:\n",
    "            if self.latsig:\n",
    "                initwidth+=1\n",
    "            if self.latsign:\n",
    "                initwidth+=1\n",
    "            if self.longitude:\n",
    "                initwidth+=1\n",
    "        elif self.freq_coord:\n",
    "            if self.latsig:\n",
    "                initwidth+=1\n",
    "            if self.latsign:\n",
    "                initwidth+=1\n",
    "            if self.longitude:\n",
    "                initwidth+=2\n",
    "        self.initwidth=initwidth\n",
    "        self.outwidth=outwidth\n",
    "        featurenum=initwidth*receptive_field**2\n",
    "        self.featurenum=featurenum\n",
    "        T=nn.Conv2d(initwidth, featurenum+1, 2*spread+1)\n",
    "        W=T.weight.data\n",
    "        W=W*0\n",
    "        W.requires_grad=False\n",
    "        names=[]\n",
    "        for i in range(initwidth):\n",
    "            for j in range(featurenum):\n",
    "                j_=j\n",
    "                j1=j_%receptive_field\n",
    "                j_=j_//receptive_field\n",
    "                j2=j_%receptive_field\n",
    "                j_=j_//receptive_field\n",
    "                j3=j_%initwidth\n",
    "                W_=torch.zeros(receptive_field,receptive_field)\n",
    "                W_[j1,j2]=1\n",
    "                W[j,i]=W_.view(1,1,receptive_field,receptive_field)\n",
    "                names.append([j3,[j1-spread,j2-spread]])\n",
    "        self.basic_names=names.copy()\n",
    "        T.weight.data=W\n",
    "        T.bias.data=T.bias.data*0\n",
    "        for j in range(featurenum,featurenum+1):\n",
    "            T.bias.data[j]=1.\n",
    "        T.bias.data.requires_grad=False\n",
    "        self.nn_layers.append(T.to(device) )\n",
    "        N=outwidth+1\n",
    "        self.res = list(combinations_with_replacement(range(N), degree))\n",
    "        self.res = [torch.tensor(I) for I in self.res]\n",
    "        self.names=[]\n",
    "        self.outputdimen=len(self.res)\n",
    "    def compute_names(self,):\n",
    "        outwidth=self.outwidth\n",
    "        self.names=[]\n",
    "        names=self.basic_names.copy()\n",
    "        for i in range(len(self.res)):\n",
    "            I=self.res[i]\n",
    "            D=torch.zeros(featurenum)\n",
    "            for j in range(featurenum):\n",
    "                D[j]=torch.sum(I==j)\n",
    "            stt=[]\n",
    "            for j in range(featurenum):\n",
    "                if D[j]>0:\n",
    "                    K=names[j].copy()\n",
    "                    K.append(int(D[j].item()))\n",
    "                    stt.append(K)\n",
    "            self.names.append(stt)\n",
    "    def forward(self,x,w):\n",
    "        bnum=x.shape[0]\n",
    "        ysp=x.shape[2]\n",
    "        xsp=x.shape[3]\n",
    "        spread=self.spread\n",
    "        featurenum=self.featurenum\n",
    "        initwidth=self.initwidth\n",
    "        receptive_field=self.receptive_field\n",
    "        y=torch.zeros(bnum,w.shape[1],ysp-2*spread,xsp-2*spread)\n",
    "        for j in range(featurenum):\n",
    "            j_=j\n",
    "            j1=j_%receptive_field\n",
    "            j_=j_//receptive_field\n",
    "            j2=j_%receptive_field\n",
    "            j_=j_//receptive_field\n",
    "            j3=j_%initwidth\n",
    "\n",
    "            y0=spread-(j1-spread)\n",
    "            y1=ysp-spread-(j1-spread)\n",
    "\n",
    "            x0=spread-(j2-spread)\n",
    "            x1=xsp-spread-(j2-spread)\n",
    "            for i in range(w.shape[1]):\n",
    "                y[:,i,:,:]+=w[j3,i]*x[:,j3,y0:y1,x0:x1]\n",
    "        return y\n",
    "    def cross_products(self, x,y,mask=[]):\n",
    "        bnum=x.shape[0]\n",
    "        ysp=x.shape[2]\n",
    "        xsp=x.shape[3]\n",
    "        spread=self.spread\n",
    "        outwidth=self.outwidth\n",
    "        initwidth=self.initwidth\n",
    "        receptive_field=self.receptive_field\n",
    "        x_=torch.zeros(bnum,outwidth+1,ysp-2*spread,xsp-2*spread)\n",
    "        for j in range(outwidth):\n",
    "            j_=j\n",
    "            j1=j_%receptive_field\n",
    "            j_=j_//receptive_field\n",
    "            j2=j_%receptive_field\n",
    "            j_=j_//receptive_field\n",
    "            j3=j_%initwidth\n",
    "\n",
    "            y0=spread-(j1-spread)\n",
    "            y1=ysp-spread-(j1-spread)\n",
    "\n",
    "            x0=spread-(j2-spread)\n",
    "            x1=xsp-spread-(j2-spread)\n",
    "\n",
    "            x_[:,j,:,:]=x[:,j3,y0:y1,x0:x1]\n",
    "        x_[:,outwidth]=x_[:,outwidth]+1.\n",
    "        #x=self.nn_layers[0](x)\n",
    "        x=x_\n",
    "        bnum=x.shape[0]\n",
    "        nchan=x.shape[1]\n",
    "        outnchan=y.shape[1]\n",
    "        ysp=x.shape[2]\n",
    "        xsp=x.shape[3]\n",
    "        sp=ysp*xsp\n",
    "        if len(mask)>0:\n",
    "            x=x*mask\n",
    "            y=y*mask\n",
    "        x=torch.reshape(x,(bnum,nchan,sp))\n",
    "        y=torch.reshape(y,(bnum,outnchan,sp))\n",
    "        x=x.permute((1,0,2))\n",
    "        y=y.permute((1,0,2))\n",
    "        x=torch.reshape(x,(nchan,bnum*sp))\n",
    "        y=torch.reshape(y,(outnchan,bnum*sp))\n",
    "        degree=self.degree\n",
    "        nfeat=len(self.res)\n",
    "        x=x.to(torch.device(\"cpu\"))\n",
    "        y=y.to(torch.device(\"cpu\"))\n",
    "        X=torch.zeros(nfeat,bnum*sp).to(torch.device(\"cpu\"))\n",
    "        for i in range(nfeat):\n",
    "            X[i]=torch.prod(x[self.res[i]],dim=0)\n",
    "        X2=X@X.T\n",
    "        XY=X@y.T\n",
    "        Y2=torch.sum(torch.square(y),dim=1)\n",
    "        return X2,XY,Y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab27723d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(ClimateNet):\n",
    "    def __init__(self,spread=0,\\\n",
    "                    width_generator=[3,128,64,32,32,32,32,32,3],\\\n",
    "                    filter_size_generator=[3,3,3,3,3,3,3,3],\\\n",
    "                    width_discriminator=[3,128,64,32,32,32,32,32,1],\\\n",
    "                    filter_size_discriminator=[9,9,3,3,1,1,1,1],\\\n",
    "                    latsig=False,\\\n",
    "                    latsign=False,\\\n",
    "                    direct_coord=False,\\\n",
    "                    freq_coord=False,\\\n",
    "                    longitude=False,\\\n",
    "                    initwidth=3,\\\n",
    "                    outwidth=3,\\\n",
    "                    random_field=1):\n",
    "        super(GAN, self).__init__(gan=True)\n",
    "        device=self.device\n",
    "        self.freq_coord=freq_coord\n",
    "        \n",
    "        self.outwidth=outwidth\n",
    "        self.initwidth=initwidth\n",
    "        self.random_field=random_field\n",
    "        self.width_generator=width_generator\n",
    "        self.width_discriminator=width_discriminator\n",
    "        self.filter_size_generator=filter_size_generator\n",
    "        self.filter_size_discriminator=filter_size_discriminator\n",
    "        self.direct_coord=direct_coord\n",
    "        self.longitude=longitude\n",
    "        self.latsign=latsign\n",
    "        self.latsig=latsig\n",
    "        self.generator_layers=[]\n",
    "        self.discriminator_layers=[]\n",
    "        \n",
    "        if self.direct_coord:\n",
    "            if self.latsig:\n",
    "                initwidth+=1\n",
    "            if self.latsign:\n",
    "                initwidth+=1\n",
    "            if self.longitude:\n",
    "                initwidth+=1\n",
    "        elif self.freq_coord:\n",
    "            if self.latsig:\n",
    "                initwidth+=1\n",
    "            if self.latsign:\n",
    "                initwidth+=1\n",
    "            if self.longitude:\n",
    "                initwidth+=2\n",
    " \n",
    "        # Discriminator build\n",
    "        discriminator=ClimateNet()\n",
    "        width=copy.deepcopy(width_discriminator)\n",
    "        filter_size=copy.deepcopy(filter_size_discriminator)\n",
    "        spread=0\n",
    "        self.nparam=0\n",
    "        width[0]=initwidth\n",
    "        for i in range(len(filter_size)):\n",
    "            if i==2:\n",
    "                discriminator.nn_layers.append(nn.BatchNorm2d(outwidth).to(device) )\n",
    "                self.nparam+=outwidth\n",
    "                width[i]+=outwidth\n",
    "            discriminator.nn_layers.append(nn.Conv2d(width[i], width[i+1], filter_size[i]).to(device) )\n",
    "            self.nparam+=width[i]*width[i+1]*filter_size[i]**2\n",
    "            spread+=(filter_size[i]-1)/2\n",
    "            \n",
    "            discriminator.nn_layers.append(nn.BatchNorm2d(width[i+1]).to(device) )\n",
    "            self.nparam+=width[i+1]\n",
    "            if i<len(filter_size)-1:\n",
    "                discriminator.nn_layers.append(nn.ReLU(inplace=True).to(device)) \n",
    "        self.receptive_field=np.int64(spread*2+1)\n",
    "        self.discriminator=discriminator\n",
    "        \n",
    "        # Generator build\n",
    "        generator=ClimateNet()\n",
    "        width=copy.deepcopy(width_generator)\n",
    "        filter_size=copy.deepcopy(filter_size_generator)\n",
    "        spread=0\n",
    "        width[0]=initwidth+random_field\n",
    "        width[-1]=outwidth\n",
    "        for i in range(len(filter_size)):\n",
    "            generator.nn_layers.append(nn.Conv2d(width[i], width[i+1], filter_size[i]).to(device) )\n",
    "            self.nparam+=width[i]*width[i+1]*filter_size[i]**2\n",
    "            spread+=(filter_size[i]-1)/2\n",
    "            if i<len(filter_size)-1:\n",
    "                generator.nn_layers.append(nn.BatchNorm2d(width[i+1]).to(device) )\n",
    "                self.nparam+=width[i+1]\n",
    "                generator.nn_layers.append(nn.ReLU(inplace=True).to(device)) \n",
    "            else:\n",
    "                generator.nn_layers.append(nn.BatchNorm2d(width[i+1]).to(device) )\n",
    "                self.nparam+=width[i+1]\n",
    "        self.spread=np.maximum(np.int64(spread),self.spread)\n",
    "        self.generator=generator\n",
    "    def discriminator_forward(self,x,y):#,yhat):\n",
    "        for i in range(6):\n",
    "            x = self.discriminator.nn_layers[i](x)\n",
    "        i=6\n",
    "        #y=torch.cat([y,yhat],dim=1)\n",
    "        y =self.discriminator.nn_layers[i](y)#-yhat)\n",
    "        x=torch.cat([x,y],dim=1)\n",
    "        for i in range(7,len(self.discriminator.nn_layers)):\n",
    "            x = self.discriminator.nn_layers[i](x)\n",
    "        return 1/(1+torch.exp(x))\n",
    "    def generator_forward(self,x,z):\n",
    "        x=torch.cat([x,z],dim=1)\n",
    "        for i in range(len(self.generator.nn_layers)):\n",
    "            x = self.generator.nn_layers[i](x)\n",
    "        return x#torch.tanh(x)*50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ccd0120",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QCNN(ClimateNet):\n",
    "    def __init__(self,qwidth=64,qfilt=[11,11],spread=0,heteroscedastic=True,coarsen=0,\\\n",
    "                    width=[128,64,32,32,32,32,32,1],\\\n",
    "                    filter_size=[5,5,3,3,3,3,3,3],\\\n",
    "                    latsig=False,\\\n",
    "                    latsign=False,\\\n",
    "                    freq_coord=False,\\\n",
    "                    direct_coord=False,\\\n",
    "                    timeshuffle=False,\\\n",
    "                    physical_force_features=False,\\\n",
    "                    longitude=False,\\\n",
    "                    rescale=[1/10,1/1e7],\\\n",
    "                    initwidth=2,\\\n",
    "                    outwidth=2,\\\n",
    "                    nprecision=1):\n",
    "        super(QCNN, self).__init__(spread=spread,coarsen=coarsen,latsig=latsig,timeshuffle=timeshuffle)\n",
    "        device=self.device\n",
    "        self.nn_layers = nn.ModuleList()\n",
    "        spread=0\n",
    "        \n",
    "        self.rescale=torch.tensor(rescale,dtype=torch.float32,requires_grad=False)\n",
    "        self.freq_coord=freq_coord\n",
    "        self.heteroscedastic=heteroscedastic\n",
    "        self.initwidth=initwidth\n",
    "        self.outwidth=outwidth\n",
    "        self.width=width\n",
    "        self.filter_size=filter_size\n",
    "        self.num_layers=len(filter_size)\n",
    "        self.direct_coord=direct_coord\n",
    "        self.longitude=longitude\n",
    "        self.latsign=latsign\n",
    "        self.latsig=latsig\n",
    "        self.nprecision=nprecision\n",
    "        self.physical_force_features=physical_force_features\n",
    "        self.nparam=0\n",
    "        self.bnflag=True\n",
    "        \n",
    "        self.nn_layers.append(nn.Conv2d(initwidth, qwidth, qfilt[0]).to(device) )\n",
    "        self.nparam+=initwidth*qwidth*qfilt[0]**2\n",
    "        self.nn_layers.append(nn.BatchNorm2d(qwidth).to(device) )\n",
    "        self.nparam+=qwidth\n",
    "        self.nn_layers.append(nn.BatchNorm2d(qwidth).to(device) )\n",
    "        self.nparam+=qwidth\n",
    "        self.nn_layers.append(nn.Conv2d(qwidth, outwidth, qfilt[1]).to(device) )\n",
    "        self.nparam+=outwidth*qwidth*qfilt[1]**2\n",
    "        self.nn_layers.append(nn.Conv2d(initwidth, width[0], filter_size[0]).to(device) )\n",
    "        self.nparam+=initwidth*width[0]*filter_size[0]**2\n",
    "        spread+=(filter_size[0]-1)/2\n",
    "        width[-1]=nprecision\n",
    "        for i in range(1,self.num_layers):\n",
    "            self.nn_layers.append(nn.BatchNorm2d(width[i-1]).to(device) )\n",
    "            self.nparam+=width[i-1]\n",
    "            self.nn_layers.append(nn.Conv2d(width[i-1], width[i], filter_size[i]).to(device) )\n",
    "            self.nparam+=width[i-1]*width[i]*filter_size[i]**2\n",
    "            spread+=(filter_size[i]-1)/2\n",
    "        \n",
    "        self.nn_layers.append(nn.Softplus().to(device))\n",
    "        spread+=coarsen\n",
    "        self.spread=np.int64(spread)\n",
    "        self.receptive_field=self.spread*2+1\n",
    "    def forward(self, x):\n",
    "        u=x*1\n",
    "        cn=0\n",
    "        u = self.nn_layers[cn](u)\n",
    "        cn+=1\n",
    "        u = torch.square(self.nn_layers[cn](u))\n",
    "        cn+=1\n",
    "        u = self.nn_layers[cn](u)\n",
    "        cn+=1\n",
    "        u = self.nn_layers[cn](u)\n",
    "        cn+=1\n",
    "        \n",
    "        for i in range(self.num_layers-1):\n",
    "            x = self.nn_layers[cn](x)\n",
    "            cn+=1\n",
    "            x = F.relu(self.nn_layers[cn](x))\n",
    "            cn+=1\n",
    "            \n",
    "        x=self.nn_layers[cn](x)\n",
    "        cn+=1\n",
    "        precision=self.nn_layers[cn](x)\n",
    "        x=torch.cat([u,precision],dim=1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "563376f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCNN(ClimateNet):\n",
    "    def __init__(self,qwidth=64,spread=0,heteroscedastic=True,coarsen=0,\\\n",
    "                    latsig=False,\\\n",
    "                    latsign=False,\\\n",
    "                    freq_coord=False,\\\n",
    "                    direct_coord=False,\\\n",
    "                    timeshuffle=False,\\\n",
    "                    physical_force_features=False,\\\n",
    "                    longitude=False,\\\n",
    "                    rescale=[1/10,1/1e7],\\\n",
    "                    initwidth=2,\\\n",
    "                    outwidth=3,\\\n",
    "                    degree=4):\n",
    "        super(PCNN, self).__init__(spread=spread,latsig=latsig,timeshuffle=timeshuffle)\n",
    "        device=self.device\n",
    "        self.nn_layers = nn.ModuleList()\n",
    "        spread=0\n",
    "        \n",
    "        self.rescale=torch.tensor(rescale,dtype=torch.float32,requires_grad=False)\n",
    "        self.freq_coord=freq_coord\n",
    "        self.heteroscedastic=heteroscedastic\n",
    "        self.initwidth=initwidth\n",
    "        self.outwidth=outwidth\n",
    "        \n",
    "        \n",
    "        self.direct_coord=direct_coord\n",
    "        self.longitude=longitude\n",
    "        self.latsign=latsign\n",
    "        self.latsig=latsig\n",
    "\n",
    "        self.physical_force_features=physical_force_features\n",
    "        \n",
    "        self.degree=degree\n",
    "        width=[64,64,32]\n",
    "        # 7 5 \n",
    "        # \n",
    "        filter_size=[7,7,5,5]\n",
    "        for j in range(2):\n",
    "            for i in range(degree-1):\n",
    "                self.nn_layers.append(nn.Conv2d(initwidth, width[0], filter_size[0],bias=False).to(device) )\n",
    "                self.nn_layers.append(nn.Conv2d(width[0], width[1], filter_size[1],bias=False).to(device) )\n",
    "                self.nn_layers.append(nn.BatchNorm2d(width[1]).to(device) )\n",
    "                self.nn_layers.append(nn.BatchNorm2d(width[1]).to(device) )\n",
    "                self.nn_layers.append(nn.Conv2d(width[1], width[2], filter_size[2],bias=False).to(device) )\n",
    "                if j==0:\n",
    "                    self.nn_layers.append(nn.Conv2d(width[2], outwidth-1, filter_size[3],bias=i==0).to(device) )\n",
    "                else:\n",
    "                    self.nn_layers.append(nn.Conv2d(width[2], 1, filter_size[3],bias=i==0).to(device) )\n",
    "        self.nn_layers.append(nn.BatchNorm2d(1).to(device) )\n",
    "            \n",
    "        spread=0\n",
    "        for i in range(len(filter_size)):\n",
    "            spread+=(filter_size[i]-1)/2\n",
    "        \n",
    "     \n",
    "        \n",
    "        self.nn_layers.append(nn.Softplus().to(device))\n",
    "        self.spread=np.int64(spread)\n",
    "        self.receptive_field=self.spread*2+1\n",
    "    def forward(self, x,decomposed=False):\n",
    "        degree=self.degree\n",
    "        cn=0\n",
    "        ymean=[[] for i in range(degree-1)]\n",
    "        for i in range(degree-1):\n",
    "            u=x*1\n",
    "            for j in range(2):\n",
    "                u = self.nn_layers[cn](u)\n",
    "                cn+=1\n",
    "            u = self.nn_layers[cn](u)#-self.nn_layers[cn].bias.reshape(1,-1,1,1)\n",
    "            cn+=1\n",
    "            u = u**(i+1)\n",
    "            \n",
    "            u = self.nn_layers[cn](u)#-self.nn_layers[cn].bias.reshape(1,-1,1,1)\n",
    "            cn+=1\n",
    "            for j in range(2):\n",
    "                u = self.nn_layers[cn](u)\n",
    "                cn+=1\n",
    "            ymean[i]=u\n",
    "            \n",
    "        yprec=[[] for i in range(degree-1)]\n",
    "        for i in range(degree-1):\n",
    "            u=x*1\n",
    "            for j in range(2):\n",
    "                u = self.nn_layers[cn](u)\n",
    "                cn+=1\n",
    "            u = self.nn_layers[cn](u)#-self.nn_layers[cn].bias.reshape(1,-1,1,1)\n",
    "            cn+=1\n",
    "            \n",
    "            u = u**(i+1)\n",
    "            \n",
    "            u = self.nn_layers[cn](u)#-self.nn_layers[cn].bias.reshape(1,-1,1,1)\n",
    "            cn+=1\n",
    "            \n",
    "            for j in range(2):\n",
    "                u = self.nn_layers[cn](u)\n",
    "                cn+=1\n",
    "            yprec[i]=u\n",
    "        if decomposed:\n",
    "            return ymean,yprec\n",
    "        precision_=yprec[0]/degree\n",
    "        mean_=ymean[0]/degree\n",
    "        for i in range(1,degree-1):\n",
    "            precision_+=yprec[i]/degree\n",
    "            mean_+=ymean[i]/degree\n",
    "        for j in range(2):\n",
    "            precision_=self.nn_layers[cn](precision_)\n",
    "            cn+=1\n",
    "        x=torch.cat([mean_,precision_],dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aaea4e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QMat(ClimateNet):\n",
    "    def __init__(self,filter_size=11,yequispaced=False):\n",
    "        super(QMat, self).__init__()\n",
    "        device = self.device\n",
    "        self.yequispaced=yequispaced\n",
    "        self.spread=np.int64((filter_size-1)/2)\n",
    "        self.filter_size=filter_size\n",
    "        m=filter_size\n",
    "        m2=m**2\n",
    "        self.nn_layers = nn.ModuleList()\n",
    "        SHT=nn.Conv2d(2, 2*m2, m,bias=False).to(device)\n",
    "        SHT.weight.data=SHT.weight.data*0\n",
    "        for i in range(m2):\n",
    "            i0,i1=np.unravel_index(i,[m,m])\n",
    "            SHT.weight.data[i,0,i0,i1]=1\n",
    "            SHT.weight.data[i+m2,1,i0,i1]=1\n",
    "        SHT.weight.requires_grad=False\n",
    "        self.shift_conv=SHT\n",
    "        self.nn_layers.append( nn.Linear(2*m2,2*m2,bias=False).to(device))\n",
    "        self.nn_layers.append( nn.Linear(2*m2,2*m2,bias=False).to(device))\n",
    "    def forward(self, x):\n",
    "        b=x.shape[0]\n",
    "        d1=x.shape[2]\n",
    "        d2=x.shape[3]\n",
    "        m2=2*self.filter_size**2\n",
    "        X=self.shift_conv(x).view(b,m2,-1)\n",
    "        X=X.permute(0,2,1)\n",
    "        \n",
    "        M0X=self.nn_layers[0](X)\n",
    "        Y0=torch.mul(M0X,X).sum(2)/m2\n",
    "        \n",
    "        M1X=self.nn_layers[1](X)\n",
    "        Y1=torch.mul(M1X,X).sum(2)/m2\n",
    "        Y=torch.stack([Y0,Y1],dim=1).view(b,2,d1-self.spread*2,d2-self.spread*2)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d364ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQCNN(ClimateNet):\n",
    "    def __init__(self,spread=0,heteroscedastic=True,coarsen=0,\\\n",
    "                    width=[64,32,16,16,16,16,16,2],\\\n",
    "                    filter_size=[5,5,3,3,3,3,3,3],\\\n",
    "                    latsig=False,\\\n",
    "                    latsign=False,\\\n",
    "                    direct_coord=False,\\\n",
    "                    timeshuffle=False,\\\n",
    "                    physical_force_features=False,\\\n",
    "                    longitude=False,\\\n",
    "                    rescale=[1/10,1/1e7]):\n",
    "        super(DQCNN, self).__init__(spread=spread,coarsen=coarsen,latsig=latsig,timeshuffle=timeshuffle)\n",
    "        device=self.device\n",
    "        self.nn_layers = nn.ModuleList()\n",
    "        spread=0\n",
    "        \n",
    "        self.rescale=torch.tensor(rescale,dtype=torch.float32,requires_grad=False)\n",
    "        \n",
    "        self.heteroscedastic=heteroscedastic\n",
    "        self.width=width\n",
    "        self.filter_size=filter_size\n",
    "        self.num_layers=len(filter_size)\n",
    "        self.direct_coord=direct_coord\n",
    "        self.longitude=longitude\n",
    "        self.latsign=latsign\n",
    "        self.latsig=latsig\n",
    "        \n",
    "        initwidth=2\n",
    "        self.physical_force_features=physical_force_features\n",
    "        \n",
    "        self.bnflag=True#self.latsig or self.latsign or self.direct_coord\n",
    "\n",
    "        self.nn_layers.append(nn.Conv2d(initwidth, width[0], filter_size[0]).to(device) )\n",
    "        spread+=(filter_size[0]-1)/2\n",
    "        for i in range(1,self.num_layers):\n",
    "            self.nn_layers.append(nn.BatchNorm2d(width[i-1]).to(device) )\n",
    "            self.nn_layers.append(nn.Conv2d(width[i-1], width[i], filter_size[i]).to(device) )\n",
    "            self.nn_layers.append(nn.Conv2d(width[i], 3, 1).to(device) )\n",
    "            spread+=(filter_size[i]-1)/2\n",
    "        self.nn_layers.append(nn.BatchNorm2d(1).to(device) )\n",
    "        self.nn_layers.append(nn.Softplus().to(device))\n",
    "        self.spread=np.int64(spread)\n",
    "        self.receptive_field=self.spread*2+1\n",
    "    def forward(self, x):\n",
    "        cn=0\n",
    "        y=torch.zeros(x.shape[0],3,x.shape[2]-2*self.spread,x.shape[3]-2*self.spread).to(self.device)\n",
    "        nspread=self.spread\n",
    "        x = self.nn_layers[cn](x)\n",
    "        cn+=1\n",
    "        for i in range(self.num_layers-1):\n",
    "            x = torch.square(self.nn_layers[cn](x))\n",
    "            cn+=1\n",
    "            \n",
    "            x = self.nn_layers[cn](x)\n",
    "            cn+=1\n",
    "            \n",
    "            x_ = self.nn_layers[cn](x)\n",
    "            cn+=1\n",
    "            \n",
    "            nspread=(x_.shape[2]-y.shape[2])//2\n",
    "            if nspread>0:\n",
    "                y += x_[:,:,nspread:-nspread,nspread:-nspread]\n",
    "            else:\n",
    "                y += x_\n",
    "        \n",
    "        mean,precision=torch.split(y,[2,1],dim=1)\n",
    "        precision=self.nn_layers[cn](precision)\n",
    "        cn+=1\n",
    "        precision=self.nn_layers[cn](precision)\n",
    "        x=torch.cat([mean,precision],dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22171623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ismember_(a,b):\n",
    "    for a_ in b:\n",
    "        if a==a_:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "096737df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonlinearReg(ClimateNet):\n",
    "    def __init__(self, order=3,width=64):\n",
    "        super(NonlinearReg, self).__init__()\n",
    "        device=self.device\n",
    "        self.order=order\n",
    "        inwidth=3**self.order*2 +3\n",
    "        width0=width +3\n",
    "        width1=width\n",
    "        outwidth=3\n",
    "        self.depth=7\n",
    "        self.indepth=[1,3,5]\n",
    "        self.spread=self.order\n",
    "        self.nn_layers=nn.ModuleList()\n",
    "        widths=[]*self.depth\n",
    "        for i in range(self.depth):\n",
    "            if ismember_(i,self.indepth):\n",
    "                widths.append([width0,width1])\n",
    "            else:\n",
    "                widths.append([width0,width0])\n",
    "                \n",
    "        widths[-1][1]=outwidth\n",
    "        widths[0][0]=inwidth\n",
    "        self.nn_layers.append(nn.Conv2d(widths[0][0], widths[0][1],1).to(device))\n",
    "        for i in range(self.depth-2):\n",
    "            self.nn_layers.append(nn.BatchNorm2d(widths[i][1]).to(device))\n",
    "            self.nn_layers.append(nn.Conv2d(widths[i+1][0], widths[i+1][1],1).to(device))\n",
    "          \n",
    "        self.nn_layers.append(nn.BatchNorm2d(widths[self.depth-2][1]).to(device))\n",
    "        self.nn_layers.append(nn.Conv2d(widths[self.depth-1][0], widths[self.depth-1][1],1).to(device))\n",
    "        \n",
    "        self.preclyr=nn.Softplus().to(device)\n",
    "    def forward(self,x):\n",
    "        (u,geo)=torch.split(x,[2,x.shape[1]-2],dim=1)\n",
    "        u=self.take_derivatives(u)\n",
    "        geo=geo[:,:,self.order:-self.order,self.order:-self.order]\n",
    "        u=torch.cat([u,geo],dim=1)\n",
    "        i=0\n",
    "        for t in range(self.depth-1):\n",
    "            if self.nn_layers[i].weight.data.shape[1]>u.shape[1]:\n",
    "                u=torch.cat([u,geo],dim=1)\n",
    "            u=self.nn_layers[i](u)\n",
    "            i+=1\n",
    "            u=torch.selu(self.nn_layers[i](u))\n",
    "            i+=1\n",
    "        if self.nn_layers[i].weight.data.shape[1]>u.shape[1]:\n",
    "            u=torch.cat([u,geo],dim=1)\n",
    "        u=self.nn_layers[i](u)\n",
    "        (mean,prec)=torch.split(u,[2,1],dim=1)\n",
    "        prec=self.preclyr(prec)\n",
    "        y=torch.cat([mean,prec],dim=1)\n",
    "        return y\n",
    "    def take_derivatives(self,u):\n",
    "        for i in range(self.order):\n",
    "            dudy=u[:,:,2:,1:-1]-u[:,:,:-2,1:-1]\n",
    "            dudx=u[:,:,1:-1,2:]-u[:,:,1:-1,:-2]\n",
    "            u=u[:,:,1:-1,1:-1]\n",
    "            u=torch.cat([u,dudy,dudx],dim=1)\n",
    "        return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "056f1fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatReg(ClimateNet):\n",
    "    def __init__(self,order=2,width=64):\n",
    "        super(MatReg, self).__init__()\n",
    "        device=self.device\n",
    "        inwidth=3\n",
    "        self.order=order\n",
    "        self.direct_coord=True\n",
    "        outwidth=((3**self.order*2)*2+2)*3\n",
    "        self.width=width\n",
    "        self.depth=5\n",
    "        self.spread=self.order\n",
    "        self.nn_layers=nn.ModuleList()\n",
    "        self.nn_layers.append(nn.Conv2d(inwidth, width,1).to(device))\n",
    "        for i in range(self.depth-2):\n",
    "            self.nn_layers.append(nn.BatchNorm2d(width).to(device))\n",
    "            self.nn_layers.append(nn.Conv2d(width, width,1).to(device))\n",
    "            \n",
    "        self.nn_layers.append(nn.BatchNorm2d(width).to(device))\n",
    "        self.nn_layers.append(nn.Conv2d(width, outwidth,1).to(device))\n",
    "        for i in range(3):\n",
    "            self.nn_layers.append(nn.BatchNorm2d(outwidth//3).to(device))\n",
    "            self.nn_layers.append(nn.Conv2d(outwidth//3, 1,1).to(device))\n",
    "        self.preclyr=nn.Softplus().to(device)\n",
    "    def forward(self,x):\n",
    "        (u,geo)=torch.split(x,[2,x.shape[1]-2],dim=1)\n",
    "        i=0\n",
    "        for _ in range(self.depth-1):\n",
    "            geo=self.nn_layers[i](geo)\n",
    "            i+=1\n",
    "            geo=torch.selu(self.nn_layers[i](geo))\n",
    "            i+=1\n",
    "        geo=self.nn_layers[i](geo)\n",
    "        i+=1\n",
    "        geo=geo[:,:,self.order:-self.order,self.order:-self.order]\n",
    "        U=self.take_derivatives(u)\n",
    "        u=U[:,:2]\n",
    "        U=torch.cat([u,U*u[:,0:1],U*u[:,1:2]],dim=1)\n",
    "        GEO=torch.split(geo,geo.shape[1]//3,dim=1)\n",
    "        Y=[U*geo for geo in GEO]\n",
    "        for j in range(3):\n",
    "            Y[j]=self.nn_layers[i](Y[j])\n",
    "            i+=1\n",
    "            Y[j]=self.nn_layers[i](Y[j])\n",
    "            i+=1\n",
    "        Y[2]=self.preclyr(Y[2])\n",
    "        y=torch.cat(Y,dim=1)\n",
    "        return y\n",
    "    def take_derivatives(self,u):\n",
    "        for i in range(self.order):\n",
    "            dudy=u[:,:,2:,1:-1]-u[:,:,:-2,1:-1]\n",
    "            dudx=u[:,:,1:-1,2:]-u[:,:,1:-1,:-2]\n",
    "            u=u[:,:,1:-1,1:-1]\n",
    "            u=torch.cat([u,dudy,dudx],dim=1)\n",
    "        return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6820ef4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class UNET(ClimateNet):\n",
    "    def __init__(self,spread=0,heteroscedastic=True,\\\n",
    "                    #width=[128,64,32,32,32,32,32,3],\\\n",
    "                     widths=[64,128,256,512],\\\n",
    "                    pools=[2,2,2],\\\n",
    "                    filter_size=[5,5,3,3,3,3,3,3],\\\n",
    "                    deep_filters=[[3,3,3,1,1],[3,3,3,1,1],[3,3,3,1,1]],\\\n",
    "                    latsig=False,\\\n",
    "                    latsign=False,\\\n",
    "                    direct_coord=False,\\\n",
    "                    freq_coord=False,\\\n",
    "                    timeshuffle=False,\\\n",
    "                    physical_force_features=False,\\\n",
    "                    longitude=False,\\\n",
    "                    rescale=[1/10,1/1e7],\\\n",
    "                    initwidth=2,\\\n",
    "                    outwidth=2,\\\n",
    "                    nprecision=1,\\\n",
    "                    verbose=False):\n",
    "        super(UNET, self).__init__()\n",
    "        device=self.device\n",
    "        bnflag=True\n",
    "        self.bnflag=bnflag\n",
    "        self.direct_coord=direct_coord\n",
    "        self.freq_coord=freq_coord\n",
    "        self.longitude=longitude\n",
    "        self.latsig=latsig\n",
    "        self.latsign=latsign\n",
    "        self.nn_layers=nn.ModuleList()\n",
    "        self.verbose=verbose\n",
    "        self.nprecision=nprecision\n",
    "        self.nparam=0\n",
    "        self.bnflag=True#self.latsig or self.latsign or self.direct_coord\n",
    "        if self.direct_coord:\n",
    "            initwidth+=3\n",
    "        elif self.freq_coord:\n",
    "            if self.latsig:\n",
    "                initwidth+=1\n",
    "            if self.latsign:\n",
    "                initwidth+=1\n",
    "            if self.longitude:\n",
    "                initwidth+=2\n",
    "        self.initwidth=initwidth\n",
    "        self.outwidth=outwidth\n",
    "        self.spread=int((np.sum(filter_size)-len(filter_size))/2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        widthin=initwidth\n",
    "        widthout=outwidth+nprecision\n",
    "        \n",
    "        #widths=[64,128,256,512]#[2,4,8,16]#\n",
    "        nlevel=len(widths)\n",
    "        self.nlevel=nlevel\n",
    "        self.locs=[]\n",
    "        self.pools=pools\n",
    "        \n",
    "        self.receptive_field=1\n",
    "        for i in range(len(pools)):\n",
    "            ww=np.sum(deep_filters[-1-i])-len(deep_filters[-1-i])\n",
    "            self.receptive_field=(self.receptive_field+ww)*pools[-i-1]\n",
    "        self.receptive_field+=np.sum(filter_size[:3])-3\n",
    "        #self.add_conv_layers([5,5,3],[widths[0]]*4,widthin=widthin) \n",
    "        self.add_conv_layers(filter_size[:3],[widths[0]]*4,widthin=widthin) \n",
    "        self.add_conv_layers(1,[widths[0]]*3)\n",
    "        for i in range(nlevel-1):\n",
    "            pool=[pools[i],pools[i]]\n",
    "            self.add_down_sampling(pool)\n",
    "            self.add_conv_layers(deep_filters[i],[widths[i+1]]*(len(deep_filters[i])+1),widthin=widths[i])\n",
    "            #self.add_conv_layers(1,[widths[i+1]]*3)\n",
    "            \n",
    "        \n",
    "        for i in range(nlevel-2):\n",
    "            pool=[pools[-1-i],pools[-1-i]]\n",
    "            self.add_up_sampling(1,[widths[-1-i],widths[-1-i]//2],pool)\n",
    "            self.add_conv_layers(deep_filters[-1-i],[widths[-2-i]]*(len(deep_filters[-1-i])+1),widthout=widths[-2-i],widthin=widths[-1-i])\n",
    "            \n",
    "        \n",
    "        i=nlevel-2\n",
    "        pool=[pools[-1-i],pools[-1-i]]\n",
    "        self.add_up_sampling(1,[widths[-1-i],widths[-1-i]//2],pool)\n",
    "        self.add_conv_layers(filter_size[3:],[widths[-2-i]]*6,widthout=widthout,widthin=widths[-1-i])\n",
    "        self.to_device()\n",
    "        self.precisionlyr=nn.Softplus().to(device)\n",
    "    def add_conv_layers(self,conv,width,widthin=0,widthout=0,final_nonlinearity=False):\n",
    "        loc0=len(self.nn_layers)\n",
    "        if widthin!=0:\n",
    "            width[0]=widthin\n",
    "        if widthout!=0:\n",
    "            width[-1]=widthout\n",
    "        if type(conv)==int:\n",
    "            conv=[conv]*(len(width)-1)\n",
    "        for i in range(len(width)-2):\n",
    "            self.nn_layers.append(nn.Conv2d(width[i], width[i+1],conv[i]))\n",
    "            self.nparam+=width[i]*width[i+1]*conv[i]**2\n",
    "            self.nn_layers.append(nn.BatchNorm2d(width[i+1]))\n",
    "            self.nparam+=width[i+1]\n",
    "            self.nn_layers.append(nn.ReLU(inplace=True))\n",
    "        self.nn_layers.append(nn.Conv2d(width[-2], width[-1],conv[-1]))\n",
    "        self.nparam+=width[-2]*width[-1]*conv[-1]**2\n",
    "        if final_nonlinearity:\n",
    "            self.nn_layers.append(nn.BatchNorm2d(width[-1]))\n",
    "            self.nparam+=width[-1]\n",
    "            self.nn_layers.append(nn.ReLU(inplace=True))\n",
    "        loc1=len(self.nn_layers)\n",
    "        self.locs.append([loc0,loc1])\n",
    "    def add_down_sampling(self,pool):\n",
    "        loc0=len(self.nn_layers)\n",
    "        self.nn_layers.append(nn.MaxPool2d(pool, stride=pool))\n",
    "        loc1=len(self.nn_layers)\n",
    "        self.locs.append([loc0,loc1])\n",
    "    def add_up_sampling(self,conv,width,pool):\n",
    "        loc0=len(self.nn_layers)\n",
    "        self.nn_layers.append(nn.ConvTranspose2d(width[0], width[1],conv,stride=pool))\n",
    "        self.nparam+=width[0]*width[1]*conv**2\n",
    "        loc1=len(self.nn_layers)\n",
    "        self.locs.append([loc0,loc1])\n",
    "    def apply_layers(self,x,K):\n",
    "        for i in range(self.locs[K][0],self.locs[K][1]):\n",
    "            x=self.nn_layers[i](x)\n",
    "        return x\n",
    "    def to_device(self,):\n",
    "        for i in range(len(self.nn_layers)):\n",
    "            self.nn_layers[i]= self.nn_layers[i].to(self.device)       \n",
    "\n",
    "    def trim_merge(self,f,x):\n",
    "        if type(x)==torch.Tensor:\n",
    "            ny=(f.shape[2]-x.shape[2])\n",
    "            nx=(f.shape[3]-x.shape[3])\n",
    "        else:\n",
    "            ny=(f.shape[2]-x[0])\n",
    "            nx=(f.shape[3]-x[1])\n",
    "        ny0,ny1=ny//2,ny//2\n",
    "        nx0,nx1=nx//2,nx//2\n",
    "        if ny0+ny1<ny:\n",
    "            ny1+=1\n",
    "        if nx0+nx1<nx:\n",
    "            nx1+=1\n",
    "        f=f[:,:,ny0:-ny1,nx0:-nx1]\n",
    "        \n",
    "        if type(x)==torch.Tensor:\n",
    "            return torch.cat([f,x],dim=1)\n",
    "        else:\n",
    "            return f\n",
    "    def zeropad(self,f,x):\n",
    "        if type(x)==torch.Tensor:\n",
    "            diffY=(f.shape[2]-x.shape[2])\n",
    "            diffX=(f.shape[3]-x.shape[3])\n",
    "        else:\n",
    "            diffY=(f.shape[2]-x.shape[2])\n",
    "            diffX=(f.shape[3]-x.shape[3])\n",
    "        \n",
    "        x = F.pad(x, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        \n",
    "        if type(x)==torch.Tensor:\n",
    "            return torch.cat([f,x],dim=1)\n",
    "        else:\n",
    "            return f\n",
    "    def forward(self,u):\n",
    "        locs=self.locs\n",
    "        \n",
    "        features=[]\n",
    "        nlevel=self.nlevel\n",
    "        t=0\n",
    "        x=u*1\n",
    "        x=self.apply_layers(x,t) # convolutions\n",
    "        if self.verbose:\n",
    "            print('conv: '+str(0)+' '+str(t)+  '  '+ str(x.shape))\n",
    "        t+=1\n",
    "        \n",
    "        f=x*1\n",
    "        f=self.apply_layers(f,t) # ptswise\n",
    "        if self.verbose:\n",
    "            print('ptswise: '+str(0)+' '+str(t)+  '  '+ str(f.shape))\n",
    "        t+=1\n",
    "        features.append(f)\n",
    "        \n",
    "        for i in range(nlevel-1):\n",
    "            x=self.apply_layers(x,t) # downsampling\n",
    "            if self.verbose:\n",
    "                print('down: '+str(i+1)+' '+str(t)+  '  '+ str(x.shape))\n",
    "            t+=1\n",
    "            \n",
    "            x=self.apply_layers(x,t) # convolutions\n",
    "            if self.verbose:\n",
    "                print('conv: '+str(i+1)+' '+str(t)+  '  '+ str(x.shape))\n",
    "            t+=1\n",
    "            \n",
    "            f=x*1\n",
    "            '''\n",
    "            f=self.apply_layers(f,t) # ptswise\n",
    "            if self.verbose:\n",
    "                print('ptswise: '+str(i+1)+' '+str(t)+  '  '+ str(f.shape))\n",
    "            t+=1'''\n",
    "            features.append(f)\n",
    "\n",
    "        f=features[-1]\n",
    "        for jj in range(1,nlevel):\n",
    "            j=nlevel-jj-1\n",
    "            x=self.apply_layers(f,t) # upsample\n",
    "            if self.verbose:\n",
    "                print('upsample: ('+str(j)+ ', '+str(0)+') '+str(t)+ '  '+ str(x.shape))\n",
    "            t+=1\n",
    "            f=features[j]\n",
    "            f=self.zeropad(f,x)\n",
    "            f=self.apply_layers(f,t) # convolutions\n",
    "            if self.verbose:\n",
    "                print('conv: '+str(i+1)+' '+str(t)+  '  '+ str(f.shape))\n",
    "            t+=1\n",
    "        (mean,prec)=torch.split(f,[self.outwidth,self.nprecision],dim=1)\n",
    "        prec=self.precisionlyr(prec)\n",
    "        y=torch.cat([mean,prec],dim=1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3abdd1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CQCNN(ClimateNet):\n",
    "    def __init__(self,width=32,classnum=8,direct_coord=False):\n",
    "        super(CQCNN, self).__init__()\n",
    "        latsig=True\n",
    "        latsign=True\n",
    "        longitude=True\n",
    "        heteroscedastic=True\n",
    "        self.heteroscedastic=heteroscedastic\n",
    "        self.spread=10\n",
    "        self.direct_coord=direct_coord\n",
    "        self.longitude=longitude\n",
    "        self.latsig=latsig\n",
    "        self.latsign=latsign\n",
    "        \n",
    "        self.nn_layers = nn.ModuleList()\n",
    "        self.other_learnables = []\n",
    "        self.locs=[]\n",
    "        self.layers={}\n",
    "        geoin=4\n",
    "        flowin=2\n",
    "        width_in=geoin+flowin\n",
    "        self.classnum=classnum\n",
    "        self.width=width\n",
    "        #width=[128,64,32,32,32,32,32,3],\\\n",
    "                    #filter_size=\n",
    "        self.create_conv_layers([5,5,3,3,3,3,3,3],[width_in,64,32,32,32,32,32,32,classnum],\\\n",
    "                                final_nonlinearity=False,\\\n",
    "                                label='class-conv')\n",
    "        self.create_batch_norm_layer(classnum,label='presoftmax')\n",
    "        self.create_softmax_layer(label='softmax')\n",
    "        for i in range(classnum):\n",
    "            self.create_conv_layers([7,5],[flowin,width,width],\\\n",
    "                                    mid_nonlinearity=False,final_nonlinearity=False,label='fun-conv0-'+str(i))\n",
    "            self.create_poly_nonlinearity(width,2,label='poly-nnlnr-'+str(i))\n",
    "            #self.create_batch_norm_layer(width,label='post-poly-nnlnr-'+str(i))\n",
    "            self.create_conv_layers([7,5],[width,width,2],\\\n",
    "                                    mid_nonlinearity=False,final_nonlinearity=False,label='fun-conv1-'+str(i))\n",
    "        for i in range(classnum):\n",
    "            self.create_conv_layers([7,5],[flowin,width,width],\\\n",
    "                                    mid_nonlinearity=False,final_nonlinearity=False,label='h-fun-conv0-'+str(i))\n",
    "            self.create_poly_nonlinearity(width,4,label='h-poly-nnlnr-'+str(i))\n",
    "            #self.create_batch_norm_layer(width,label='h-post-poly-nnlnr-'+str(i))\n",
    "            self.create_conv_layers([7,5],[width,width,1],\\\n",
    "                                    mid_nonlinearity=False,final_nonlinearity=False,label='h-fun-conv1-'+str(i))\n",
    "        self.create_batch_norm_layer(1,label='h-pre-softplus-nnlnr')\n",
    "        self.to_device()\n",
    "        self.precisionlyr=nn.Softplus().to(self.device)\n",
    "        \n",
    "    def to_device(self,):\n",
    "        for i in range(len(self.nn_layers)):\n",
    "            self.nn_layers[i]= self.nn_layers[i].to(self.device)   \n",
    "        for i in range(len(self.other_learnables)):\n",
    "            self.other_learnables[i]= self.other_learnables[i].to(self.device)   \n",
    "    def forward(self,x):\n",
    "        uv,_=torch.split(x,[2,x.shape[1]-2],dim=1)\n",
    "        t=0\n",
    "        x=self.apply_layers(x,label='class-conv') #convolution\n",
    "        x=self.apply_layers(x,label='presoftmax') #batchnorm\n",
    "        cl0=self.apply_layers(x,label='softmax') #softmax\n",
    "        cl=torch.split(cl0,1,dim=1)\n",
    "        mean=torch.zeros(cl[0].shape[0],2,cl[0].shape[2],cl[0].shape[3]).to(self.device)\n",
    "        pre_precision=torch.zeros(cl[0].shape[0],1,cl[0].shape[2],cl[0].shape[3]).to(self.device)\n",
    "        for i in range(self.classnum):\n",
    "            y=self.apply_layers(uv,label='fun-conv0-'+str(i)) \n",
    "            #y=self.apply_layers(y,label='pre-poly-nnlnr-'+str(i))\n",
    "            y=self.apply_poly_nonlinearity(y,label='poly-nnlnr-'+str(i))\n",
    "            mean+=cl[i]*self.apply_layers(y,label='fun-conv1-'+str(i))\n",
    "        for i in range(self.classnum):\n",
    "            y=self.apply_layers(uv,label='h-fun-conv0-'+str(i)) \n",
    "            #y=self.apply_layers(y,label='h-pre-poly-nnlnr-'+str(i))\n",
    "            y=self.apply_poly_nonlinearity(y,label='h-poly-nnlnr-'+str(i))\n",
    "            pre_precision+=cl[i]*self.apply_layers(y,label='h-fun-conv1-'+str(i))\n",
    "        pre_precision=self.apply_layers(pre_precision,label='h-pre-softplus-nnlnr')\n",
    "        prec=self.precisionlyr(pre_precision)\n",
    "        #print(mean.shape,prec.shape,cl0.shape)\n",
    "        return torch.cat([mean,prec,cl0],dim=1)\n",
    "    def quadratic_forward(self,x,class_index=0):\n",
    "        uv,_=torch.split(x,[2,x.shape[1]-2],dim=1)\n",
    "        i=class_index\n",
    "        y=self.apply_layers(uv,label='fun-conv0-'+str(i)) \n",
    "        #y=self.apply_layers(y,label='pre-poly-nnlnr-'+str(i))\n",
    "        y=self.apply_poly_nonlinearity(y,label='poly-nnlnr-'+str(i))\n",
    "        y=self.apply_layers(y,label='fun-conv1-'+str(i))\n",
    "        return y\n",
    "    def register_layers(self,loc0,loc1,label):\n",
    "        self.layers[label]=[loc0,loc1]\n",
    "    def create_poly_nonlinearity(self,width,degree,label='poly-nnlnr'):\n",
    "        loc0=len(self.nn_layers)\n",
    "        self.nn_layers.append(nn.Conv2d(1,1,1,bias=False))\n",
    "        for i in range(degree):\n",
    "            self.nn_layers.append(nn.BatchNorm2d(width))\n",
    "            self.nn_layers.append(nn.Conv2d(1,1,1,bias=False))\n",
    "        loc1=len(self.nn_layers)\n",
    "        self.register_layers(loc0,loc1,label)\n",
    "    def create_batch_norm_layer(self,width,label='batchnorm'):\n",
    "        loc0=len(self.nn_layers)\n",
    "        self.nn_layers.append(nn.BatchNorm2d(width))\n",
    "        loc1=len(self.nn_layers)\n",
    "        self.register_layers(loc0,loc1,label)\n",
    "    def create_softmax_layer(self,label='softmax'):\n",
    "        loc0=len(self.nn_layers)\n",
    "        self.nn_layers.append(nn.Softmax(dim=1)) \n",
    "        loc1=len(self.nn_layers)\n",
    "        self.register_layers(loc0,loc1,label)\n",
    "    def apply_poly_nonlinearity(self,x,label):\n",
    "        loc0=self.layers[label][0]\n",
    "        loc1=self.layers[label][1]\n",
    "        y=self.nn_layers[loc0].weight.data+x*0\n",
    "        j=1\n",
    "        for i in range(loc0+1,loc1,2):\n",
    "            b=self.nn_layers[i](x)\n",
    "            y+=self.nn_layers[i+1].weight.data*(b**j)/np.math.factorial(i-loc0)\n",
    "            j+=1\n",
    "        return y\n",
    "    def apply_layers(self,x,label):\n",
    "        for i in range(self.layers[label][0],self.layers[label][1]):\n",
    "            x=self.nn_layers[i](x)\n",
    "        return x\n",
    "    def create_conv_layers(self,conv,width,widthin=0,widthout=0,mid_nonlinearity=True,final_nonlinearity=False,label='conv'):\n",
    "        loc0=len(self.nn_layers)\n",
    "        if widthin!=0:\n",
    "            width[0]=widthin\n",
    "        if widthout!=0:\n",
    "            width[-1]=widthout\n",
    "        if type(conv)==int:\n",
    "            conv=[conv]*(len(width)-1)\n",
    "        for i in range(len(width)-2):\n",
    "            self.nn_layers.append(nn.Conv2d(width[i], width[i+1],conv[i]))\n",
    "            if mid_nonlinearity:\n",
    "                self.nn_layers.append(nn.BatchNorm2d(width[i+1]))\n",
    "                self.nn_layers.append(nn.ReLU())\n",
    "        self.nn_layers.append(nn.Conv2d(width[-2], width[-1],conv[-1]))\n",
    "        if final_nonlinearity:\n",
    "            self.nn_layers.append(nn.BatchNorm2d(width[-1]))\n",
    "            self.nn_layers.append(nn.ReLU())\n",
    "        loc1=len(self.nn_layers)\n",
    "        self.register_layers(loc0,loc1,label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fbba7a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Improved_QCNN(ClimateNet):\n",
    "    def __init__(self,width=64,filter_size=[11,11],\\\n",
    "                    direct_coord=False):\n",
    "        super(Improved_QCNN, self).__init__()\n",
    "        latsig=True\n",
    "        latsign=True\n",
    "        timeshuffle=False\n",
    "        longitude=True\n",
    "        heteroscedastic=True\n",
    "        self.heteroscedastic=heteroscedastic\n",
    "        self.spread=10\n",
    "        self.direct_coord=direct_coord\n",
    "        self.longitude=longitude\n",
    "        self.latsig=latsig\n",
    "        self.latsign=latsign\n",
    "        \n",
    "        self.nn_layers = nn.ModuleList()\n",
    "        self.locs=[]\n",
    "        \n",
    "        geoin=4\n",
    "        flowin=2\n",
    "        \n",
    "        widths=[geoin,64,32,16]\n",
    "        \n",
    "        \n",
    "        self.add_conv_layers([11,7,5],[geoin,16,16,16],final_nonlinearity=True)\n",
    "        self.add_transconv_layers([5,7,11],[16,16,16,width],final_nonlinearity=False)\n",
    "        self.add_conv_layers([1],[flowin,width],final_nonlinearity=False)\n",
    "        self.add_batch_norm_layer(width)\n",
    "        self.add_conv_layers([11],[width,width])\n",
    "        self.add_batch_norm_layer(width)\n",
    "        self.add_conv_layers([11],[width,16],final_nonlinearity=False)\n",
    "        self.add_conv_layers([1,1,1,1],[16,16,16,16,3],final_nonlinearity=False)\n",
    "        self.to_device()\n",
    "        self.precisionlyr=nn.Softplus().to(self.device)\n",
    "        \n",
    "    def to_device(self,):\n",
    "        for i in range(len(self.nn_layers)):\n",
    "            self.nn_layers[i]= self.nn_layers[i].to(self.device)   \n",
    "    def forward(self,x):\n",
    "        uv,geo=torch.split(x,[2,x.shape[1]-2],dim=1)\n",
    "        t=0\n",
    "        geo=self.apply_layers(geo,t) #convolution\n",
    "        #print('geo: '+str(geo.shape))\n",
    "        t+=1\n",
    "        geo=self.apply_layers(geo,t) #deconvolution\n",
    "        #print('geo: '+str(geo.shape))\n",
    "        t+=1\n",
    "        uv=self.apply_layers(uv,t) #upwidth\n",
    "        #print('uv: '+str(uv.shape))\n",
    "        t+=1\n",
    "        uv=uv*(1+geo)\n",
    "        uv=self.apply_layers(uv,t) #batch normalization\n",
    "        t+=1\n",
    "        uv=self.apply_layers(uv,t) #convolution\n",
    "        t+=1\n",
    "        uv=torch.square(uv) #squaring\n",
    "        uv=self.apply_layers(uv,t) #batch normalization\n",
    "        t+=1\n",
    "        uv=self.apply_layers(uv,t) #convolution\n",
    "        t+=1\n",
    "        uv=self.apply_layers(uv,t) #pointwise nonlinearity\n",
    "        t+=1\n",
    "        mean,prec=torch.split(uv,[2,1],dim=1)\n",
    "        prec=self.precisionlyr(prec)\n",
    "        return torch.cat([mean,prec],dim=1)\n",
    "    def add_batch_norm_layer(self,width):\n",
    "        loc0=len(self.nn_layers)\n",
    "        self.nn_layers.append(nn.BatchNorm2d(width)) #filter multip\n",
    "        loc1=len(self.nn_layers)\n",
    "        self.locs.append([loc0,loc1])\n",
    "        \n",
    "    def add_conv_layers(self,conv,width,widthin=0,widthout=0,final_nonlinearity=False):\n",
    "        loc0=len(self.nn_layers)\n",
    "        if widthin!=0:\n",
    "            width[0]=widthin\n",
    "        if widthout!=0:\n",
    "            width[-1]=widthout\n",
    "        if type(conv)==int:\n",
    "            conv=[conv]*(len(width)-1)\n",
    "        for i in range(len(width)-2):\n",
    "            self.nn_layers.append(nn.Conv2d(width[i], width[i+1],conv[i]))\n",
    "            self.nn_layers.append(nn.BatchNorm2d(width[i+1]))\n",
    "            self.nn_layers.append(nn.ReLU())\n",
    "        self.nn_layers.append(nn.Conv2d(width[-2], width[-1],conv[-1]))\n",
    "        if final_nonlinearity:\n",
    "            self.nn_layers.append(nn.BatchNorm2d(width[-1]))\n",
    "            self.nn_layers.append(nn.ReLU())\n",
    "        loc1=len(self.nn_layers)\n",
    "        self.locs.append([loc0,loc1])\n",
    "    def add_transconv_layers(self,conv,width,pool=[1,1],widthin=0,widthout=0,nnlnr=True,final_nonlinearity=False):\n",
    "        loc0=len(self.nn_layers)\n",
    "        if widthin!=0:\n",
    "            width[0]=widthin\n",
    "        if widthout!=0:\n",
    "            width[-1]=widthout\n",
    "        if type(conv)==int:\n",
    "            conv=[conv]*(len(width)-1)\n",
    "        for i in range(len(width)-2):\n",
    "            self.nn_layers.append(nn.ConvTranspose2d(width[i], width[i+1],conv[i]))\n",
    "            self.nn_layers.append(nn.BatchNorm2d(width[i+1]))\n",
    "            if nnlnr:\n",
    "                self.nn_layers.append(nn.ReLU())\n",
    "        self.nn_layers.append(nn.ConvTranspose2d(width[-2], width[-1],conv[-1]))\n",
    "        if final_nonlinearity:\n",
    "            self.nn_layers.append(nn.BatchNorm2d(width[-1]))\n",
    "            if nnlnr:\n",
    "                self.nn_layers.append(nn.ReLU())\n",
    "        loc1=len(self.nn_layers)\n",
    "        self.locs.append([loc0,loc1])\n",
    "        \n",
    "    def apply_layers(self,x,K):\n",
    "        for i in range(self.locs[K][0],self.locs[K][1]):\n",
    "            x=self.nn_layers[i](x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a63d6e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(ClimateNet):\n",
    "    def __init__(self,):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        latsig=False\n",
    "        latsign=False\n",
    "        timeshuffle=False\n",
    "        longitude=False\n",
    "        heteroscedastic=False\n",
    "        direct_coord=False\n",
    "        self.heteroscedastic=heteroscedastic\n",
    "        self.spread=0\n",
    "        self.generative=True\n",
    "        self.rescale=[]\n",
    "        self.direct_coord=direct_coord\n",
    "        self.longitude=longitude\n",
    "        self.latsig=latsig\n",
    "        self.latsign=latsign\n",
    "        \n",
    "        self.nn_layers = nn.ModuleList()\n",
    "        self.locs=[]\n",
    "        \n",
    "        #self.add_conv_layers([9,7,5,2],[2,32,32,64,256],final_nonlinearity=True)\n",
    "        self.add_conv_layers([5,5,3,3,3,3,3],[2,32,64,64,64,64,128,128],final_nonlinearity=True)\n",
    "        #self.add_transconv_layers([2,5,7,9],[256,64,32,32,2],final_nonlinearity=False)\n",
    "        self.add_transconv_layers([3,3,3,3,3,5,5],[128,128,64,64,64,64,32,2],final_nonlinearity=False)\n",
    "        self.to_device()\n",
    "        \n",
    "    def to_device(self,):\n",
    "        for i in range(len(self.nn_layers)):\n",
    "            self.nn_layers[i]= self.nn_layers[i].to(self.device)   \n",
    "    def forward(self,x):\n",
    "        uv,geo=torch.split(x,[2,x.shape[1]-2],dim=1)\n",
    "        for t in range(2):\n",
    "            uv=self.apply_layers(uv,t) \n",
    "        return uv\n",
    "    def encode(self,x):\n",
    "        uv,geo=torch.split(x,[2,x.shape[1]-2],dim=1)\n",
    "        return self.apply_layers(uv,0) \n",
    "    def decode(self,x):\n",
    "        uv,geo=torch.split(x,[2,x.shape[1]-2],dim=1)\n",
    "        return self.apply_layers(uv,1) \n",
    "    def add_batch_norm_layer(self,width):\n",
    "        loc0=len(self.nn_layers)\n",
    "        self.nn_layers.append(nn.BatchNorm2d(width)) #filter multip\n",
    "        loc1=len(self.nn_layers)\n",
    "        self.locs.append([loc0,loc1])\n",
    "        \n",
    "    def add_conv_layers(self,conv,width,stride=[],widthin=0,widthout=0,final_nonlinearity=False):\n",
    "        loc0=len(self.nn_layers)\n",
    "        if widthin!=0:\n",
    "            width[0]=widthin\n",
    "        if widthout!=0:\n",
    "            width[-1]=widthout\n",
    "        if type(conv)==int:\n",
    "            conv=[conv]*(len(width)-1)\n",
    "        if len(stride)==0:\n",
    "            stride=[1]*(len(width)-1)\n",
    "        for i in range(len(width)-2):\n",
    "            self.nn_layers.append(nn.Conv2d(width[i], width[i+1],conv[i],stride=stride[i]))\n",
    "            self.nn_layers.append(nn.BatchNorm2d(width[i+1]))\n",
    "            self.nn_layers.append(nn.ReLU())\n",
    "        self.nn_layers.append(nn.Conv2d(width[-2], width[-1],conv[-1],stride=stride[-1]))\n",
    "        if final_nonlinearity:\n",
    "            self.nn_layers.append(nn.BatchNorm2d(width[-1]))\n",
    "            self.nn_layers.append(nn.ReLU())\n",
    "        loc1=len(self.nn_layers)\n",
    "        self.locs.append([loc0,loc1])\n",
    "    def add_transconv_layers(self,conv,width,stride=[],pool=[1,1],widthin=0,widthout=0,nnlnr=True,final_nonlinearity=False):\n",
    "        loc0=len(self.nn_layers)\n",
    "        if widthin!=0:\n",
    "            width[0]=widthin\n",
    "        if widthout!=0:\n",
    "            width[-1]=widthout\n",
    "        if type(conv)==int:\n",
    "            conv=[conv]*(len(width)-1)\n",
    "        if len(stride)==0:\n",
    "            stride=[1]*(len(width)-1)\n",
    "        for i in range(len(width)-2):\n",
    "            self.nn_layers.append(nn.ConvTranspose2d(width[i], width[i+1],conv[i],stride=stride[i]))\n",
    "            self.nn_layers.append(nn.BatchNorm2d(width[i+1]))\n",
    "            if nnlnr:\n",
    "                self.nn_layers.append(nn.ReLU())\n",
    "        self.nn_layers.append(nn.ConvTranspose2d(width[-2], width[-1],conv[-1],stride=stride[-1]))\n",
    "        if final_nonlinearity:\n",
    "            self.nn_layers.append(nn.BatchNorm2d(width[-1]))\n",
    "            if nnlnr:\n",
    "                self.nn_layers.append(nn.ReLU())\n",
    "        loc1=len(self.nn_layers)\n",
    "        self.locs.append([loc0,loc1])\n",
    "        \n",
    "    def apply_layers(self,x,K):\n",
    "        for i in range(self.locs[K][0],self.locs[K][1]):\n",
    "            x=self.nn_layers[i](x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18b568cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'net=UNET(verbose=True,ypad=150,xpad=200)'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''net=UNET(verbose=True,ypad=150,xpad=200)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ffde69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74f1b197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(645-y.shape[2]) // 2,  (900-y.shape[3])//2'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''(645-y.shape[2]) // 2,  (900-y.shape[3])//2'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac1e21b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'y=net.forward(x)'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''y=net.forward(x)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "7556f4ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'x=torch.randn(2,6,net.ypad*2+645,net.xpad*2+900)'"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''x=torch.randn(2,6,net.ypad*2+645,net.xpad*2+900)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d962be1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "closing parenthesis ']' does not match opening parenthesis '(' (<ipython-input-137-a2146ff5cbb9>, line 69)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-137-a2146ff5cbb9>\"\u001b[0;36m, line \u001b[0;32m69\u001b[0m\n\u001b[0;31m    widthdown.append(initwidth,width[ii]])\u001b[0m\n\u001b[0m                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m closing parenthesis ']' does not match opening parenthesis '('\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb53c9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
